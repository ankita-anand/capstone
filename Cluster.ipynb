{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from embedder import Embedder\n",
    "import numpy as np\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "import json\n",
    "\n",
    "embedder = Embedder()\n",
    "data = json.load(open('data/wikidata.json'))\n",
    "embeddings = {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = {concept:embedder.tokenize(data[concept]) for concept in data}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://en.wikipedia.org/wiki/Normal_distribution#Cumulative_distribution_function \t\t\t\t 9036\n",
      "b'The cumulative distribution function (CDF) of the standard normal distribution, usually denoted with the capital Greek letter  (phi), is the integral\\n\\n\\nThe related error function  gives the probability of a random variable with normal distribution of mean 0 and variance 1/2 falling in the range ; that is\\n\\n\\nThese integrals cannot be expressed in terms of elementary functions, and are often said to be special functions. However, many numerical approximations are known; see below.\\nThe two functions are closely related, namely\\n\\n\\nFor a generic normal distribution with density , mean  and deviation , the cumulative distribution function is\\n\\n\\nThe complement of the standard normal CDF, , is often called the Q-function, especially in engineering texts. It gives the probability that the value of a standard normal random variable  will exceed : . Other definitions of the -function, all of which are simple transformations of , are also used occasionally.\\nThe graph of the standard normal CDF  has 2-fold rotational symmetry around the point (0,1/2); that is, . Its antiderivative (indefinite integral) is \\n\\n\\nThe CDF of the standard normal distribution can be expanded by Integration by parts into a series:\\n\\n\\nwhere  denotes the double factorial.\\nAn asymptotic expansion of the CDF for large x can also be derived using integration by parts; see Error function#Asymptotic expansion.\\n\\nStandard deviation and coverage\\n\\n For the normal distribution, the values less than one standard deviation away from the mean account for 68.27% of the set; while two standard deviations from the mean account for 95.45%; and three standard deviations account for 99.73%.\\nAbout 68% of values drawn from a normal distribution are within one standard deviation  away from the mean; about 95% of the values lie within two standard deviations; and about 99.7% are within three standard deviations. This fact is known as the 68-95-99.7 (empirical) rule, or the 3-sigma rule.\\nMore precisely, the probability that a normal deviate lies in the range between  and  is given by\\n\\n\\nTo 12 significant figures, the values for  are:\\n\\n\\n\\n\\n\\n\\n\\nOEIS\\n\\n\\n1\\n0.682689492137\\n0.317310507863\\n\\n\\n\\n3\\n.15148718753\\n\\n\\nOEIS:A178647\\n\\n\\n2\\n0.954499736104\\n0.045500263896\\n\\n\\n\\n21\\n.9778945080\\n\\n\\nOEIS:A110894\\n\\n\\n3\\n0.997300203937\\n0.002699796063\\n\\n\\n\\n370\\n.398347345\\n\\n\\nOEIS:A270712\\n\\n\\n4\\n0.999936657516\\n0.000063342484\\n\\n\\n\\n15787\\n.1927673\\n\\n\\n\\n5\\n0.999999426697\\n0.000000573303\\n\\n\\n\\n1744277\\n.89362\\n\\n\\n\\n6\\n0.999999998027\\n0.000000001973\\n\\n\\n\\n506797345\\n.897\\n\\n\\nQuantile function\\n\\nThe quantile function of a distribution is the inverse of the cumulative distribution function.  The quantile function of the standard normal distribution is called the probit function, and can be expressed in terms of the inverse error function:\\n\\n\\nFor a normal random variable with mean  and variance , the quantile function is\\n\\n\\nThe quantile  of the standard normal distribution is commonly denoted as . These values are used in hypothesis testing, construction of confidence intervals and Q-Q plots. A normal random variable  will exceed  with probability , and will lie outside the interval  with probability . In particular, the quantile  is 1.96; therefore a normal random variable will lie outside the interval  in only 5% of cases.\\nThe following table gives the quantile  such that  will lie in the range  with a specified probability . These values are useful to determine tolerance interval for sample averages and other statistical estimators with normal (or asymptotically normal) distributions:. NOTE: the following table shows , not  as defined above.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n0.80\\n1.281551565545\\n0.999\\n3.290526731492\\n\\n\\n0.90\\n1.644853626951\\n0.9999\\n3.890591886413\\n\\n\\n0.95\\n1.959963984540\\n0.99999\\n4.417173413469\\n\\n\\n0.98\\n2.326347874041\\n0.999999\\n4.891638475699\\n\\n\\n0.99\\n2.575829303549\\n0.9999999\\n5.326723886384\\n\\n\\n0.995\\n2.807033768344\\n0.99999999\\n5.730728868236\\n\\n\\n0.998\\n3.090232306168\\n0.999999999\\n6.109410204869\\n\\nFor small , the quantile function has the useful asymptotic expansion \\n\\n\\nProperties\\nThe normal distribution is the only distribution whose cumulants beyond the first two (i.e., other than the mean and variance) are zero. It is also the continuous distribution with the maximum entropy for a specified mean and variance. Geary has shown, assuming that the mean and variance are finite, that the normal distribution is the only distribution where the mean and variance calculated from a set of independent draws are independent of each other.\\nThe normal distribution is a subclass of the elliptical distributions. The normal distribution is symmetric about its mean, and is non-zero over the entire real line. As such it may not be a suitable model for variables that are inherently positive or strongly skewed, such as the weight of a person or the price of a share. Such variables may be better described by other distributions, such as the log-normal distribution or the Pareto distribution.\\nThe value of the normal distribution is practically zero when the value  lies more than a few standard deviations away from the mean (e.g., a spread of three standard deviations covers all but 0.27% of the total distribution). Therefore, it may not be an appropriate model when one expects a significant fraction of outliersvalues that lie many standard deviations away from the meanand least squares and other statistical inference methods that are optimal for normally distributed variables often become highly unreliable when applied to such data. In those cases, a more heavy-tailed distribution should be assumed and the appropriate robust statistical inference methods applied.\\nThe Gaussian distribution belongs to the family of stable distributions which are the attractors of sums of independent, identically distributed distributions whether or not the mean or variance is finite. Except for the Gaussian which is a limiting case, all stable distributions have heavy tails and infinite variance. It is one of the few distributions that are stable and that have probability density functions that can be expressed analytically, the others being the Cauchy distribution and the Lvy distribution.\\n\\nSymmetries and derivatives\\nThe normal distribution with density  (mean  and standard deviation ) has the following properties:\\n\\nIt is symmetric around the point  which is at the same time the mode, the median and the mean of the distribution.\\nIt is unimodal: its first derivative is positive for  negative for  and zero only at \\nThe area under the curve and over the -axis is unity (i.e. equal to one).\\nIts density has two inflection points (where the second derivative of  is zero and changes sign), located one standard deviation away from the mean, namely at  and \\nIts density is log-concave.\\nIts density is infinitely differentiable, indeed supersmooth of order 2.\\nFurthermore, the density  of the standard normal distribution (i.e.  and ) also has the following properties:\\n\\nIts first derivative is \\nIts second derivative is \\nMore generally, its th derivative is  where  is the th (probabilist) Hermite polynomial.\\nThe probability that a normally distributed variable  with known  and  is in a particular set, can be calculated by using the fact that the fraction  has a standard normal distribution.\\nMoments\\n\\nThe plain and absolute moments of a variable  are the expected values of  and , respectively. If the expected value  of  is zero, these parameters are called central moments.  Usually we are interested only in moments with integer order .\\nIf  has a normal distribution, these moments exist and are finite for any  whose real part is greater than1. For any non-negative integer , the plain central moments are:\\n\\n\\nHere  denotes the double factorial, that is, the product of all numbers from  to1 that have the same parity as \\nThe central absolute moments coincide with plain moments for all even orders, but are nonzero for odd orders. For any non-negative integer \\n\\n\\nThe last formula is valid also for any non-integer   When the mean  the plain and absolute moments can be expressed in terms of confluent hypergeometric functions  and []\\n\\n\\nThese expressions remain valid even if  is not integer. See also generalized Hermite polynomials.\\n\\n\\n\\nOrder\\nNon-central moment\\nCentral moment\\n\\n\\n1\\n\\n\\n\\n\\n\\n\\n2\\n\\n\\n\\n\\n\\n\\n3\\n\\n\\n\\n\\n\\n\\n4\\n\\n\\n\\n\\n\\n\\n5\\n\\n\\n\\n\\n\\n\\n6\\n\\n\\n\\n\\n\\n\\n7\\n\\n\\n\\n\\n\\n\\n8\\n\\n\\n\\n\\n\\nThe expectation of  conditioned on the event that  lies in an interval  is given by\\n\\n\\nwhere  and  respectively are the density and the cumulative distribution function of . For  this is known as the inverse Mills ratio. Note that above, density  of  is used instead of standard normal density as in inverse Mills ratio, so here we have  instead of .\\n\\nFourier transform and characteristic function\\nThe Fourier transform of a normal density  with mean  and standard deviation  is\\n\\n\\nwhere  is the imaginary unit. If the mean , the first factor is 1, and the Fourier transform is, apart from a constant factor, a normal density on the frequency domain, with mean 0 and standard deviation . In particular, the standard normal distribution  is an eigenfunction of the Fourier transform.\\nIn probability theory, the Fourier transform of the probability distribution of a real-valued random variable  is closely connected to the characteristic function  of that variable, which is defined as the expected value of , as a function of the real variable  (the frequency parameter of the Fourier transform). This definition can be analytically extended to a complex-value variable . The relation between both is:\\n\\n\\nMoment and cumulant generating functions\\nThe moment generating function of a real random variable  is the expected value of , as a function of the real parameter . For a normal distribution with density , mean  and deviation , the moment generating function exists and is equal to\\n\\n\\nThe cumulant generating function is the logarithm of the moment generating function, namely\\n\\n\\nSince this is a quadratic polynomial in , only the first two cumulants are nonzero, namely the mean and the variance.\\n\\nStein operator and class\\nWithin Stein\\'s method the Stein operator  and class of a random variable   are  and .\\n\\nZero-variance limit\\nIn the limit when  tends to zero, the probability density  eventually tends to zero at any , but grows without limit if , while its integral remains equal to 1. Therefore, the normal distribution cannot be defined as an ordinary function when .\\nHowever, one can define the normal distribution with zero variance as a generalized function; specifically, as Dirac\\'s \"delta function\"  translated by the mean , that is \\nIts CDF is then the Heaviside step function translated by the mean , namely\\n\\n\\n\\n\\nMaximum entropy\\nOf all probability distributions over the reals with a specified mean  and variance, the normal distribution  is the one with maximum entropy. If  is a continuous random variable with probability density , then the entropy of  is defined as\\n\\n\\nwhere  is understood to be zero whenever . This functional can be maximized, subject to the constraints that the distribution is properly normalized and has a specified variance, by using variational calculus. A function with two Lagrange multipliers is defined:\\n\\n\\nwhere  is, for now, regarded as some density function with mean  and standard deviation .\\nAt maximum entropy, a small variation  about  will produce a variation  about  which is equal to 0:\\n\\n\\nSince this must hold for any small , the term in brackets must be zero, and solving for  yields:\\n\\n\\nUsing the constraint equations to solve for  and  yields the density of the normal distribution:\\n\\n\\nThe entropy of normal distribution equals to\\n\\n\\nOperations on normal deviates\\nThe family of normal distributions is closed under linear transformations: if   is normally distributed with mean  and standard deviation , then the variable  , for any real numbers  and , is also normally distributed, with\\nmean  and standard deviation .\\nAlso if  and  are two independent normal random variables, with means ,  and standard deviations , , then their sum  will also be normally distributed,[proof] with mean  and variance .\\nIn particular, if  and  are independent normal deviates with zero mean and variance , then  and  are also independent and normally distributed, with zero mean and variance . This is a special case of the polarization identity.\\nAlso, if ,  are two independent normal deviates with mean  and deviation , and ,  are arbitrary real numbers, then the variable\\n\\n\\nis also normally distributed with mean  and deviation . It follows that the normal distribution is stable (with exponent ).\\nMore generally, any linear combination of independent normal deviates is a normal deviate.\\n\\nInfinite divisibility and Cramr\\'s theorem\\nFor any positive integer , any normal distribution with mean  and variance  is the distribution of the sum of  independent normal deviates, each with mean  and variance .  This property is called infinite divisibility.\\nConversely, if  and  are independent random variables and their sum  has a normal distribution, then both  and  must be normal deviates.\\nThis result is known as Cramrs decomposition theorem, and is equivalent to saying that the convolution of two distributions is normal if and only if both are normal. Cramr\\'s theorem implies that a linear combination of independent non-Gaussian variables will never have an exactly normal distribution, although it may approach it arbitrarily closely.\\n\\nBernstein\\'s theorem\\nBernstein\\'s theorem states that if  and  are independent and  and  are also independent, then both X and Y must necessarily have normal distributions.\\nMore generally, if  are independent random variables, then two distinct linear combinations  and will be independent if and only if all  are normal and , where  denotes the variance of .\\n\\nOther properties\\nIf the characteristic function  of some random variable  is of the form , where  is a polynomial, then the Marcinkiewicz theorem (named after Jzef Marcinkiewicz) asserts that  can be at most a quadratic polynomial, and therefore  is a normal random variable. The consequence of this result is that the normal distribution is the only distribution with a finite number (two) of non-zero cumulants.If  and  are jointly normal and uncorrelated, then they are independent. The requirement that  and  should be jointly normal is essential; without it the property does not hold.[proof] For non-normal random variables uncorrelatedness does not imply independence.The KullbackLeibler divergence of one normal distribution  from another  is given by:\\n\\nThe Hellinger distance between the same distributions is equal to\\n\\nThe Fisher information matrix for a normal distribution is diagonal and takes the form\\nThe conjugate prior of the mean of a normal distribution is another normal distribution. Specifically, if  are iid  and the prior is , then the posterior distribution for the estimator of  will be\\nThe family of normal distributions not only forms an exponential family (EF), but in fact forms a natural exponential family (NEF) with quadratic variance function (NEF-QVF). Many properties of normal distributions generalize to properties of NEF-QVF distributions, NEF distributions, or EF distributions generally. NEF-QVF distributions comprises 6 families, including Poisson, Gamma, binomial, and negative binomial distributions, while many of the common families studied in probability and statistics are NEF or EF.In information geometry, the family of normal distributions forms a statistical manifold with constant curvature . The same family is flat with respect to the (1)-connections  and .\\nRelated distributions\\nCentral limit theorem\\n As the number of discrete events increases, the function begins to resemble a normal distribution\\n Comparison of probability density functions,  for the sum of  fair 6-sided dice to show their convergence to a normal distribution with increasing , in accordance to the central limit theorem. In the bottom-right graph, smoothed profiles of the previous graphs are rescaled, superimposed and compared with a normal distribution (black curve).\\n\\nThe central limit theorem states that under certain (fairly common) conditions, the sum of many random variables will have an approximately normal distribution. More specifically, where  are independent and identically distributed random variables with the same arbitrary distribution, zero mean, and variance  and  is their\\nmean scaled by \\n\\n\\nThen, as  increases, the probability distribution of  will tend to the normal distribution with zero mean and variance .\\nThe theorem can be extended to variables  that are not independent and/or not identically distributed if certain constraints are placed on the degree of dependence and the moments of the distributions.\\nMany test statistics, scores, and estimators encountered in practice contain sums of certain random variables in them, and even more estimators can be represented as sums of random variables through the use of influence functions.  The central limit theorem implies that those statistical parameters will have asymptotically normal distributions.\\nThe central limit theorem also implies that certain distributions can be approximated by the normal distribution, for example:\\n\\nThe binomial distribution  is approximately normal with mean  and variance  for large  and for  not too close to 0 or 1.\\nThe Poisson distribution with parameter  is approximately normal with mean  and variance , for large values of .\\nThe chi-squared distribution  is approximately normal with mean  and variance , for large .\\nThe Student\\'s t-distribution  is approximately normal with mean 0 and variance 1 when  is large.\\nWhether these approximations are sufficiently accurate depends on the purpose for which they are needed, and the rate of convergence to the normal distribution. It is typically the case that such approximations are less accurate in the tails of the distribution.\\nA general upper bound for the approximation error in the central limit theorem is given by the BerryEsseen theorem, improvements of the approximation are given by the Edgeworth expansions.\\n\\nOperations on a single random variable\\nIf X is distributed normally with mean  and variance 2, then\\n\\nThe exponential of X is distributed log-normally: eX ~ ln(N (, 2)).\\nThe absolute value of X has folded normal distribution: |X| ~ Nf (, 2). If  = 0 this is known as the half-normal distribution.\\nThe absolute value of normalized residuals, |X  |/, has chi distribution with one degree of freedom: |X  |/ ~ .\\nThe square of X/ has the noncentral chi-squared distribution with one degree of freedom: X2/2 ~ (2/2). If  = 0, the distribution is called simply chi-squared.\\nThe distribution of the variable X restricted to an interval [a, b] is called the truncated normal distribution.\\n(X  )2 has a Lvy distribution with location 0 and scale 2.\\nCombination of two independent random variables\\nIf  and  are two independent standard normal random variables with mean 0 and variance 1, then\\n\\nTheir sum and difference is distributed normally with mean zero and variance two: .\\nTheir product  follows the \"product-normal\" distribution with density function  where  is the modified Bessel function of the second kind. This distribution is symmetric around zero, unbounded at , and has the characteristic function .\\nTheir ratio follows the standard Cauchy distribution: .\\nTheir Euclidean norm  has the Rayleigh distribution.\\nCombination of two or more independent random variables\\nIf  are independent standard normal random variables, then the sum of their squares has the chi-squared distribution with  degrees of freedom\\n\\nIf  are independent normally distributed random variables with means  and variances , then their sample mean is independent from the sample standard deviation, which can be demonstrated using Basu\\'s theorem or Cochran\\'s theorem. The ratio of these two quantities will have the Student\\'s t-distribution with  degrees of freedom:\\n\\nIf ,  are independent standard normal random variables, then the ratio of their normalized sums of squares will have the F-distribution with  degrees of freedom:\\n\\nOperations on the density function\\nThe split normal distribution is most directly defined in terms of joining scaled sections of the density functions of different normal distributions and rescaling the density to integrate to one.  The truncated normal distribution results from rescaling a section of a single density function.\\n\\nExtensions\\nThe notion of normal distribution, being one of the most important distributions in probability theory, has been extended far beyond the standard framework of the univariate (that is one-dimensional) case (Case 1). All these extensions are also called normal or Gaussian laws, so a certain ambiguity in names exists.\\n\\nThe multivariate normal distribution describes the Gaussian law in the k-dimensional Euclidean space. A vector X  Rk is multivariate-normally distributed if any linear combination of its components kj=1aj Xj has a (univariate) normal distribution. The variance of X is a kk symmetric positive-definite matrixV. The multivariate normal distribution is a special case of the elliptical distributions. As such, its iso-density loci in the k = 2 case are ellipses and in the case of arbitrary k are ellipsoids.\\nRectified Gaussian distribution a rectified version of normal distribution with all the negative elements reset to 0\\nComplex normal distribution deals with the complex normal vectors. A complex vector X  Ck is said to be normal if both its real and imaginary components jointly possess a 2k-dimensional multivariate normal distribution. The variance-covariance structure of X is described by two matrices: the variance matrix, and the relation matrixC.\\nMatrix normal distribution describes the case of normally distributed matrices.\\nGaussian processes are the normally distributed stochastic processes. These can be viewed as elements of some infinite-dimensional Hilbert spaceH, and thus are the analogues of multivariate normal vectors for the case k = . A random element h  H is said to be normal if for any constant a  H the scalar product (a, h) has a (univariate) normal distribution. The variance structure of such Gaussian random element can be described in terms of the linear covariance operator K: H  H. Several Gaussian processes became popular enough to have their own names:\\nBrownian motion,\\nBrownian bridge,\\nOrnsteinUhlenbeck process.\\nGaussian q-distribution is an abstract mathematical construction that represents a \"q-analogue\" of the normal distribution.\\nthe q-Gaussian is an analogue of the Gaussian distribution, in the sense that it maximises the Tsallis entropy, and is one type of Tsallis distribution. Note that this distribution is different from the Gaussian q-distribution above.\\nA random variable X has a two-piece normal distribution if it has a distribution\\n\\n\\n\\nwhere  is the mean and 1 and 2 are the standard deviations of the distribution to the left and right of the mean respectively.\\nThe mean, variance and third central moment of this distribution have been determined\\n\\n\\n\\n\\nwhere E(X), V(X) and T(X) are the mean, variance, and third central moment respectively.\\nOne of the main practical uses of the Gaussian law is to model the empirical distributions of many different random variables encountered in practice. In such case a possible extension would be a richer family of distributions, having more than two parameters and therefore being able to fit the empirical distribution more accurately. The examples of such extensions are:\\n\\nPearson distribution  a four-parameter family of probability distributions that extend the normal law to include different skewness and kurtosis values.\\nThe generalized normal distribution, also known as the exponential power distribution, allows for distribution tails with thicker or thinner asymptotic behaviors.\\n\\n\\nStatistical Inference\\nEstimation of parameters\\n\\nIt is often the case that we don\\'t know the parameters of the normal distribution, but instead want to estimate them. That is, having a sample  from a normal  population we would like to learn the approximate values of parameters  and . The standard approach to this problem is the maximum likelihood method, which requires maximization of the log-likelihood function:\\n\\n\\nTaking derivatives with respect to  and  and solving the resulting system of first order conditions yields the maximum likelihood estimates:\\n\\n\\nSample mean\\n\\nEstimator  is called the sample mean, since it is the arithmetic mean of all observations. The statistic  is complete and sufficient for , and therefore by the LehmannScheff theorem,  is the uniformly minimum variance unbiased (UMVU) estimator. In finite samples it is distributed normally:\\n\\n\\nThe variance of this estimator is equal to the -element of the inverse Fisher information matrix . This implies that the estimator is finite-sample efficient. Of practical importance is the fact that the standard error of  is proportional to , that is, if one wishes to decrease the standard error by a factor of 10, one must increase the number of points in the sample by a factor of 100. This fact is widely used in determining sample sizes for opinion polls and the number of trials in Monte Carlo simulations.\\nFrom the standpoint of the asymptotic theory,  is consistent, that is, it converges in probability to  as . The estimator is also asymptotically normal, which is a simple corollary of the fact that it is normal in finite samples:\\n\\n\\nSample variance\\n\\nThe estimator  is called the sample variance, since it is the variance of the sample (). In practice, another estimator is often used instead of the . This other estimator is denoted  , and is also called the sample variance, which represents a certain ambiguity in terminology; its square root  is called the sample standard deviation. The estimator  differs from  by having (n  1) instead ofn in the denominator (the so-called Bessel\\'s correction):\\n\\n\\nThe difference between  and  becomes negligibly small for large n\\'s. In finite samples however, the motivation behind the use of  is that it is an unbiased estimator of the underlying parameter , whereas  is biased. Also, by the LehmannScheff theorem the estimator  is uniformly minimum variance unbiased (UMVU), which makes it the \"best\" estimator among all unbiased ones. However it can be shown that the biased estimator  is \"better\" than the  in terms of the mean squared error (MSE) criterion. In finite samples both  and  have scaled chi-squared distribution with (n  1) degrees of freedom:\\n\\n\\nThe first of these expressions shows that the variance of  is equal to , which is slightly greater than the -element of the inverse Fisher information matrix . Thus,  is not an efficient estimator for , and moreover, since  is UMVU, we can conclude that the finite-sample efficient estimator for  does not exist.\\nApplying the asymptotic theory, both estimators  and  are consistent, that is they converge in probability to  as the sample size . The two estimators are also both asymptotically normal:\\n\\n\\nIn particular, both estimators are asymptotically efficient for  .\\n\\nConfidence intervals\\n\\nBy Cochran\\'s theorem, for normal distributions the sample mean  and the sample variance s2 are independent, which means there can be no gain in considering their joint distribution. There is also a converse theorem: if in a sample the sample mean and sample variance are independent, then the sample must have come from the normal distribution. The independence between  and s can be employed to construct the so-called t-statistic:\\n\\n\\nThis quantity t has the Student\\'s t-distribution with (n  1) degrees of freedom, and it is an ancillary statistic (independent of the value of the parameters). Inverting the distribution of this t-statistics will allow us to construct the confidence interval for ; similarly, inverting the 2 distribution of the statistic s2 will give us the confidence interval for 2:\\n\\n\\n\\nwhere tk,p and 2k,p are the pth quantiles of the t- and 2-distributions respectively. These confidence intervals are of the confidence level 1  , meaning that the true values  and 2 fall outside of these intervals with probability (or significance level) . In practice people usually take  = 5%, resulting in the 95% confidence intervals. The approximate formulas in the display above were derived from the asymptotic distributions of  and s2. The approximate formulas become valid for large values of n, and are more convenient for the manual calculation since the standard normal quantiles z/2 do not depend on n. In particular, the most popular value of  = 5%, results in |z0.025| = 1.96.\\n\\n\\nNormality tests\\n\\nNormality tests assess the likelihood that the given data set {x1, ..., xn} comes from a normal distribution. Typically the null hypothesis H0 is that the observations are distributed normally with unspecified mean  and variance 2, versus the alternative Ha that the distribution is arbitrary. Many tests (over 40) have been devised for this problem, the more prominent of them are outlined below:\\n\\n\"Visual\" tests are more intuitively appealing but subjective at the same time, as they rely on informal human judgement to accept or reject the null hypothesis.\\nQ-Q plot is a plot of the sorted values from the data set against the expected values of the corresponding quantiles from the standard normal distribution. That is, it\\'s a plot of point of the form (1(pk), x(k)), where plotting points pk are equal to pk=(k)/(n+12) and  is an adjustment constant, which can be anything between 0 and1. If the null hypothesis is true, the plotted points should approximately lie on a straight line.\\nP-P plot similar to the Q-Q plot, but used much less frequently. This method consists of plotting the points ((z(k)), pk), where . For normally distributed data this plot should lie on a 45 line between (0,0) and(1,1).\\nShapiro-Wilk test employs the fact that the line in the Q-Q plot has the slope of . The test compares the least squares estimate of that slope with the value of the sample variance, and rejects the null hypothesis if these two quantities differ significantly.\\nNormal probability plot (rankit plot)\\nMoment tests:\\nD\\'Agostino\\'s K-squared test\\nJarqueBera test\\nEmpirical distribution function tests:\\nLilliefors test (an adaptation of the KolmogorovSmirnov test)\\nAndersonDarling test\\nBayesian analysis of the normal distribution\\nBayesian analysis of normally distributed data is complicated by the many different possibilities that may be considered:\\n\\nEither the mean, or the variance, or neither, may be considered a fixed quantity.\\nWhen the variance is unknown, analysis may be done directly in terms of the variance, or in terms of the precision, the reciprocal of the variance.  The reason for expressing the formulas in terms of precision is that the analysis of most cases is simplified.\\nBoth univariate and multivariate cases need to be considered.\\nEither conjugate or improper prior distributions may be placed on the unknown variables.\\nAn additional set of cases occurs in Bayesian linear regression, where in the basic model the data is assumed to be normally distributed, and normal priors are placed on the regression coefficients. The resulting analysis is similar to the basic cases of independent identically distributed data, but more complex.\\nThe formulas for the non-linear-regression cases are summarized in the conjugate prior article.\\n\\nSum of two quadratics\\nScalar form\\nThe following auxiliary formula is useful for simplifying the posterior update equations, which otherwise become fairly tedious.\\n\\n\\nThis equation rewrites the sum of two quadratics in x by expanding the squares, grouping the terms in x, and completing the square.  Note the following about the complex constant factors attached to some of the terms:\\n\\nThe factor  has the form of a weighted average of y and z.\\n This shows that this factor can be thought of as resulting from a situation where the reciprocals of quantities a and b add directly, so to combine a and b themselves, it\\'s necessary to reciprocate, add, and reciprocate the result again to get back into the original units.  This is exactly the sort of operation performed by the harmonic mean, so it is not surprising that  is one-half the harmonic mean of a and b.\\nVector form\\nA similar formula can be written for the sum of two vector quadratics: If x, y, z are vectors of length k, and A and B are symmetric, invertible matrices of size , then\\n\\n\\nwhere\\n\\n\\nNote that the form x A x is called a quadratic form and is a scalar:\\n\\n\\nIn other words, it sums up all possible combinations of products of pairs of elements from x, with a separate coefficient for each.  In addition, since , only the sum  matters for any off-diagonal elements of A, and there is no loss of generality in assuming that A is symmetric.  Furthermore, if A is symmetric, then the form \\n\\nSum of differences from the mean\\nAnother useful formula is as follows:\\n\\n\\nwhere \\n\\nWith known variance\\nFor a set of i.i.d. normally distributed data points X of size n where each individual point x follows  with known variance 2, the conjugate prior distribution is also normally distributed.\\nThis can be shown more easily by rewriting the variance as the precision, i.e. using  = 1/2. Then if  and  we proceed as follows.\\nFirst, the likelihood function is (using the formula above for the sum of differences from the mean):\\n\\n\\nThen, we proceed as follows:\\n\\n\\nIn the above derivation, we used the formula above for the sum of two quadratics and eliminated all constant factors not involving.  The result is the kernel of a normal distribution, with mean  and precision , i.e.\\n\\n\\nThis can be written as a set of Bayesian update equations for the posterior parameters in terms of the prior parameters:\\n\\n\\nThat is, to combine n data points with total precision of n (or equivalently, total variance of n/2) and mean of values , derive a new total precision simply by adding the total precision of the data to the prior total precision, and form a new mean through a precision-weighted average, i.e. a weighted average of the data mean and the prior mean, each weighted by the associated total precision. This makes logical sense if the precision is thought of as indicating the certainty of the observations: In the distribution of the posterior mean, each of the input components is weighted by its certainty, and the certainty of this distribution is the sum of the individual certainties. (For the intuition of this, compare the expression \"the whole is (or is not) greater than the sum of its parts\".  In addition, consider that the knowledge of the posterior comes from a combination of the knowledge of the prior and likelihood, so it makes sense that we are more certain of it than of either of its components.)\\nThe above formula reveals why it is more convenient to do Bayesian analysis of conjugate priors for the normal distribution in terms of the precision.  The posterior precision is simply the sum of the prior and likelihood precisions, and the posterior mean is computed through a precision-weighted average, as described above.  The same formulas can be written in terms of variance by reciprocating all the precisions, yielding the more ugly formulas\\n\\n\\nWith known mean\\nFor a set of i.i.d. normally distributed data points X of size n where each individual point x follows  with known mean , the conjugate prior of the variance has an inverse gamma distribution or a scaled inverse chi-squared distribution.  The two are equivalent except for having different parameterizations.  Although the inverse gamma is more commonly used, we use the scaled inverse chi-squared for the sake of convenience.  The prior for 2 is as follows:\\n\\n\\nThe likelihood function from above, written in terms of the variance, is:\\n\\n\\nwhere\\n\\n\\nThen:\\n\\n\\nThe above is also a scaled inverse chi-squared distribution where\\n\\n\\nor equivalently\\n\\n\\nReparameterizing in terms of an inverse gamma distribution, the result is:\\n\\n\\nWith unknown mean and unknown variance\\nFor a set of i.i.d. normally distributed data points X of size n where each individual point x follows  with unknown mean  and unknown variance 2, a combined (multivariate) conjugate prior is placed over the mean and variance, consisting of a normal-inverse-gamma distribution.\\nLogically, this originates as follows:\\n\\nFrom the analysis of the case with unknown mean but known variance, we see that the update equations involve sufficient statistics computed from the data consisting of the mean of the data points and the total variance of the data points, computed in turn from the known variance divided by the number of data points.\\nFrom the analysis of the case with unknown variance but known mean, we see that the update equations involve sufficient statistics over the data consisting of the number of data points and sum of squared deviations.\\nKeep in mind that the posterior update values serve as the prior distribution when further data is handled.  Thus, we should logically think of our priors in terms of the sufficient statistics just described, with the same semantics kept in mind as much as possible.\\nTo handle the case where both mean and variance are unknown, we could place independent priors over the mean and variance, with fixed estimates of the average mean, total variance, number of data points used to compute the variance prior, and sum of squared deviations.  Note however that in reality, the total variance of the mean depends on the unknown variance, and the sum of squared deviations that goes into the variance prior (appears to) depend on the unknown mean.  In practice, the latter dependence is relatively unimportant: Shifting the actual mean shifts the generated points by an equal amount, and on average the squared deviations will remain the same.  This is not the case, however, with the total variance of the mean: As the unknown variance increases, the total variance of the mean will increase proportionately, and we would like to capture this dependence.\\nThis suggests that we create a conditional prior of the mean on the unknown variance, with a hyperparameter specifying the mean of the pseudo-observations associated with the prior, and another parameter specifying the number of pseudo-observations.  This number serves as a scaling parameter on the variance, making it possible to control the overall variance of the mean relative to the actual variance parameter.  The prior for the variance also has two hyperparameters, one specifying the sum of squared deviations of the pseudo-observations associated with the prior, and another specifying once again the number of pseudo-observations.  Note that each of the priors has a hyperparameter specifying the number of pseudo-observations, and in each case this controls the relative variance of that prior.  These are given as two separate hyperparameters so that the variance (aka the confidence) of the two priors can be controlled separately.\\nThis leads immediately to the normal-inverse-gamma distribution, which is the product of the two distributions just defined, with conjugate priors used (an inverse gamma distribution over the variance, and a normal distribution over the mean, conditional on the variance) and with the same four parameters just defined.\\nThe priors are normally defined as follows:\\n\\n\\nThe update equations can be derived, and look as follows:\\n\\n\\nThe respective numbers of pseudo-observations add the number of actual observations to them.  The new mean hyperparameter is once again a weighted average, this time weighted by the relative numbers of observations.  Finally, the update for  is similar to the case with known mean, but in this case the sum of squared deviations is taken with respect to the observed data mean rather than the true mean, and as a result a new \"interaction term\" needs to be added to take care of the additional error source stemming from the deviation between prior and data mean.\\n\\n[Proof]\\nThe prior distributions are\\n\\n\\nTherefore, the joint prior is\\n\\n\\nThe likelihood function from the section above with known variance is:\\n\\n\\nWriting it in terms of variance rather than precision, we get:\\n\\n\\nwhere \\nTherefore, the posterior is (dropping the hyperparameters as conditioning factors):\\n\\n\\nIn other words, the posterior distribution has the form of a product of a normal distribution over p(|2) times an inverse gamma distribution over p(2), with parameters that are the same as the update equations above.\\n\\n\\nOccurrence and applications\\nThe occurrence of normal distribution in practical problems can be loosely classified into four categories:\\n\\nExactly normal distributions;\\nApproximately normal laws, for example when such approximation is justified by the central limit theorem; and\\nDistributions modeled as normal  the normal distribution being the distribution with maximum entropy for a given mean and variance.\\nRegression problems  the normal distribution being found after systematic effects have been modeled sufficiently well.\\nExact normality\\n The ground state of a quantum harmonic oscillator has the Gaussian distribution.\\nCertain quantities in physics are distributed normally, as was first demonstrated by James Clerk Maxwell. Examples of such quantities are:\\n\\nProbability density function of a ground state in a quantum harmonic oscillator.\\nThe position of a particle that experiences diffusion. If initially the particle is located at a specific point (that is its probability distribution is the dirac delta function), then after time t its location is described by a normal distribution with variance t, which satisfies the diffusion equation. If the initial location is given by a certain density function , then the density at time t is the convolution of g and the normal PDF.\\nApproximate normality\\nApproximately normal distributions occur in many situations, as explained by the central limit theorem. When the outcome is produced by many small effects acting additively and independently, its distribution will be close to normal. The normal approximation will not be valid if the effects act multiplicatively (instead of additively), or if there is a single external influence that has a considerably larger magnitude than the rest of the effects.\\n\\nIn counting problems, where the central limit theorem includes a discrete-to-continuum approximation and where infinitely divisible and decomposable distributions are involved, such as\\nBinomial random variables, associated with binary response variables;\\nPoisson random variables, associated with rare events;\\nThermal radiation has a BoseEinstein distribution on very short time scales, and a normal distribution on longer timescales due to the central limit theorem.\\nAssumed normality\\n Histogram of sepal widths for Iris versicolor from Fisher\\'s Iris flower data set, with superimposed best-fitting normal distribution.\\nI can only recognize the occurrence of the normal curve  the Laplacian curve of errors  as a very abnormal phenomenon. It is roughly approximated to in certain distributions; for this reason, and on account for its beautiful simplicity, we may, perhaps, use it as a first approximation, particularly in theoretical investigations.Pearson (1901)\\n\\nThere are statistical methods to empirically test that assumption, see the above Normality tests section.\\n\\nIn biology, the logarithm of various variables tend to have a normal distribution, that is, they tend to have a log-normal distribution (after separation on male/female subpopulations), with examples including:\\nMeasures of size of living tissue (length, height, skin area, weight);\\nThe length of inert appendages (hair, claws, nails, teeth) of biological specimens, in the direction of growth; presumably the thickness of tree bark also falls under this category;\\nCertain physiological measurements, such as blood pressure of adult humans.\\nIn finance, in particular the BlackScholes model, changes in the logarithm of exchange rates, price indices, and stock market indices are assumed normal (these variables behave like compound interest, not like simple interest, and so are multiplicative). Some mathematicians such as Benoit Mandelbrot have argued that log-Levy distributions, which possesses heavy tails would be a more appropriate model, in particular for the analysis for stock market crashes. The use of the assumption of normal distribution occurring in financial models has also been criticized by Nassim Nicholas Taleb in his works.\\nMeasurement errors in physical experiments are often modeled by a normal distribution. This use of a normal distribution does not imply that one is assuming the measurement errors are normally distributed, rather using the normal distribution produces the most conservative predictions possible given only knowledge about the mean and variance of the errors.\\nIn standardized testing, results can be made to have a normal distribution by either selecting the number and difficulty of questions (as in the IQ test) or transforming the raw test scores into \"output\" scores by fitting them to the normal distribution. For example, the SAT\\'s traditional range of 200800 is based on a normal distribution with a mean of 500 and a standard deviation of 100.\\n Fitted cumulative normal distribution to October rainfalls, see distribution fitting\\nMany scores are derived from the normal distribution, including percentile ranks (\"percentiles\" or \"quantiles\"), normal curve equivalents, stanines, z-scores, and T-scores. Additionally, some behavioral statistical procedures assume that scores are normally distributed; for example, t-tests and ANOVAs. Bell curve grading assigns relative grades based on a normal distribution of scores.\\nIn hydrology the distribution of long duration river discharge or rainfall, e.g. monthly and yearly totals, is often thought to be practically normal according to the central limit theorem. The blue picture, made with CumFreq, illustrates an example of fitting the normal distribution to ranked October rainfalls showing the 90% confidence belt based on the binomial distribution. The rainfall data are represented by plotting positions as part of the cumulative frequency analysis.\\nProduced normality\\nIn regression analysis, lack of normality in residuals simply indicates that the model postulated is inadequate in accounting for the tendency in the data and needs to be augmented; in other words, normality in residuals can always be achieved given a properly constructed model.\\n\\nComputational methods\\nGenerating values from normal distribution\\n The bean machine, a device invented by Francis Galton, can be called the first generator of normal random variables. This machine consists of a vertical board with interleaved rows of pins. Small balls are dropped from the top and then bounce randomly left or right as they hit the pins. The balls are collected into bins at the bottom and settle down into a pattern resembling the Gaussian curve.\\nIn computer simulations, especially in applications of the Monte-Carlo method, it is often desirable to generate values that are normally distributed. The algorithms listed below all generate the standard normal deviates, since a N(, 2) can be generated as X =  + Z, where Z is standard normal. All these algorithms rely on the availability of a random number generator U capable of producing uniform random variates.\\n\\nThe most straightforward method is based on the probability integral transform property: if U is distributed uniformly on (0,1), then 1(U) will have the standard normal distribution. The drawback of this method is that it relies on calculation of the probit function 1, which cannot be done analytically. Some approximate methods are described in Hart (1968) and in the erf article. Wichura gives a fast algorithm for computing this function to 16 decimal places, which is used by R to compute random variates of the normal distribution.\\nAn easy to program approximate approach, that relies on the central limit theorem, is as follows: generate 12 uniform U(0,1) deviates, add them all up, and subtract 6  the resulting random variable will have approximately standard normal distribution. In truth, the distribution will be IrwinHall, which is a 12-section eleventh-order polynomial approximation to the normal distribution. This random deviate will have a limited range of (6,6).\\nThe BoxMuller method uses two independent random numbers U and V distributed uniformly on (0,1). Then the two random variables X and Y\\n\\nwill both have the standard normal distribution, and will be independent. This formulation arises because for a bivariate normal random vector (X, Y) the squared norm X2 + Y2 will have the chi-squared distribution with two degrees of freedom, which is an easily generated exponential random variable corresponding to the quantity 2ln(U) in these equations; and the angle is distributed uniformly around the circle, chosen by the random variable V.\\nThe Marsaglia polar method is a modification of the BoxMuller method which does not require computation of the sine and cosine functions. In this method, U and V are drawn from the uniform (1,1) distribution, and then S = U2 + V2 is computed. If S is greater or equal to 1, then the method starts over, otherwise the two quantities\\n\\nare returned. Again, X and Y are independent, standard normal random variables.\\nThe Ratio method is a rejection method. The algorithm proceeds as follows:\\nGenerate two independent uniform deviates U and V;\\nCompute X = 8/e (V  0.5)/U;\\nOptional: if X2  5  4e1/4U then accept X and terminate algorithm;\\nOptional: if X2  4e1.35/U + 1.4 then reject X and start over from step 1;\\nIf X2  4 lnU then accept X, otherwise start over the algorithm.\\nThe two optional steps allow the evaluation of the logarithm in the last step to be avoided in most cases.  These steps can be greatly improved so that the logarithm is rarely evaluated.\\nThe ziggurat algorithm is faster than the BoxMuller transform and still exact. In about 97% of all cases it uses only two random numbers, one random integer and one random uniform, one multiplication and an if-test. Only in 3% of the cases, where the combination of those two falls outside the \"core of the ziggurat\" (a kind of rejection sampling using logarithms), do exponentials and more uniform random numbers have to be employed.\\nInteger arithmetic can be used to sample from the standard normal distribution.  This method is exact in the sense that it satisfies the conditions of ideal approximation; i.e., it is equivalent to sampling a real number from the standard normal distribution and rounding this to the nearest representable floating point number.\\nThere is also some investigation into the connection between the fast Hadamard transform and the normal distribution, since the transform employs just addition and subtraction and by the central limit theorem random numbers from almost any distribution will be transformed into the normal distribution. In this regard a series of Hadamard transforms can be combined with random permutations to turn arbitrary data sets into a normally distributed data.\\nNumerical approximations for the normal CDF\\nThe standard normal CDF is widely used in scientific and statistical computing.\\nThe values (x) may be approximated very accurately by a variety of methods, such as numerical integration, Taylor series, asymptotic series and continued fractions. Different approximations are used depending on the desired level of accuracy.\\n\\nZelen & Severo (1964) give the approximation for (x) for x > 0 with the absolute error |(x)|<7.5108 (algorithm 26.2.17):\\n\\nwhere (x) is the standard normal PDF, and b0 = 0.2316419, b1 = 0.319381530, b2 = 0.356563782, b3 =  1.781477937, b4 = 1.821255978, b5 = 1.330274429.Hart (1968) lists some dozens of approximations  by means of rational functions, with or without exponentials  for the erfc() function. His algorithms vary in the degree of complexity and the resulting precision, with maximum absolute precision of 24 digits. An algorithm by West (2009) combines Hart\\'s algorithm 5666 with a continued fraction approximation in the tail to provide a fast computation algorithm with a 16-digit precision.Cody (1969) after recalling Hart68 solution is not suited for erf, gives a solution for both erf and erfc, with maximal relative error bound, via Rational Chebyshev Approximation.Marsaglia (2004) suggested a simple algorithm based on the Taylor series expansion\\n\\n\\n\\nfor calculating (x) with arbitrary precision. The drawback of this algorithm is comparatively slow calculation time (for example it takes over 300 iterations to calculate the function with 16 digits of precision when x = 10).The GNU Scientific Library calculates values of the standard normal CDF using Hart\\'s algorithms and approximations with Chebyshev polynomials.\\nShore (1982) introduced simple approximations that may be incorporated in stochastic optimization models of engineering and operations research, like reliability engineering and inventory analysis. Denoting p=(z), the simplest approximation for the quantile function is:\\n\\n\\nThis approximation delivers for z a maximum absolute error of 0.026 (for 0.5p0.9999, corresponding to 0z3.719). For p<1/2 replace p by 1p and change sign. Another approximation, somewhat less accurate, is the single-parameter approximation:\\n\\n\\nThe latter had served to derive a simple approximation for the loss integral of the normal distribution, defined by\\n\\n\\nThis approximation is particularly accurate for the right far-tail (maximum error of 103 for z1.4). Highly accurate approximations for the CDF, based on Response Modeling Methodology (RMM, Shore, 2011, 2012), are shown in Shore (2005).\\nSome more approximations can be found at: Error function#Approximation with elementary functions. In particular, small relative error on the whole domain for the CDF  and the quantile function  as well, is achieved via an explicitly invertible formula by Sergei Winitzki in 2008.\\n\\nHistory\\nDevelopment\\nSome authors attribute the credit for the discovery of the normal distribution to de Moivre, who in 1738 published in the second edition of his \"The Doctrine of Chances\" the study of the coefficients in the binomial expansion of (a + b)n. De Moivre proved that the middle term in this expansion has the approximate magnitude of , and that \"If m or n be a Quantity infinitely great, then the Logarithm of the Ratio, which a Term distant from the middle by the Interval , has to the middle Term, is .\" Although this theorem can be interpreted as the first obscure expression for the normal probability law, Stigler points out that de Moivre himself did not interpret his results as anything more than the approximate rule for the binomial coefficients, and in particular de Moivre lacked the concept of the probability density function.\\n\\n Carl Friedrich Gauss discovered the normal distribution in 1809 as a way to rationalize the method of least squares.\\nIn 1809 Gauss published his monograph \"Theoria motus corporum coelestium in sectionibus conicis solem ambientium\" where among other things he introduces several important statistical concepts, such as the method of least squares, the method of maximum likelihood, and the normal distribution. Gauss used M, M, M, ... to denote the measurements of some unknown quantityV, and sought the \"most probable\" estimator of that quantity: the one that maximizes the probability (MV)  (MV)  (MV)  ... of obtaining the observed experimental results. In his notation  is the probability law of the measurement errors of magnitude . Not knowing what the function  is, Gauss requires that his method should reduce to the well-known answer: the arithmetic mean of the measured values. Starting from these principles, Gauss demonstrates that the only law that rationalizes the choice of arithmetic mean as an estimator of the location parameter, is the normal law of errors:\\n\\n\\nwhere h is \"the measure of the precision of the observations\". Using this normal law as a generic model for errors in the experiments, Gauss formulates what is now known as the non-linear weighted least squares (NWLS) method.\\n\\n Marquis de Laplace proved the central limit theorem in 1810, consolidating the importance of the normal distribution in statistics.\\nAlthough Gauss was the first to suggest the normal distribution law, Laplace made significant contributions. It was Laplace who first posed the problem of aggregating several observations in 1774, although his own solution led to the Laplacian distribution. It was Laplace who first calculated the value of the integral  et2dt =  in 1782, providing the normalization constant for the normal distribution. Finally, it was Laplace who in 1810 proved and presented to the Academy the fundamental central limit theorem, which emphasized the theoretical importance of the normal distribution.\\nIt is of interest to note that in 1809 an American mathematician Adrain published two derivations of the normal probability law, simultaneously and independently from Gauss. His works remained largely unnoticed by the scientific community, until in 1871 they were \"rediscovered\" by Abbe.\\nIn the middle of the 19th century Maxwell demonstrated that the normal distribution is not just a convenient mathematical tool, but may also occur in natural phenomena: \"The number of particles whose velocity, resolved in a certain direction, lies between x and x+dx is\\n\\n\\nNaming\\nSince its introduction, the normal distribution has been known by many different names: the law of error, the law of facility of errors, Laplace\\'s second law, Gaussian law, etc. Gauss himself apparently coined the term with reference to the \"normal equations\" involved in its applications, with normal having its technical meaning of orthogonal rather than \"usual\". However, by the end of the 19th century some authors had started using the name normal distribution, where the word \"normal\" was used as an adjective the term now being seen as a reflection of the fact that this distribution was seen as typical, common and thus \"normal\". Peirce (one of those authors) once defined \"normal\" thus: \"...the \\'normal\\' is not the average (or any other kind of mean) of what actually occurs, but of what would, in the long run, occur under certain circumstances.\" Around the turn of the 20th century Pearson popularized the term normal as a designation for this distribution.\\n\\nMany years ago I called the LaplaceGaussian curve the normal curve, which name, while it avoids an international question of priority, has the disadvantage of leading people to believe that all other distributions of frequency are in one sense or another \\'abnormal\\'. Pearson (1920)\\n\\nAlso, it was Pearson who first wrote the distribution in terms of the standard deviation  as in modern notation. Soon after this, in year 1915, Fisher added the location parameter to the formula for normal distribution, expressing it in the way it is written nowadays:\\n\\n\\nThe term \"standard normal\", which denotes the normal distribution with zero mean and unit variance came into general use around the 1950s, appearing in the popular textbooks by P.G. Hoel (1947) \"Introduction to mathematical statistics\" and A.M. Mood (1950) \"Introduction to the theory of statistics\".\\nWhen the name is used, the \"Gaussian distribution\" was named after Carl Friedrich Gauss, who introduced the distribution in 1809 as a way of rationalizing the method of least squares as outlined above. Among English speakers, both \"normal distribution\" and \"Gaussian distribution\" are in common use, with different terms preferred by different communities.\\n\\n\\n\\nWrapped normal distribution  the Normal distribution applied to a circular domain\\nBates distribution  similar to the IrwinHall distribution, but rescaled back into the 0 to 1 range\\nBehrensFisher problem  the long-standing problem of testing whether two normal samples with different variances have same means;\\nBhattacharyya distance  method used to separate mixtures of normal distributions\\nErdsKac theoremon the occurrence of the normal distribution in number theory\\nGaussian blurconvolution, which uses the normal distribution as a kernel\\nNormally distributed and uncorrelated does not imply independent\\nReciprocal normal distribution\\nRatio normal distribution\\nStandard normal table\\nSub-Gaussian distribution\\nSum of normally distributed random variables\\nTweedie distribution  The normal distribution is a member of the family of Tweedie exponential dispersion models\\nZ-test using the normal distribution\\nStein\\'s lemma\\nNotes\\n\\n\\nCitations\\n\\nSources\\n\\n\\n\\n\\n\\nWikimedia Commons has media related to Normal distribution.\\n\\n\\nNormal distribution calculator, More powerful calculator'\n",
      "\n",
      "https://en.wikipedia.org/wiki/Artificial_intelligence#Approaches \t\t\t\t 10000\n",
      "b'There is no established unifying theory or paradigm that guides AI research. Researchers disagree about many issues. A few of the most long standing questions that have remained unanswered are these: should artificial intelligence simulate natural intelligence by studying psychology or neurobiology? Or is human biology as irrelevant to AI research as bird biology is to aeronautical engineering?\\nCan intelligent behavior be described using simple, elegant principles (such as logic or optimization)? Or does it necessarily require solving a large number of completely unrelated problems?\\n\\nCybernetics and brain simulation\\n\\nIn the 1940s and 1950s, a number of researchers explored the connection between neurobiology, information theory, and cybernetics. Some of them built machines that used electronic networks to exhibit rudimentary intelligence, such as W. Grey Walter\\'s turtles and the Johns Hopkins Beast. Many of these researchers gathered for meetings of the Teleological Society at Princeton University and the Ratio Club in England. By 1960, this approach was largely abandoned, although elements of it would be revived in the 1980s.\\n\\nSymbolic\\n\\nWhen access to digital computers became possible in the mid 1950s, AI research began to explore the possibility that human intelligence could be reduced to symbol manipulation. The research was centered in three institutions: Carnegie Mellon University, Stanford and MIT, and as described below, each one developed its own style of research. John Haugeland named these symbolic approaches to AI \"good old fashioned AI\" or \"GOFAI\". During the 1960s, symbolic approaches had achieved great success at simulating high-level \"thinking\" in small demonstration programs. Approaches based on cybernetics or artificial neural networks were abandoned or pushed into the background.\\nResearchers in the 1960s and the 1970s were convinced that symbolic approaches would eventually succeed in creating a machine with artificial general intelligence and considered this the goal of their field.\\n\\nCognitive simulation\\nEconomist Herbert Simon and Allen Newell studied human problem-solving skills and attempted to formalize them, and their work laid the foundations of the field of artificial intelligence, as well as cognitive science, operations research and management science. Their research team used the results of psychological experiments to develop programs that simulated the techniques that people used to solve problems. This tradition, centered at Carnegie Mellon University would eventually culminate in the development of the Soar architecture in the middle 1980s.\\n\\nLogic-based\\nUnlike Simon and Newell, John McCarthy felt that machines did not need to simulate human thought, but should instead try to find the essence of abstract reasoning and problem-solving, regardless whether people used the same algorithms. His laboratory at Stanford (SAIL) focused on using formal logic to solve a wide variety of problems, including knowledge representation, planning and learning. Logic was also the focus of the work at the University of Edinburgh and elsewhere in Europe which led to the development of the programming language Prolog and the science of logic programming.\\n\\nAnti-logic or scruffy\\nResearchers at MIT (such as Marvin Minsky and Seymour Papert) found that solving difficult problems in vision and natural language processing required ad-hoc solutionsthey argued that there was no simple and general principle (like logic) that would capture all the aspects of intelligent behavior. Roger Schank described their \"anti-logic\" approaches as \"scruffy\" (as opposed to the \"neat\" paradigms at CMU and Stanford). Commonsense knowledge bases (such as Doug Lenat\\'s Cyc) are an example of \"scruffy\" AI, since they must be built by hand, one complicated concept at a time.\\n\\nKnowledge-based\\nWhen computers with large memories became available around 1970, researchers from all three traditions began to build knowledge into AI applications. This \"knowledge revolution\" led to the development and deployment of expert systems (introduced by Edward Feigenbaum), the first truly successful form of AI software. A key component of the system architecture for all expert systems is the knowledge base, which stores facts and rules that illustrate AI. The knowledge revolution was also driven by the realization that enormous amounts of knowledge would be required by many simple AI applications.\\n\\nSub-symbolic\\nBy the 1980s, progress in symbolic AI seemed to stall and many believed that symbolic systems would never be able to imitate all the processes of human cognition, especially perception, robotics, learning and pattern recognition. A number of researchers began to look into \"sub-symbolic\" approaches to specific AI problems. Sub-symbolic methods manage to approach intelligence without specific representations of knowledge.\\n\\nEmbodied intelligence\\nThis includes embodied, situated, behavior-based, and nouvelle AI. Researchers from the related field of robotics, such as Rodney Brooks, rejected symbolic AI and focused on the basic engineering problems that would allow robots to move and survive. Their work revived the non-symbolic point of view of the early cybernetics researchers of the 1950s and reintroduced the use of control theory in AI. This coincided with the development of the embodied mind thesis in the related field of cognitive science: the idea that aspects of the body (such as movement, perception and visualization) are required for higher intelligence.\\nWithin developmental robotics, developmental learning approaches are elaborated upon to allow robots to accumulate repertoires of novel skills through autonomous self-exploration, social interaction with human teachers, and the use of guidance mechanisms (active learning, maturation, motor synergies, etc.).\\n\\nComputational intelligence and soft computing\\nInterest in neural networks and \"connectionism\" was revived by David Rumelhart and others in the middle of the 1980s. Artificial neural networks are an example of soft computingthey are solutions to problems which cannot be solved with complete logical certainty, and where an approximate solution is often sufficient. Other soft computing approaches to AI include fuzzy systems, Grey system theory, evolutionary computation and many statistical tools. The application of soft computing to AI is studied collectively by the emerging discipline of computational intelligence.\\n\\nStatistical learning\\nMuch of traditional GOFAI got bogged down on ad hoc patches to symbolic computation that worked on their own toy models but failed to generalize to real-world results. However, around the 1990s, AI researchers adopted sophisticated mathematical tools, such as hidden Markov models (HMM), information theory, and normative Bayesian decision theory to compare or to unify competing architectures. The shared mathematical language permitted a high level of collaboration with more established fields (like mathematics, economics or operations research). Compared with GOFAI, new \"statistical learning\" techniques such as HMM and neural networks were gaining higher levels of accuracy in many practical domains such as data mining, without necessarily acquiring a semantic understanding of the datasets. The increased successes with real-world data led to increasing emphasis on comparing different approaches against shared test data to see which approach performed best in a broader context than that provided by idiosyncratic toy models; AI research was becoming more scientific. Nowadays results of experiments are often rigorously measurable, and are sometimes (with difficulty) reproducible. Different statistical learning techniques have different limitations; for example, basic HMM cannot model the infinite possible combinations of natural language. Critics note that the shift from GOFAI to statistical learning is often also a shift away from explainable AI. In AGI research, some scholars caution against over-reliance on statistical learning, and argue that continuing research into GOFAI will still be necessary to attain general intelligence.\\n\\nIntegrating the approaches\\nIntelligent agent paradigm\\nAn intelligent agent is a system that perceives its environment and takes actions which maximize its chances of success. The simplest intelligent agents are programs that solve specific problems. More complicated agents include human beings and organizations of human beings (such as firms). The paradigm allows researchers to directly compare or even combine different approaches to isolated problems, by asking which agent is best at maximizing a given \"goal function\". An agent that solves a specific problem can use any approach that workssome agents are symbolic and logical, some are sub-symbolic artificial neural networks and others may use new approaches. The paradigm also gives researchers a common language to communicate with other fieldssuch as decision theory and economicsthat also use concepts of abstract agents. Building a complete agent requires researchers to address realistic problems of integration; for example, because sensory systems give uncertain information about the environment, planning systems must be able to function in the presence of uncertainty. The intelligent agent paradigm became widely accepted during the 1990s.\\nAgent architectures and cognitive architectures\\nResearchers have designed systems to build intelligent systems out of interacting intelligent agents in a multi-agent system. A hierarchical control system provides a bridge between sub-symbolic AI at its lowest, reactive levels and traditional symbolic AI at its highest levels, where relaxed time constraints permit planning and world modeling. Some cognitive architectures are custom-built to solve a narrow problem; others, such as Soar, are designed to mimic human cognition and to provide insight into general intelligence. Modern extensions of Soar are hybrid intelligent systems that include both symbolic and sub-symbolic components.\\nTools\\nAI has developed many tools to solve the most difficult problems in computer science. A few of the most general of these methods are discussed below.\\n\\nSearch and optimization\\n\\nMany problems in AI can be solved in theory by intelligently searching through many possible solutions: Reasoning can be reduced to performing a search. For example, logical proof can be viewed as searching for a path that leads from premises to conclusions, where each step is the application of an inference rule. Planning algorithms search through trees of goals and subgoals, attempting to find a path to a target goal, a process called means-ends analysis. Robotics algorithms for moving limbs and grasping objects use local searches in configuration space. Many learning algorithms use search algorithms based on optimization.\\nSimple exhaustive searches are rarely sufficient for most real-world problems: the search space (the number of places to search) quickly grows to astronomical numbers. The result is a search that is too slow or never completes. The solution, for many problems, is to use \"heuristics\" or \"rules of thumb\" that prioritize choices in favor of those that are more likely to reach a goal and to do so in a shorter number of steps. In some search methodologies heuristics can also serve to entirely eliminate some choices that are unlikely to lead to a goal (called \"pruning the search tree\"). Heuristics supply the program with a \"best guess\" for the path on which the solution lies. Heuristics limit the search for solutions into a smaller sample size.\\nA very different kind of search came to prominence in the 1990s, based on the mathematical theory of optimization. For many problems, it is possible to begin the search with some form of a guess and then refine the guess incrementally until no more refinements can be made. These algorithms can be visualized as blind hill climbing: we begin the search at a random point on the landscape, and then, by jumps or steps, we keep moving our guess uphill, until we reach the top. Other optimization algorithms are simulated annealing, beam search and random optimization.\\n\\n A particle swarm seeking the global minimum\\nEvolutionary computation uses a form of optimization search. For example, they may begin with a population of organisms (the guesses) and then allow them to mutate and recombine, selecting only the fittest to survive each generation (refining the guesses). Classic evolutionary algorithms include genetic algorithms, gene expression programming, and genetic programming. Alternatively, distributed search processes can coordinate via swarm intelligence algorithms. Two popular swarm algorithms used in search are particle swarm optimization (inspired by bird flocking) and ant colony optimization (inspired by ant trails).\\n\\nLogic\\n\\nLogic is used for knowledge representation and problem solving, but it can be applied to other problems as well. For example, the satplan algorithm uses logic for planning and inductive logic programming is a method for learning.\\nSeveral different forms of logic are used in AI research. Propositional logic involves truth functions such as \"or\" and \"not\". First-order logic adds quantifiers and predicates, and can express facts about objects, their properties, and their relations with each other. Fuzzy set theory assigns a \"degree of truth\" (between 0 and 1) to vague statements such as \"Alice is old\" (or rich, or tall, or hungry) that are too linguistically imprecise to be completely true or false. Fuzzy logic is successfully used in control systems to allow experts to contribute vague rules such as \"if you are close to the destination station and moving fast, increase the train\\'s brake pressure\"; these vague rules can then be numerically refined within the system. Fuzzy logic fails to scale well in knowledge bases; many AI researchers question the validity of chaining fuzzy-logic inferences.\\nDefault logics, non-monotonic logics and circumscription are forms of logic designed to help with default reasoning and the qualification problem. Several extensions of logic have been designed to handle specific domains of knowledge, such as: description logics; situation calculus, event calculus and fluent calculus (for representing events and time); causal calculus; belief calculus (belief revision); and modal logics. Logics to model contradictory or inconsistent statements arising in multi-agent systems have also been designed, such as paraconsistent logics.\\n\\nProbabilistic methods for uncertain reasoning\\n\\n Expectation-maximization clustering of Old Faithful eruption data starts from a random guess but then successfully converges on an accurate clustering of the two physically distinct modes of eruption.\\nMany problems in AI (in reasoning, planning, learning, perception, and robotics) require the agent to operate with incomplete or uncertain information. AI researchers have devised a number of powerful tools to solve these problems using methods from probability theory and economics.\\nBayesian networks are a very general tool that can be used for various problems: reasoning (using the Bayesian inference algorithm), learning (using the expectation-maximization algorithm), planning (using decision networks) and perception (using dynamic Bayesian networks). Probabilistic algorithms can also be used for filtering, prediction, smoothing and finding explanations for streams of data, helping perception systems to analyze processes that occur over time (e.g., hidden Markov models or Kalman filters). Compared with symbolic logic, formal Bayesian inference is computationally expensive. For inference to be tractable, most observations must be conditionally independent of one another. Complicated graphs with diamonds or other \"loops\" (undirected cycles) can require a sophisticated method such as Markov chain Monte Carlo, which spreads an ensemble of random walkers throughout the Bayesian network and attempts to converge to an assessment of the conditional probabilities. Bayesian networks are used on Xbox Live to rate and match players; wins and losses are \"evidence\" of how good a player is[]. AdSense uses a Bayesian network with over 300 million edges to learn which ads to serve.\\nA key concept from the science of economics is \"utility\": a measure of how valuable something is to an intelligent agent. Precise mathematical tools have been developed that analyze how an agent can make choices and plan, using decision theory, decision analysis, and information value theory. These tools include models such as Markov decision processes, dynamic decision networks, game theory and mechanism design.\\n\\nClassifiers and statistical learning methods\\n\\nThe simplest AI applications can be divided into two types: classifiers (\"if shiny then diamond\") and controllers (\"if shiny then pick up\"). Controllers do, however, also classify conditions before inferring actions, and therefore classification forms a central part of many AI systems. Classifiers are functions that use pattern matching to determine a closest match. They can be tuned according to examples, making them very attractive for use in AI. These examples are known as observations or patterns. In supervised learning, each pattern belongs to a certain predefined class. A class can be seen as a decision that has to be made. All the observations combined with their class labels are known as a data set. When a new observation is received, that observation is classified based on previous experience.\\nA classifier can be trained in various ways; there are many statistical and machine learning approaches. The decision tree is perhaps the most widely used machine learning algorithm. Other widely used classifiers are the neural network,\\nk-nearest neighbor algorithm,\\nkernel methods such as the support vector machine (SVM),\\nGaussian mixture model, and the extremely popular naive Bayes classifier. Classifier performance depends greatly on the characteristics of the data to be classified, such as the dataset size, distribution of samples across classes, the dimensionality, and the level of noise. Model-based classifiers perform well if the assumed model is an extremely good fit for the actual data. Otherwise, if no matching model is available, and if accuracy (rather than speed or scalability) is the sole concern, conventional wisdom is that discriminative classifiers (especially SVM) tend to be more accurate than model-based classifiers such as \"naive Bayes\" on most practical data sets.\\n\\nArtificial neural networks\\n\\n A neural network is an interconnected group of nodes, akin to the vast network of neurons in the human brain.\\nNeural networks were inspired by the architecture of neurons in the human brain. A simple \"neuron\" N accepts input from other neurons, each of which, when activated (or \"fired\"), cast a weighted \"vote\" for or against whether neuron N should itself activate. Learning requires an algorithm to adjust these weights based on the training data; one simple algorithm (dubbed \"fire together, wire together\") is to increase the weight between two connected neurons when the activation of one triggers the successful activation of another. The neural network forms \"concepts\" that are distributed among a subnetwork of shared neurons that tend to fire together; a concept meaning \"leg\" might be coupled with a subnetwork meaning \"foot\" that includes the sound for \"foot\". Neurons have a continuous spectrum of activation; in addition, neurons can process inputs in a nonlinear way rather than weighing straightforward votes. Modern neural networks can learn both continuous functions and, surprisingly, digital logical operations. Neural networks\\' early successes included predicting the stock market and (in 1995) a mostly self-driving car. In the 2010s, advances in neural networks using deep learning thrust AI into widespread public consciousness and contributed to an enormous upshift in corporate AI spending; for example, AI-related M&A in 2017 was over 25 times as large as in 2015.\\nThe study of non-learning artificial neural networks began in the decade before the field of AI research was founded, in the work of Walter Pitts and Warren McCullouch. Frank Rosenblatt invented the perceptron, a learning network with a single layer, similar to the old concept of linear regression. Early pioneers also include Alexey Grigorevich Ivakhnenko, Teuvo Kohonen, Stephen Grossberg, Kunihiko Fukushima, Christoph von der Malsburg, David Willshaw, Shun-Ichi Amari, Bernard Widrow, John Hopfield, Eduardo R. Caianiello, and others[].\\nThe main categories of networks are acyclic or feedforward neural networks (where the signal passes in only one direction) and recurrent neural networks (which allow feedback and short-term memories of previous input events). Among the most popular feedforward networks are perceptrons, multi-layer perceptrons and radial basis networks. Neural networks can be applied to the problem of intelligent control (for robotics) or learning, using such techniques as Hebbian learning (\"fire together, wire together\"), GMDH or competitive learning.\\nToday, neural networks are often trained by the backpropagation algorithm, which had been around since 1970 as the reverse mode of automatic differentiation published by Seppo Linnainmaa, and was introduced to neural networks by Paul Werbos.\\nHierarchical temporal memory is an approach that models some of the structural and algorithmic properties of the neocortex.\\nTo summarize, most neural networks use some form of gradient descent on a hand-created neural topology. However, some research groups, such as Uber, argue that simple neuroevolution to mutate new neural network topologies and weights may be competitive with sophisticated gradient descent approaches[]. One advantage of neuroevolution is that it may be less prone to get caught in \"dead ends\".\\n\\nDeep feedforward neural networks\\n\\nDeep learning is any artificial neural network that can learn a long chain of causal links[dubious   discuss]. For example, a feedforward network with six hidden layers can learn a seven-link causal chain (six hidden layers + output layer) and has a \"credit assignment path\" (CAP) depth of seven[]. Many deep learning systems need to be able to learn chains ten or more causal links in length. Deep learning has transformed many important subfields of artificial intelligence[why?], including computer vision, speech recognition, natural language processing and others.\\nAccording to one overview, the expression \"Deep Learning\" was introduced to the machine learning community by Rina Dechter in 1986 and gained traction after\\nIgor Aizenberg and colleagues introduced it to artificial neural networks in 2000. The first functional Deep Learning networks were published by Alexey Grigorevich Ivakhnenko and V. G. Lapa in 1965.[pageneeded] These networks are trained one layer at a time. Ivakhnenko\\'s 1971 paper describes the learning of a deep feedforward multilayer perceptron with eight layers, already much deeper than many later networks. In 2006, a publication by Geoffrey Hinton and Ruslan Salakhutdinov introduced another way of pre-training many-layered feedforward neural networks (FNNs) one layer at a time, treating each layer in turn as an unsupervised restricted Boltzmann machine, then using supervised backpropagation for fine-tuning. Similar to shallow artificial neural networks, deep neural networks can model complex non-linear relationships. Over the last few years, advances in both machine learning algorithms and computer hardware have led to more efficient methods for training deep neural networks that contain many layers of non-linear hidden units and a very large output layer.\\nDeep learning often uses convolutional neural networks (CNNs), whose origins can be traced back to the Neocognitron introduced by Kunihiko Fukushima in 1980. In 1989, Yann LeCun and colleagues applied backpropagation to such an architecture. In the early 2000s, in an industrial application, CNNs already processed an estimated 10% to 20% of all the checks written in the US.\\nSince 2011, fast implementations of CNNs on GPUs have\\nwon many visual pattern recognition competitions.\\nCNNs with 12 convolutional layers were used in conjunction with reinforcement learning by Deepmind\\'s \"AlphaGo Lee\", the program that beat a top Go champion in 2016.\\n\\nDeep recurrent neural networks\\n\\nEarly on, deep learning was also applied to sequence learning with recurrent neural networks (RNNs) which are in theory Turing complete and can run arbitrary programs to process arbitrary sequences of inputs. The depth of an RNN is unlimited and depends on the length of its input sequence; thus, an RNN is an example of deep learning. RNNs can be trained by gradient descent but suffer from the vanishing gradient problem. In 1992, it was shown that unsupervised pre-training of a stack of recurrent neural networks can speed up subsequent supervised learning of deep sequential problems.\\nNumerous researchers now use variants of a deep learning recurrent NN called the long short-term memory (LSTM) network published by Hochreiter & Schmidhuber in 1997. LSTM is often trained by Connectionist Temporal Classification (CTC). At Google, Microsoft and Baidu this approach has revolutionized speech recognition. For example, in 2015, Google\\'s speech recognition experienced a dramatic performance jump of 49% through CTC-trained LSTM, which is now available through Google Voice to billions of smartphone users. Google also used LSTM to improve machine translation, Language Modeling and Multilingual Language Processing. LSTM combined with CNNs also improved automatic image captioning and a plethora of other applications.\\n\\nEvaluating progress\\n\\nAI, like electricity or the steam engine, is a general purpose technology. There is no consensus on how to characterize which tasks AI tends to excel at. While projects such as AlphaZero have succeeded in generating their own knowledge from scratch, many other machine learning projects require large training datasets. Researcher Andrew Ng has suggested, as a \"highly imperfect rule of thumb\", that \"almost anything a typical human can do with less than one second of mental thought, we can probably now or in the near future automate using AI.\" Moravec\\'s paradox suggests that AI lags humans at many tasks that the human brain has specifically evolved to perform well.\\nGames provide a well-publicized benchmark for assessing rates of progress. AlphaGo around 2016 brought the era of classical board-game benchmarks to a close. Games of imperfect knowledge provide new challenges to AI in the area of game theory. E-sports such as StarCraft continue to provide additional public benchmarks. There are many competitions and prizes, such as the Imagenet Challenge, to promote research in artificial intelligence. The most common areas of competition include general machine intelligence, conversational behavior, data-mining, robotic cars, and robot soccer as well as conventional games.\\nThe \"imitation game\" (an interpretation of the 1950 Turing test that assesses whether a computer can imitate a human) is nowadays considered too exploitable to be a meaningful benchmark. A derivative of the Turing test is the Completely Automated Public Turing test to tell Computers and Humans Apart (CAPTCHA). As the name implies, this helps to determine that a user is an actual person and not a computer posing as a human. In contrast to the standard Turing test, CAPTCHA is administered by a machine and targeted to a human as opposed to being administered by a human and targeted to a machine. A computer asks a user to complete a simple test then generates a grade for that test. Computers are unable to solve the problem, so correct solutions are deemed to be the result of a person taking the test. A common type of CAPTCHA is the test that requires the typing of distorted letters, numbers or symbols that appear in an image undecipherable by a computer.\\nProposed \"universal intelligence\" tests aim to compare how well machines, humans, and even non-human animals perform on problem sets that are generic as possible. At an extreme, the test suite can contain every possible problem, weighted by Kolmogorov complexity; unfortunately, these problem sets tend to be dominated by impoverished pattern-matching exercises where a tuned AI can easily exceed human performance levels.\\n\\nApplications\\n An automated online assistant providing customer service on a web page  one of many very primitive applications of artificial intelligence\\n\\nAI is relevant to any intellectual task. Modern artificial intelligence techniques are pervasive and are too numerous to list here. Frequently, when a technique reaches mainstream use, it is no longer considered artificial intelligence; this phenomenon is described as the AI effect.\\nHigh-profile examples of AI include autonomous vehicles (such as drones and self-driving cars), medical diagnosis, creating art (such as poetry), proving mathematical theorems, playing games (such as Chess or Go), search engines (such as Google search), online assistants (such as Siri), image recognition in photographs, spam filtering, predicting flight delays, prediction of judicial decisions, targeting online advertisements,  and energy storage\\nWith social media sites overtaking TV as a source for news for young people and news organizations increasingly reliant on social media platforms for generating distribution, major publishers now use artificial intelligence (AI) technology to post stories more effectively and generate higher volumes of traffic.\\nAI can also produce Deepfakes, a content-altering technology. ZDNet reports, \"It presents something that did not actually occur, Though 88% of Americans believe Deepfakes can cause more harm than good, only 47% of them believe they can be targeted. The boom of election year also opens public discourse to threats of videos of falsified politician media.\\n\\nHealthcare\\n\\n A patient-side surgical arm of Da Vinci Surgical SystemAI in healthcare is often used for classification, whether to automate initial evaluation of a CT scan or EKG or to identify high risk patients for population health. The breadth of applications is rapidly increasing.\\nAs an example, AI is being applied to the high cost problem of dosage issueswhere findings suggested that AI could save $16 billion. In 2016, a ground breaking study in California found that a mathematical formula developed with the help of AI correctly determined the accurate dose of immunosuppressant drugs to give to organ patients.  X-ray of a hand, with automatic calculation of bone age by computer software\\nArtificial intelligence is assisting doctors. According to Bloomberg Technology, Microsoft has developed AI to help doctors find the right treatments for cancer. There is a great amount of research and drugs developed relating to cancer. In detail, there are more than 800 medicines and vaccines to treat cancer. This negatively affects the doctors, because there are too many options to choose from, making it more difficult to choose the right drugs for the patients. Microsoft is working on a project to develop a machine called \"Hanover\"[]. Its goal is to memorize all the papers necessary to cancer and help predict which combinations of drugs will be most effective for each patient. One project that is being worked on at the moment is fighting myeloid leukemia, a fatal cancer where the treatment has not improved in decades. Another study was reported to have found that artificial intelligence was as good as trained doctors in identifying skin cancers. Another study is using artificial intelligence to try to monitor multiple high-risk patients, and this is done by asking each patient numerous questions based on data acquired from live doctor to patient interactions. One study was done with transfer learning, the machine performed a diagnosis similarly to a well-trained ophthalmologist, and could generate a decision within 30 seconds on whether or not the patient should be referred for treatment, with more than 95% accuracy.\\nAccording to CNN, a recent study by surgeons at the Children\\'s National Medical Center in Washington successfully demonstrated surgery with an autonomous robot. The team supervised the robot while it performed soft-tissue surgery, stitching together a pig\\'s bowel during open surgery, and doing so better than a human surgeon, the team claimed. IBM has created its own artificial intelligence computer, the IBM Watson, which has beaten human intelligence (at some levels). Watson has struggled to achieve success and adoption in healthcare.\\n\\nAutomotive\\n\\nAdvancements in AI have contributed to the growth of the automotive industry through the creation and evolution of self-driving vehicles. As of 2016[update], there are over 30 companies utilizing AI into the creation of self-driving cars. A few companies involved with AI include Tesla, Google, and Apple.\\nMany components contribute to the functioning of self-driving cars. These vehicles incorporate systems such as braking, lane changing, collision prevention, navigation and mapping. Together, these systems, as well as high performance computers, are integrated into one complex vehicle.\\nRecent developments in autonomous automobiles have made the innovation of self-driving trucks possible, though they are still in the testing phase. The UK government has passed legislation to begin testing of self-driving truck platoons in 2018. Self-driving truck platoons are a fleet of self-driving trucks following the lead of one non-self-driving truck, so the truck platoons aren\\'t entirely autonomous yet. Meanwhile, the Daimler, a German automobile corporation, is testing the Freightliner Inspiration which is a semi-autonomous truck that will only be used on the highway.\\nOne main factor that influences the ability for a driver-less automobile to function is mapping. In general, the vehicle would be pre-programmed with a map of the area being driven. This map would include data on the approximations of street light and curb heights in order for the vehicle to be aware of its surroundings. However, Google has been working on an algorithm with the purpose of eliminating the need for pre-programmed maps and instead, creating a device that would be able to adjust to a variety of new surroundings. Some self-driving cars are not equipped with steering wheels or brake pedals, so there has also been research focused on creating an algorithm that is capable of maintaining a safe environment for the passengers in the vehicle through awareness of speed and driving conditions.\\nAnother factor that is influencing the ability of a driver-less automobile is the safety of the passenger. To make a driver-less automobile, engineers must program it to handle high-risk situations. These situations could include a head-on collision with pedestrians. The car\\'s main goal should be to make a decision that would avoid hitting the pedestrians and saving the passengers in the car. But there is a possibility the car would need to make a decision that would put someone in danger. In other words, the car would need to decide to save the pedestrians or the passengers. The programming of the car in these situations is crucial to a successful driver-less automobile.\\n\\nFinance and economics\\nFinancial institutions have long used artificial neural network systems to detect charges or claims outside of the norm, flagging these for human investigation. The use of AI in banking can be traced back to 1987 when Security Pacific National Bank in US set-up a Fraud Prevention Task force to counter the unauthorized use of debit cards. Programs like Kasisto and Moneystream are using AI in financial services.\\nBanks use artificial intelligence systems today to organize operations, maintain book-keeping, invest in stocks, and manage properties. AI can react to changes overnight or when business is not taking place. In August 2001, robots beat humans in a simulated financial trading competition. AI has also reduced fraud and financial crimes by monitoring behavioral patterns of users for any abnormal changes or anomalies.\\nAI is also being used by corporations. Whereas AI CEO\\'s are still 30 years away, robotic process automation (RPA) is already being used today in corporate finance. RPA uses artificial intelligence to train and teach software robots to process transactions, monitor compliance and audit processes automatically.\\nThe use of AI machines in the market in applications such as online trading and decision making has changed major economic theories. For example, AI based buying and selling platforms have changed the law of supply and demand in that it is now possible to easily estimate individualized demand and supply curves and thus individualized pricing. Furthermore, AI machines reduce information asymmetry in the market and thus making markets more efficient while reducing the volume of trades[]. Furthermore, AI in the markets limits the consequences of behavior in the markets again making markets more efficient[]. Other theories where AI has had impact include in rational choice, rational expectations, game theory, Lewis turning point, portfolio optimization and counterfactual thinking[].. In August 2019, the AICPA introduced AI training course for accounting professionals.\\n\\nCybersecurity\\nThis section needs additional citations for verification. Please help improve this article by adding citations to reliable sources. Unsourced material may be challenged and removed.Find sources:\"Artificial intelligence\"news newspapers books scholar JSTOR (January 2020) (Learn how and when to remove this template message)\\nThe cybersecurity arena faces significant challenges in the form of larges scale hacking attacks of different types which harm organizations of all kinds and create billions of dollars in business damage. Artificial intelligence and Natural Language Processing (NLP) has begun to be used by security companies - for example SIEM (Security Information and Event Management) solutions.  The more advanced of these solutions use AI and NLP to automatically sort the data in networks into high risk and low risk information.  This enables security teams to focus on the attacks that have the potential to do real harm to the organization, and not become victims of attacks such as Denial of Service (DoS), Malware and others.\\n\\nGovernment\\n\\n\\nArtificial intelligence paired with facial recognition systems may be used for mass surveillance. This is already the case in some parts of China. An artificial intelligence has also competed in the Tama City mayoral elections in 2018.\\nIn 2019, the tech city of Bengaluru in India is set to deploy AI managed traffic signal systems across the 387 traffic signals in the city. This system will involve use of cameras to ascertain traffic density and accordingly calculate the time needed to clear the traffic volume which will determine the signal duration for vehicular traffic across streets.\\n\\nLaw-related professions\\nArtificial intelligence (AI) is becoming a mainstay component of law-related professions. In some circumstances, this analytics-crunching technology is using algorithms and machine learning to do work that was previously done by entry-level lawyers.[]\\nIn Electronic Discovery (eDiscovery), the industry has been focused on machine learning (predictive coding/technology assisted review), which is a subset of AI. To add to the soup of applications, Natural Language Processing (NLP) and Automated Speech Recognition (ASR) are also in vogue in the industry.\\n\\nVideo games\\n\\nIn video games, artificial intelligence is routinely used to generate dynamic purposeful behavior in non-player characters (NPCs). In addition, well-understood AI techniques are routinely used for pathfinding. Some researchers consider NPC AI in games to be a \"solved problem\" for most production tasks. Games with more atypical AI include the AI director of Left 4 Dead (2008) and the neuroevolutionary training of platoons in Supreme Commander 2 (2010).\\n\\nMilitary\\n\\nThe main military applications of Artificial Intelligence and Machine Learning are to enhance C2, Communications, Sensors, Integration and Interoperability.\\nArtificial Intelligence technologies enable coordination of sensors and effectors, threat detection and identification, marking of enemy positions, target acquisition, coordination and deconfliction of distributed Join Fires between networked combat vehicles and tanks also inside Manned and Unmanned Teams (MUM-T).\\nWorldwide annual military spending on robotics rose from US$5.1 billion in 2010 to US$7.5 billion in 2015. Military drones capable of autonomous action are widely considered a useful asset. Many artificial intelligence researchers seek to distance themselves from military applications of AI.\\n\\nHospitality\\nIn the hospitality industry, Artificial Intelligence based solutions are used to reduce staff load and increase efficiency by cutting repetitive tasks frequency, trends analysis, guest interaction, and customer needs prediction. Hotel services backed by Artificial Intelligence are represented in the form of a chatbot, application, virtual voice assistant and service robots.\\n\\nAudit\\nFor financial statements audit, AI makes continuous audit possible. AI tools could analyze many sets of different information immediately. The potential benefit would be the overall audit risk will be reduced, the level of assurance will be increased and the time duration of audit will be reduced.\\n\\nAdvertising\\nIt is possible to use AI to predict or generalize the behavior of customers from their digital footprints in order to target them with personalized promotions or build customer personas automatically. A documented case reports that online gambling companies were using AI to improve customer targeting.\\nMoreover, the application of Personality computing AI models can help reducing the cost of advertising campaigns by adding psychological targeting to more traditional sociodemographic or behavioral targeting.\\n\\nArt\\n\\nArtificial Intelligence has inspired numerous creative applications including its usage to produce visual art. The exhibition \"Thinking Machines: Art and Design in the Computer Age, 19591989\" at MoMA provides a good overview of the historical applications of AI for art, architecture, and design. Recent exhibitions showcasing the usage of AI to produce art include the Google-sponsored benefit and auction at the Gray Area Foundation in San Francisco, where artists experimented with the DeepDream algorithm and the exhibition \"Unhuman: Art in the Age of AI,\" which took place in Los Angeles and Frankfurt in the fall of 2017. In the spring of 2018, the Association of Computing Machinery dedicated a special magazine issue to the subject of computers and art highlighting the role of machine learning in the arts. The Austrian Ars Electronica and Museum of Applied Arts, Vienna opened exhibitions on AI in 2019. The Ars Electronica\\'s 2019 festival \"Out of the box\" extensively thematized the role of arts for a sustainable societal transformation with AI.\\n\\nPhilosophy and ethics\\n\\nThere are three philosophical questions related to AI:\\n\\nIs artificial general intelligence possible? Can a machine solve any problem that a human being can solve using intelligence? Or are there hard limits to what a machine can accomplish?\\nAre intelligent machines dangerous? How can we ensure that machines behave ethically and that they are used ethically?\\nCan a machine have a mind, consciousness and mental states in exactly the same sense that human beings do? Can a machine be sentient, and thus deserve certain rights? Can a machine intentionally cause harm?\\nThe limits of artificial general intelligence\\n\\nCan a machine be intelligent? Can it \"think\"?\\n\\nAlan Turing\\'s \"polite convention\"\\nWe need not decide if a machine can \"think\"; we need only decide if a machine can act as intelligently as a human being. This approach to the philosophical problems associated with artificial intelligence forms the basis of the Turing test.\\nThe Dartmouth proposal\\n\"Every aspect of learning or any other feature of intelligence can be so precisely described that a machine can be made to simulate it.\" This conjecture was printed in the proposal for the Dartmouth Conference of 1956.\\nNewell and Simon\\'s physical symbol system hypothesis\\n\"A physical symbol system has the necessary and sufficient means of general intelligent action.\" Newell and Simon argue that intelligence consists of formal operations on symbols. Hubert Dreyfus argued that, on the contrary, human expertise depends on unconscious instinct rather than conscious symbol manipulation and on having a \"feel\" for the situation rather than explicit symbolic knowledge. (See Dreyfus\\' critique of AI.)\\nGdelian arguments\\nGdel himself, John Lucas (in 1961) and Roger Penrose (in a more detailed argument from 1989 onwards) made highly technical arguments that human mathematicians can consistently see the truth of their own \"Gdel statements\" and therefore have computational abilities beyond that of mechanical Turing machines. However, some people do not agree with the \"Gdelian arguments\".\\nThe artificial brain argument\\nThe brain can be simulated by machines and because brains are intelligent, simulated brains must also be intelligent; thus machines can be intelligent. Hans Moravec, Ray Kurzweil and others have argued that it is technologically feasible to copy the brain directly into hardware and software and that such a simulation will be essentially identical to the original.\\nThe AI effect\\nMachines are already intelligent, but observers have failed to recognize it. When Deep Blue beat Garry Kasparov in chess, the machine was acting intelligently. However, onlookers commonly discount the behavior of an artificial intelligence program by arguing that it is not \"real\" intelligence after all; thus \"real\" intelligence is whatever intelligent behavior people can do that machines still cannot. This is known as the AI Effect: \"AI is whatever hasn\\'t been done yet.\"\\nPotential harm\\nWidespread use of artificial intelligence could have unintended consequences that are dangerous or undesirable. Scientists from the Future of Life Institute, among others, described some short-term research goals to see how AI influences the economy, the laws and ethics that are involved with AI and how to minimize AI security risks. In the long-term, the scientists have proposed to continue optimizing function while minimizing possible security risks that come along with new technologies.\\nThe potential negative effects of AI and automation were a major issue for Andrew Yang\\'s 2020 presidential campaign. Irakli Beridze, Head of the Centre for Artificial Intelligence and Robotics at UNICRI, United Nations, has expressed that \"I think the dangerous applications for AI, from my point of view, would be criminals or large terrorist organizations using it to disrupt large processes or simply do pure harm. [Terrorists could cause harm] via digital warfare, or it could be a combination of robotics, drones, with AI and other things as well that could be really dangerous. And, of course, other risks come from things like job losses. If we have massive numbers of people losing jobs and dont find a solution, it will be extremely dangerous. Things like lethal autonomous weapons systems should be properly governed  otherwise theres massive potential of misuse.\"\\n\\nExistential risk\\n\\nPhysicist Stephen Hawking, Microsoft founder Bill Gates, and SpaceX founder Elon Musk have expressed concerns about the possibility that AI could evolve to the point that humans could not control it, with Hawking theorizing that this could \"spell the end of the human race\".\\n\\nThe development of full artificial intelligence could spell the end of the human race. Once humans develop artificial intelligence, it will take off on its own and redesign itself at an ever-increasing rate. Humans, who are limited by slow biological evolution, couldn\\'t compete and would be superseded.Stephen Hawking\\n\\nIn his book Superintelligence, philosopher Nick Bostrom provides an argument that artificial intelligence will pose a threat to humankind. He argues that sufficiently intelligent AI, if it chooses actions based on achieving some goal, will exhibit convergent behavior such as acquiring resources or protecting itself from being shut down. If this AI\\'s goals do not fully reflect humanity\\'sone example is an AI told to compute as many digits of pi as possibleit might harm humanity in order to acquire more resources or prevent itself from being shut down, ultimately to better achieve its goal.  Bostrom also emphasizes the difficulty of fully conveying humanity\\'s values to an advanced AI.  He uses the hypothetical example of giving an AI the goal to make humans smile to illustrate a misguided attempt.  If the AI in that scenario were to become superintelligent, Bostrom argues, it may resort to methods that most humans would find horrifying, such as inserting \"electrodes into the facial muscles of humans to cause constant, beaming grins\" because that would be an efficient way to achieve its goal of making humans smile.  In his book Human Compatible, AI researcher Stuart J. Russell echoes some of Bostrom\\'s concerns while also proposing an approach to developing provably beneficial machines focused on uncertainty and deference to humans, possibly involving inverse reinforcement learning.\\nConcern over risk from artificial intelligence has led to some high-profile donations and investments. A group of prominent tech titans including Peter Thiel, Amazon Web Services and Musk have committed $1 billion to OpenAI, a nonprofit company aimed at championing responsible AI development. The opinion of experts within the field of artificial intelligence is mixed, with sizable fractions both concerned and unconcerned by risk from eventual superhumanly-capable AI. Other technology industry leaders believe that artificial intelligence is helpful in its current form and will continue to assist humans. Oracle CEO Mark Hurd has stated that AI \"will actually create more jobs, not less jobs\" as humans will be needed to manage AI systems. Facebook CEO Mark Zuckerberg believes AI will \"unlock a huge amount of positive things,\" such as curing disease and increasing the safety of autonomous cars. In January 2015, Musk donated $10 million to the Future of Life Institute to fund research on understanding AI decision making. The goal of the institute is to \"grow wisdom with which we manage\" the growing power of technology. Musk also funds companies developing artificial intelligence such as DeepMind and Vicarious to \"just keep an eye on what\\'s going on with artificial intelligence. I think there is potentially a dangerous outcome there.\"\\nFor the danger of uncontrolled advanced AI to be realized, the hypothetical AI would have to overpower or out-think all of humanity, which a minority of experts argue is a possibility far enough in the future to not be worth researching. Other counterarguments revolve around humans being either intrinsically or convergently valuable from the perspective of an artificial intelligence.\\n\\nDevaluation of humanity\\n\\nJoseph Weizenbaum wrote that AI applications cannot, by definition, successfully simulate genuine human empathy and that the use of AI technology in fields such as customer service or psychotherapy was deeply misguided. Weizenbaum was also bothered that AI researchers (and some philosophers) were willing to view the human mind as nothing more than a computer program (a position now known as computationalism). To Weizenbaum these points suggest that AI research devalues human life.\\n\\nSocial justice\\nOne concern is that AI programs may be programmed to be biased against certain groups, such as women and minorities, because most of the developers are wealthy Caucasian men. Support for artificial intelligence is higher among men (with 47% approving) than women (35% approving).\\nAlgorithms have a host of applications in today\\'s legal system already, assisting officials ranging from judges to parole officers and public defenders in gauging the predicted likelihood of recidivism of defendants. COMPAS (an acronym for Correctional Offender Management Profiling for Alternative Sanctions) counts among the most widely utilized commercially available solutions. It has been suggested that COMPAS assigns an exceptionally elevated risk of recidivism to black defendants while, conversely, ascribing low risk estimate to white defendants significantly more often than statistically expected.\\n\\nDecrease in demand for human labor\\n\\nThe relationship between automation and employment is complicated. While automation eliminates old jobs, it also creates new jobs through micro-economic and macro-economic effects. Unlike previous waves of automation, many middle-class jobs may be eliminated by artificial intelligence; The Economist states that \"the worry that AI could do to white-collar jobs what steam power did to blue-collar ones during the Industrial Revolution\" is \"worth taking seriously\". Subjective estimates of the risk vary widely; for example, Michael Osborne and Carl Benedikt Frey estimate 47% of U.S. jobs are at \"high risk\" of potential automation, while an OECD report classifies only 9% of U.S. jobs as \"high risk\". Jobs at extreme risk range from paralegals to fast food cooks, while job demand is likely to increase for care-related professions ranging from personal healthcare to the clergy. Author Martin Ford and others go further and argue that many jobs are routine, repetitive and (to an AI) predictable; Ford warns that these jobs may be automated in the next couple of decades, and that many of the new jobs may not be \"accessible to people with average capability\", even with retraining. Economists point out that in the past technology has tended to increase rather than reduce total employment, but acknowledge that \"we\\'re in uncharted territory\" with AI.\\n\\nAutonomous weapons\\n\\nCurrently, 50+ countries are researching battlefield robots, including the United States, China, Russia, and the United Kingdom. Many people concerned about risk from superintelligent AI also want to limit the use of artificial soldiers and drones.\\n\\nEthical machines\\nMachines with intelligence have the potential to use their intelligence to prevent harm and minimize the risks; they may have the ability to use ethical reasoning to better choose their actions in the world. Research in this area includes machine ethics, artificial moral agents, friendly AI and discussion towards building a human rights framework is also in talks.\\n\\nArtificial moral agents\\nWendell Wallach introduced the concept of artificial moral agents (AMA) in his book Moral Machines For Wallach, AMAs have become a part of the research landscape of artificial intelligence as guided by its two central questions which he identifies as \"Does Humanity Want Computers Making Moral Decisions\" and \"Can (Ro)bots Really Be Moral\". For Wallach the question is not centered on the issue of whether machines can demonstrate the equivalent of moral behavior in contrast to the constraints which society may place on the development of AMAs.\\n\\nMachine ethics\\n\\nThe field of machine ethics is concerned with giving machines ethical principles, or a procedure for discovering a way to resolve the ethical dilemmas they might encounter, enabling them to function in an ethically responsible manner through their own ethical decision making. The field was delineated in the AAAI Fall 2005 Symposium on Machine Ethics: \"Past research concerning the relationship between technology and ethics has largely focused on responsible and irresponsible use of technology by human beings, with a few people being interested in how human beings ought to treat machines. In all cases, only human beings have engaged in ethical reasoning. The time has come for adding an ethical dimension to at least some machines. Recognition of the ethical ramifications of behavior involving machines, as well as recent and potential developments in machine autonomy, necessitate this. In contrast to computer hacking, software property issues, privacy issues and other topics normally ascribed to computer ethics, machine ethics is concerned with the behavior of machines towards human users and other machines. Research in machine ethics is key to alleviating concerns with autonomous systemsit could be argued that the notion of autonomous machines without such a dimension is at the root of all fear concerning machine intelligence. Further, investigation of machine ethics could enable the discovery of problems with current ethical theories, advancing our thinking about Ethics.\" Machine ethics is sometimes referred to as machine morality, computational ethics or computational morality. A variety of perspectives of this nascent field can be found in the collected edition \"Machine Ethics\" that stems from the AAAI Fall 2005 Symposium on Machine Ethics.\\n\\nMalevolent and friendly AI\\n\\nPolitical scientist Charles T. Rubin believes that AI can be neither designed nor guaranteed to be benevolent. He argues that \"any sufficiently advanced benevolence may be indistinguishable from malevolence.\" Humans should not assume machines or robots would treat us favorably because there is no a priori reason to believe that they would be sympathetic to our system of morality, which has evolved along with our particular biology (which AIs would not share). Hyper-intelligent software may not necessarily decide to support the continued existence of humanity and would be extremely difficult to stop. This topic has also recently begun to be discussed in academic publications as a real source of risks to civilization, humans, and planet Earth.\\nOne proposal to deal with this is to ensure that the first generally intelligent AI is \\'Friendly AI\\' and will be able to control subsequently developed AIs. Some question whether this kind of check could actually remain in place.\\nLeading AI researcher Rodney Brooks writes, \"I think it is a mistake to be worrying about us developing malevolent AI anytime in the next few hundred years. I think the worry stems from a fundamental error in not distinguishing the difference between the very real recent advances in a particular aspect of AI, and the enormity and complexity of building sentient volitional intelligence.\"\\n\\nMachine consciousness, sentience and mind\\n\\nIf an AI system replicates all key aspects of human intelligence, will that system also be sentientwill it have a mind which has conscious experiences? This question is closely related to the philosophical problem as to the nature of human consciousness, generally referred to as the hard problem of consciousness.\\n\\nConsciousness\\n\\nDavid Chalmers identified two problems in understanding the mind, which he named the \"hard\" and \"easy\" problems of consciousness. The easy problem is understanding how the brain processes signals, makes plans and controls behavior. The hard problem is explaining how this feels or why it should feel like anything at all. Human information processing is easy to explain, however human subjective experience is difficult to explain.\\nFor example, consider what happens when a person is shown a color swatch and identifies it, saying \"it\\'s red\". The easy problem only requires understanding the machinery in the brain that makes it possible for a person to know that the color swatch is red. The hard problem is that people also know something elsethey also know what red looks like. (Consider that a person born blind can know that something is red without knowing what red looks like.) Everyone knows subjective experience exists, because they do it every day (e.g., all sighted people know what red looks like). The hard problem is explaining how the brain creates it, why it exists, and how it is different from knowledge and other aspects of the brain.\\n\\nComputationalism and functionalism\\n\\nComputationalism is the position in the philosophy of mind that the human mind or the human brain (or both) is an information processing system and that thinking is a form of computing. Computationalism argues that the relationship between mind and body is similar or identical to the relationship between software and hardware and thus may be a solution to the mind-body problem. This philosophical position was inspired by the work of AI researchers and cognitive scientists in the 1960s and was originally proposed by philosophers Jerry Fodor and Hilary Putnam.\\n\\nStrong AI hypothesis\\n\\nThe philosophical position that John Searle has named \"strong AI\" states: \"The appropriately programmed computer with the right inputs and outputs would thereby have a mind in exactly the same sense human beings have minds.\" Searle counters this assertion with his Chinese room argument, which asks us to look inside the computer and try to find where the \"mind\" might be.\\n\\nRobot rights\\n\\nIf a machine can be created that has intelligence, could it also feel? If it can feel, does it have the same rights as a human? This issue, now known as \"robot rights\", is currently being considered by, for example, California\\'s Institute for the Future, although many critics believe that the discussion is premature. Some critics of transhumanism argue that any hypothetical robot rights would lie on a spectrum with animal rights and human rights.  The subject is profoundly discussed in the 2010 documentary film Plug & Pray, and many sci fi media such as Star Trek Next Generation, with the character of Commander Data, who fought being disassembled for research, and wanted to \"become human\", and the robotic holograms in Voyager.\\n\\nSuperintelligence\\n\\nAre there limits to how intelligent machinesor human-machine hybridscan be? A superintelligence, hyperintelligence, or superhuman intelligence is a hypothetical agent that would possess intelligence far surpassing that of the brightest and most gifted human mind. Superintelligence may also refer to the form or degree of intelligence possessed by such an agent.\\n\\nTechnological singularity\\n\\nIf research into Strong AI produced sufficiently intelligent software, it might be able to reprogram and improve itself. The improved software would be even better at improving itself, leading to recursive self-improvement. The new intelligence could thus increase exponentially and dramatically surpass humans. Science fiction writer Vernor Vinge named this scenario \"singularity\". Technological singularity is when accelerating progress in technologies will cause a runaway effect wherein artificial intelligence will exceed human intellectual capacity and control, thus radically changing or even ending civilization. Because the capabilities of such an intelligence may be impossible to comprehend, the technological singularity is an occurrence beyond which events are unpredictable or even unfathomable.\\nRay Kurzweil has used Moore\\'s law (which describes the relentless exponential improvement in digital technology) to calculate that desktop computers will have the same processing power as human brains by the year 2029, and predicts that the singularity will occur in 2045.\\n\\nTranshumanism\\n\\nRobot designer Hans Moravec, cyberneticist Kevin Warwick and inventor Ray Kurzweil have predicted that humans and machines will merge in the future into cyborgs that are more capable and powerful than either. This idea, called transhumanism, has roots in Aldous Huxley and Robert Ettinger.\\nEdward Fredkin argues that \"artificial intelligence is the next stage in evolution\", an idea first proposed by Samuel Butler\\'s \"Darwin among the Machines\" as far back as 1863, and expanded upon by George Dyson in his book of the same name in 1998.\\n\\nEconomics\\nThe long-term economic effects of AI are uncertain. A survey of economists showed disagreement about whether the increasing use of robots and AI will cause a substantial increase in long-term unemployment, but they generally agree that it could be a net benefit, if productivity gains are redistributed. A February 2020 European Union white paper on artificial intelligence advocated for artificial intelligence for economic benefits, including \"improving healthcare (e.g. making diagnosis more  precise,  enabling  better  prevention  of  diseases), increasing  the  efficiency  of  farming,contributing  to climate  change mitigation  and  adaptation, [and] improving  the  efficiency  of production systems through predictive maintenance\", while acknowledging potential risks.\\n\\nIn fiction\\n\\n The word \"robot\" itself was coined by Karel apek in his 1921 play R.U.R., the title standing for \"Rossum\\'s Universal Robots\"\\nThought-capable artificial beings appeared as storytelling devices since antiquity,\\nand have been a persistent theme in science fiction.\\nA common trope in these works began with Mary Shelley\\'s Frankenstein, where a human creation becomes a threat to its masters. This includes such works as Arthur C. Clarke\\'s and Stanley Kubrick\\'s 2001: A Space Odyssey (both 1968), with HAL 9000, the murderous computer in charge of the Discovery One spaceship, as well as The Terminator (1984) and The Matrix (1999). In contrast, the rare loyal robots such as Gort from The Day the Earth Stood Still (1951) and Bishop from Aliens (1986) are less prominent in popular culture.\\nIsaac Asimov introduced the Three Laws of Robotics in many books and stories, most notably the \"Multivac\" series about a super-intelligent computer of the same name. Asimov\\'s laws are often brought up during lay discussions of machine ethics; while almost all artificial intelligence researchers are familiar with Asimov\\'s laws through popular culture, they generally consider the laws useless for many reasons, one of which is their ambiguity.\\nTranshumanism (the merging of humans and machines) is explored in the manga Ghost in the Shell and the science-fiction series Dune. In the 1980s, artist Hajime Sorayama\\'s Sexy Robots series were painted and published in Japan depicting the actual organic human form with lifelike muscular metallic skins and later \"the Gynoids\" book followed that was used by or influenced movie makers including George Lucas and other creatives. Sorayama never considered these organic robots to be real part of nature but always unnatural product of the human mind, a fantasy existing in the mind even when realized in actual form.\\nSeveral works use AI to force us to confront the fundamental question of what makes us human, showing us artificial beings that have the ability to feel, and thus to suffer. This appears in Karel apek\\'s R.U.R., the films A.I. Artificial Intelligence and Ex Machina, as well as the novel Do Androids Dream of Electric Sheep?, by Philip K. Dick. Dick considers the idea that our understanding of human subjectivity is altered by technology created with artificial intelligence.\\n\\n\\n\\n\\nAbductive reasoning\\nA.I. Rising\\nArtificial intelligence arms race\\nBehavior selection algorithm\\nBusiness process automation\\nCase-based reasoning\\nCitizen Science\\nCommonsense reasoning\\nEmergent algorithm\\nEvolutionary computation\\nGlossary of artificial intelligence\\nMachine learning\\nMathematical optimization\\nMulti-agent system\\nPersonality computing\\nRobotic process automation\\nSoft computing\\nUniversal basic income\\nWeak AI\\n\\nExplanatory notes\\n\\n\\n\\nAI textbooks\\n\\nHistory of AI\\n\\nOther sources\\n\\n\\n\\n\\n\\n\\n\\nAITopics  A large directory of links and other resources maintained by the Association for the Advancement of Artificial Intelligence, the leading organization of academic AI researchers.\\nArtificial Intelligence, BBC Radio 4 discussion with John Agar, Alison Adam & Igor Aleksander (In Our Time, Dec. 8, 2005)'\n",
      "\n",
      "https://en.wikipedia.org/wiki/Logistic_regression#Model_fitting \t\t\t\t 6941\n",
      "b'This section needs expansion. You can help by adding to it. (October 2016)\\nLogistic regression is an important machine learning algorithm. The goal is to model the probability of a random variable  being 0 or 1 given experimental data.\\nConsider a generalized linear model function parameterized by ,\\n\\n\\nTherefore,\\n\\n\\nand since , we see that  is given by  We now calculate the likelihood function assuming that all the observations in the sample are independently Bernoulli distributed,\\n\\n\\nTypically, the log likelihood is maximized,\\n\\n\\nwhich is maximized using optimization techniques such as gradient descent.\\nAssuming the  pairs are drawn uniformly from the underlying distribution, then in the limit of largeN,\\n\\n\\nwhere  is the conditional entropy and  is the KullbackLeibler divergence. This leads to the intuition that by maximizing the log-likelihood of a model, you are minimizing the KL divergence of your model from the maximal entropy distribution. Intuitively searching for the model that makes the fewest assumptions in its parameters.\\n\\n\"Rule of ten\"\\n\\nA widely used rule of thumb, the \"one in ten rule\", states that logistic regression models give stable values for the explanatory variables if based on a minimum of about 10 events per explanatory variable (EPV); where event denotes the cases belonging to the less frequent category in the dependent variable. Thus a study designed to use  explanatory variables for an event (e.g. myocardial infarction) expected to occur in a proportion  of participants in the study will require a total of  participants. However, there is considerable debate about the reliability of this rule, which is based on simulation studies and lacks a secure theoretical underpinning. According to some authors the rule is overly conservative, some circumstances; with the authors stating \"If we (somewhat subjectively) regard confidence interval coverage less than 93 percent, type I error greater than 7 percent, or relative bias greater than 15 percent as problematic, our results indicate that problems are fairly frequent with 24 EPV, uncommon with 59 EPV, and still observed with 1016 EPV. The worst instances of each problem were not severe with 59 EPV and usually comparable to those with 1016 EPV\".\\nOthers have found results that are not consistent with the above, using different criteria.  A useful criterion is whether the fitted model will be expected to achieve the same predictive discrimination in a new sample as it appeared to achieve in the model development sample.  For that criterion, 20 events per candidate variable may be required.  Also, one can argue that 96 observations are needed only to estimate the model\\'s intercept precisely enough that the margin of error in predicted probabilities is 0.1 with an 0.95 confidence level.\\n\\nMaximum likelihood estimation\\nThe regression coefficients are usually estimated using maximum likelihood estimation. Unlike linear regression with normally distributed residuals, it is not possible to find a closed-form expression for the coefficient values that maximize the likelihood function, so that an iterative process must be used instead; for example Newton\\'s method. This process begins with a tentative solution, revises it slightly to see if it can be improved, and repeats this revision until no more improvement is made, at which point the process is said to have converged.\\nIn some instances, the model may not reach convergence. Non-convergence of a model indicates that the coefficients are not meaningful because the iterative process was unable to find appropriate solutions. A failure to converge may occur for a number of reasons: having a large ratio of predictors to cases, multicollinearity, sparseness, or complete separation.\\n\\nHaving a large ratio of variables to cases results in an overly conservative Wald statistic (discussed below) and can lead to non-convergence.\\nMulticollinearity refers to unacceptably high correlations between predictors. As multicollinearity increases, coefficients remain unbiased but standard errors increase and the likelihood of model convergence decreases. To detect multicollinearity amongst the predictors, one can conduct a linear regression analysis with the predictors of interest for the sole purpose of examining the tolerance statistic   used to assess whether multicollinearity is unacceptably high.\\nSparseness in the data refers to having a large proportion of empty cells (cells with zero counts). Zero cell counts are particularly problematic with categorical predictors. With continuous predictors, the model can infer values for the zero cell counts, but this is not the case with categorical predictors. The model will not converge with zero cell counts for categorical predictors because the natural logarithm of zero is an undefined value so that the final solution to the model cannot be reached. To remedy this problem, researchers may collapse categories in a theoretically meaningful way or add a constant to all cells.\\nAnother numerical problem that may lead to a lack of convergence is complete separation, which refers to the instance in which the predictors perfectly predict the criterion all cases are accurately classified. In such instances, one should reexamine the data, as there is likely some kind of error.[further explanation needed]\\nOne can also take semi-parametric or non-parametric approaches, e.g., via local-likelihood or nonparametric quasi-likelihood methods, which avoid assumptions of a parametric form for the index function and is robust to the choice of the link function (e.g., probit or logit).\\nIteratively reweighted least squares (IRLS)\\nBinary logistic regression ( or ) can, for example, be calculated using iteratively reweighted least squares (IRLS), which is equivalent to minimizing the log-likelihood of a Bernoulli distributed  process using Newton\\'s method. If the problem is written in vector matrix form, with parameters , explanatory variables  and expected value of the Bernoulli distribution , the parameters  can be found using the following iterative algorithm:\\n\\n\\nwhere  is a diagonal weighting matrix,  the vector of expected values,\\n\\n\\nThe regressor matrix and  the vector of response variables. More details can be found in the literature.\\n\\nEvaluating goodness of fit\\nGoodness of fit in linear regression models is generally measured using R2. Since this has no direct analog in logistic regression, various methods including the following can be used instead.\\n\\nDeviance and likelihood ratio tests\\nIn linear regression analysis, one is concerned with partitioning variance via the sum of squares calculations  variance in the criterion is essentially divided into variance accounted for by the predictors and residual variance. In logistic regression analysis, deviance is used in lieu of a sum of squares calculations. Deviance is analogous to the sum of squares calculations in linear regression  and is a measure of the lack of fit to the data in a logistic regression model. When a \"saturated\" model is available (a model with a theoretically perfect fit), deviance is calculated by comparing a given model with the saturated model.  This computation gives the likelihood-ratio test:\\n\\n\\nIn the above equation,  represents the deviance and ln represents the natural logarithm. The log of this likelihood ratio (the ratio of the fitted model to the saturated model) will produce a negative value, hence the need for a negative sign.  can be shown to follow an approximate chi-squared distribution.  Smaller values indicate better fit as the fitted model deviates less from the saturated model. When assessed upon a chi-square distribution, nonsignificant chi-square values indicate very little unexplained variance and thus, good model fit. Conversely, a significant chi-square value indicates that a significant amount of the variance is unexplained.\\nWhen the saturated model is not available (a common case), deviance is calculated simply as 2(log likelihood of the fitted model), and the reference to the saturated model\\'s log likelihood can be removed from all that follows without harm.\\nTwo measures of deviance are particularly important in logistic regression: null deviance and model deviance. The null deviance represents the difference between a model with only the intercept (which means \"no predictors\") and the saturated model. The model deviance represents the difference between a model with at least one predictor and the saturated model. In this respect, the null model provides a baseline upon which to compare predictor models. Given that deviance is a measure of the difference between a given model and the saturated model, smaller values indicate better fit. Thus, to assess the contribution of a predictor or set of predictors, one can subtract the model deviance from the null deviance and assess the difference on a   chi-square distribution with degrees of freedom equal to the difference in the number of parameters estimated.\\nLet\\n\\n\\nThen the difference of both is:\\n\\n\\nIf the model deviance is significantly smaller than the null deviance then one can conclude that the predictor or set of predictors significantly improved model fit. This is analogous to the -test used in linear regression analysis to assess the significance of prediction.\\n\\nPseudo-R2s\\nIn linear regression the squared multiple correlation, R2 is used to assess goodness of fit as it represents the proportion of variance in the criterion that is explained by the predictors. In logistic regression analysis, there is no agreed upon analogous measure, but there are several competing measures each with limitations.\\nFour of the most commonly used indices and one less commonly used one are examined on this page:\\n\\nLikelihood ratio R2L\\nCox and Snell R2CS\\nNagelkerke R2N\\nMcFadden R2McF\\nTjur R2T\\nR2L is given by \\n\\n\\nThis is the most analogous index to the squared multiple correlations in linear regression. It represents the proportional reduction in the deviance wherein the deviance is treated as a measure of variation analogous but not identical to the variance in linear regression analysis. One limitation of the likelihood ratio R2 is that it is not monotonically related to the odds ratio, meaning that it does not necessarily increase as the odds ratio increases and does not necessarily decrease as the odds ratio decreases.\\nR2CS is an alternative index of goodness of fit related to the R2 value from linear regression. It is given by:\\n\\n\\nwhere LM and L0 are the likelihoods for the model being fitted and the null model, respectively. The Cox and Snell index is problematic as its maximum value is . The highest this upper bound can be is 0.75, but it can easily be as low as 0.48 when the marginal proportion of cases is small.\\nR2N provides a correction to the Cox and Snell R2 so that the maximum value is equal to 1. Nevertheless, the Cox and Snell and likelihood ratio R2s show greater agreement with each other than either does with the Nagelkerke R2. Of course, this might not be the case for values exceeding .75 as the Cox and Snell index is capped at this value. The likelihood ratio R2 is often preferred to the alternatives as it is most analogous to R2 in linear regression, is independent of the base rate (both Cox and Snell and Nagelkerke R2s increase as the proportion of cases increase from 0 to .5) and varies between 0 and 1.\\nR2McF is defined as\\n\\n\\nand is preferred over R2CS by Allison. The two expressions R2McF and R2CS are then related respectively by,\\n\\n\\nHowever, Allison now prefers R2T which is a relatively new measure developed by Tjur. It can be calculated in two steps:\\n\\nFor each level of the dependent variable, find the mean of the predicted probabilities of an event.\\nTake the absolute value of the difference between these means\\nA word of caution is in order when interpreting pseudo-R2 statistics. The reason these indices of fit are referred to as pseudo R2 is that they do not represent the proportionate reduction in error as the R2 in linear regression does. Linear regression assumes homoscedasticity, that the error variance is the same for all values of the criterion. Logistic regression will always be heteroscedastic  the error variances differ for each value of the predicted score. For each value of the predicted score there would be a different value of the proportionate reduction in error. Therefore, it is inappropriate to think of R2 as a proportionate reduction in error in a universal sense in logistic regression.\\n\\nHosmerLemeshow test\\nThe HosmerLemeshow test uses a test statistic that asymptotically follows a  distribution to assess whether or not the observed event rates match expected event rates in subgroups of the model population.  This test is considered to be obsolete by some statisticians because of its dependence on arbitrary binning of predicted probabilities and relative low power.\\n\\nCoefficients\\nAfter fitting the model, it is likely that researchers will want to examine the contribution of individual predictors. To do so, they will want to examine the regression coefficients. In linear regression, the regression coefficients represent the change in the criterion for each unit change in the predictor. In logistic regression, however, the regression coefficients represent the change in the logit for each unit change in the predictor. Given that the logit is not intuitive, researchers are likely to focus on a predictor\\'s effect on the exponential function of the regression coefficient  the odds ratio (see definition). In linear regression, the significance of a regression coefficient is assessed by computing a t test. In logistic regression, there are several different tests designed to assess the significance of an individual predictor, most notably the likelihood ratio test and the Wald statistic.\\n\\nLikelihood ratio test\\nThe likelihood-ratio test discussed above to assess model fit is also the recommended procedure to assess the contribution of individual \"predictors\" to a given model. In the case of a single predictor model, one simply compares the deviance of the predictor model with that of the null model on a chi-square distribution with a single degree of freedom. If the predictor model has significantly smaller deviance (c.f chi-square using the difference in degrees of freedom of the two models), then one can conclude that there is a significant association between the \"predictor\" and the outcome. Although some common statistical packages (e.g. SPSS) do provide likelihood ratio test statistics, without this computationally intensive test it would be more difficult to assess the contribution of individual predictors in the multiple logistic regression case.[] To assess the contribution of individual predictors one can enter the predictors hierarchically, comparing each new model with the previous to determine the contribution of each predictor. There is some debate among statisticians about the appropriateness of so-called \"stepwise\" procedures.[weaselwords] The fear is that they may not preserve nominal statistical properties and may become misleading.\\n\\nWald statistic\\nAlternatively, when assessing the contribution of individual predictors in a given model, one may examine the significance of the Wald statistic. The Wald statistic, analogous to the t-test in linear regression, is used to assess the significance of coefficients. The Wald statistic is the ratio of the square of the regression coefficient to the square of the standard error of the coefficient and is asymptotically distributed as a chi-square distribution.\\n\\n\\nAlthough several statistical packages (e.g., SPSS, SAS) report the Wald statistic to assess the contribution of individual predictors, the Wald statistic has limitations. When the regression coefficient is large, the standard error of the regression coefficient also tends to be larger increasing the probability of Type-II error. The Wald statistic also tends to be biased when data are sparse.\\n\\nCase-control sampling\\nSuppose cases are rare. Then we might wish to sample them more frequently than their prevalence in the population. For example, suppose there is a disease that affects 1 person in 10,000 and to collect our data we need to do a complete physical. It may be too expensive to do thousands of physicals of healthy people in order to obtain data for only a few diseased individuals. Thus, we may evaluate more diseased individuals, perhaps all of the rare outcomes. This is also retrospective sampling, or equivalently it is called unbalanced data. As a rule of thumb, sampling controls at a rate of five times the number of cases will produce sufficient control data.\\nLogistic regression is unique in that it may be estimated on unbalanced data, rather than randomly sampled data, and still yield correct coefficient estimates of the effects of each independent variable on the outcome.  That is to say, if we form a logistic model from such data, if the model is correct in the general population, the  parameters are all correct except for . We can correct  if we know the true prevalence as follows:\\n\\n\\nwhere  is the true prevalence and  is the prevalence in the sample.\\n\\nFormal mathematical specification\\nThere are various equivalent specifications of logistic regression, which fit into different types of more general models.  These different specifications allow for different sorts of useful generalizations.\\n\\nSetup\\nThe basic setup of logistic regression is as follows. We are given a dataset containing N points. Each point i consists of a set of m input variables x1,i ... xm,i (also called independent variables, predictor variables, features, or attributes), and a binary outcome variable Yi (also known as a dependent variable, response variable, output variable, or class), i.e. it can assume only the two possible values 0 (often meaning \"no\" or \"failure\") or 1 (often meaning \"yes\" or \"success\"). The goal of logistic regression is to use the dataset to create a predictive model of the outcome variable.\\nSome examples:\\n\\nThe observed outcomes are the presence or absence of a given disease (e.g. diabetes) in a set of patients, and the explanatory variables might be characteristics of the patients thought to be pertinent (sex, race, age, blood pressure, body-mass index, etc.).\\nThe observed outcomes are the votes (e.g. Democratic or Republican) of a set of people in an election, and the explanatory variables are the demographic characteristics of each person (e.g. sex, race, age, income, etc.).  In such a case, one of the two outcomes is arbitrarily coded as 1, and the other as 0.\\nAs in linear regression, the outcome variables Yi are assumed to depend on the explanatory variables x1,i ... xm,i.\\n\\nExplanatory variables\\nAs shown above in the above examples, the explanatory variables may be of any type: real-valued, binary, categorical, etc.  The main distinction is between continuous variables (such as income, age and blood pressure) and discrete variables (such as sex or race).  Discrete variables referring to more than two possible choices are typically coded using dummy variables (or indicator variables), that is, separate explanatory variables taking the value 0 or 1 are created for each possible value of the discrete variable, with a 1 meaning \"variable does have the given value\" and a 0 meaning \"variable does not have that value\".  For example, a four-way discrete variable of blood type with the possible values \"A, B, AB, O\" can be converted to four separate two-way dummy variables, \"is-A, is-B, is-AB, is-O\", where only one of them has the value 1 and all the rest have the value 0.  This allows for separate regression coefficients to be matched for each possible value of the discrete variable. (In a case like this, only three of the four dummy variables are independent of each other, in the sense that once the values of three of the variables are known, the fourth is automatically determined.  Thus, it is necessary to encode only three of the four possibilities as dummy variables.  This also means that when all four possibilities are encoded, the overall model is not identifiable in the absence of additional constraints such as a regularization constraint.  Theoretically, this could cause problems, but in reality almost all logistic regression models are fitted with regularization constraints.)\\n\\nOutcome variables\\nFormally, the outcomes Yi are described as being Bernoulli-distributed data, where each outcome is determined by an unobserved probability pi that is specific to the outcome at hand, but related to the explanatory variables.  This can be expressed in any of the following equivalent forms:\\n\\n\\nThe meanings of these four lines are:\\n\\nThe first line expresses the probability distribution of each Yi: Conditioned on the explanatory variables, it follows a Bernoulli distribution with parameters pi, the probability of the outcome of 1 for trial i. As noted above, each separate trial has its own probability of success, just as each trial has its own explanatory variables.  The probability of success pi is not observed, only the outcome of an individual Bernoulli trial using that probability.\\nThe second line expresses the fact that the expected value of each Yi is equal to the probability of success pi, which is a general property of the Bernoulli distribution.  In other words, if we run a large number of Bernoulli trials using the same probability of success pi, then take the average of all the 1 and 0 outcomes, then the result would be close to pi.  This is because doing an average this way simply computes the proportion of successes seen, which we expect to converge to the underlying probability of success.\\nThe third line writes out the probability mass function of the Bernoulli distribution, specifying the probability of seeing each of the two possible outcomes.\\nThe fourth line is another way of writing the probability mass function, which avoids having to write separate cases and is more convenient for certain types of calculations.  This relies on the fact that Yi can take only the value 0 or 1.  In each case, one of the exponents will be 1, \"choosing\" the value under it, while the other is 0, \"canceling out\" the value under it.  Hence, the outcome is either pi or 1pi, as in the previous line.\\nLinear predictor function\\nThe basic idea of logistic regression is to use the mechanism already developed for linear regression by modeling the probability pi using a linear predictor function, i.e. a linear combination of the explanatory variables and a set of regression coefficients that are specific to the model at hand but the same for all trials.  The linear predictor function  for a particular data point i is written as:\\n\\n\\nwhere  are regression coefficients indicating the relative effect of a particular explanatory variable on the outcome.\\nThe model is usually put into a more compact form as follows:\\n\\nThe regression coefficients 0, 1, ..., m are grouped into a single vector  of size m+1.\\nFor each data point i, an additional explanatory pseudo-variable x0,i is added, with a fixed value of 1, corresponding to the intercept coefficient 0.\\nThe resulting explanatory variables x0,i, x1,i, ..., xm,i are then grouped into a single vector Xi of size m+1.\\nThis makes it possible to write the linear predictor function as follows:\\n\\n\\nusing the notation for a dot product between two vectors.\\n\\nAs a generalized linear model\\nThe particular model used by logistic regression, which distinguishes it from standard linear regression and from other types of regression analysis used for binary-valued outcomes, is the way the probability of a particular outcome is linked to the linear predictor function:\\n\\n\\nWritten using the more compact notation described above, this is:\\n\\n\\nThis formulation expresses logistic regression as a type of generalized linear model, which predicts variables with various types of probability distributions by fitting a linear predictor function of the above form to some sort of arbitrary transformation of the expected value of the variable.\\nThe intuition for transforming using the logit function (the natural log of the odds) was explained above.  It also has the practical effect of converting the probability (which is bounded to be between 0 and 1) to a variable that ranges over   thereby matching the potential range of the linear prediction function on the right side of the equation.\\nNote that both the probabilities pi and the regression coefficients are unobserved, and the means of determining them is not part of the model itself.  They are typically determined by some sort of optimization procedure, e.g. maximum likelihood estimation, that finds values that best fit the observed data (i.e. that give the most accurate predictions for the data already observed), usually subject to regularization conditions that seek to exclude unlikely values, e.g. extremely large values for any of the regression coefficients.  The use of a regularization condition is equivalent to doing maximum a posteriori (MAP) estimation, an extension of maximum likelihood.  (Regularization is most commonly done using a squared regularizing function, which is equivalent to placing a zero-mean Gaussian prior distribution on the coefficients, but other regularizers are also possible.)  Whether or not regularization is used, it is usually not possible to find a closed-form solution; instead, an iterative numerical method must be used, such as iteratively reweighted least squares (IRLS) or, more commonly these days, a quasi-Newton method such as the L-BFGS method.\\nThe interpretation of the j parameter estimates is as the additive effect on the log of the odds for a unit change in the j the explanatory variable.  In the case of a dichotomous explanatory variable, for instance, gender  is the estimate of the odds of having the outcome for, say, males compared with females.\\nAn equivalent formula uses the inverse of the logit function, which is the logistic function, i.e.:\\n\\n\\nThe formula can also be written as a probability distribution (specifically, using a probability mass function):\\n\\n\\nAs a latent-variable model\\nThe above model has an equivalent formulation as a latent-variable model.  This formulation is common in the theory of discrete choice models and makes it easier to extend to certain more complicated models with multiple, correlated choices, as well as to compare logistic regression to the closely related probit model.\\nImagine that, for each trial i, there is a continuous latent variable Yi* (i.e. an unobserved random variable) that is distributed as follows:\\n\\n\\nwhere\\n\\n\\ni.e. the latent variable can be written directly in terms of the linear predictor function and an additive random error variable that is distributed according to a standard logistic distribution.\\nThen Yi can be viewed as an indicator for whether this latent variable is positive:\\n\\n\\nThe choice of modeling the error variable specifically with a standard logistic distribution, rather than a general logistic distribution with the location and scale set to arbitrary values, seems restrictive, but in fact, it is not.  It must be kept in mind that we can choose the regression coefficients ourselves, and very often can use them to offset changes in the parameters of the error variable\\'s distribution.  For example, a logistic error-variable distribution with a non-zero location parameter  (which sets the mean) is equivalent to a distribution with a zero location parameter, where  has been added to the intercept coefficient.  Both situations produce the same value for Yi* regardless of settings of explanatory variables.  Similarly, an arbitrary scale parameter s is equivalent to setting the scale parameter to 1 and then dividing all regression coefficients by s.  In the latter case, the resulting value of Yi* will be smaller by a factor of s than in the former case, for all sets of explanatory variables  but critically, it will always remain on the same side of 0, and hence lead to the same Yi choice.\\n(Note that this predicts that the irrelevancy of the scale parameter may not carry over into more complex models where more than two choices are available.)\\nIt turns out that this formulation is exactly equivalent to the preceding one, phrased in terms of the generalized linear model and without any latent variables.  This can be shown as follows, using the fact that the cumulative distribution function (CDF) of the standard logistic distribution is the logistic function, which is the inverse of the logit function, i.e.\\n\\n\\nThen:\\n\\n\\nThis formulationwhich is standard in discrete choice modelsmakes clear the relationship between logistic regression (the \"logit model\") and the probit model, which uses an error variable distributed according to a standard normal distribution instead of a standard logistic distribution.  Both the logistic and normal distributions are symmetric with a basic unimodal, \"bell curve\" shape.  The only difference is that the logistic distribution has somewhat heavier tails, which means that it is less sensitive to outlying data (and hence somewhat more robust to model mis-specifications or erroneous data).\\n\\nTwo-way latent-variable model\\nYet another formulation uses two separate latent variables:\\n\\n\\nwhere\\n\\n\\nwhere EV1(0,1) is a standard type-1 extreme value distribution: i.e.\\n\\n\\nThen\\n\\n\\nThis model has a separate latent variable and a separate set of regression coefficients for each possible outcome of the dependent variable.  The reason for this separation is that it makes it easy to extend logistic regression to multi-outcome categorical variables, as in the multinomial logit model. In such a model, it is natural to model each possible outcome using a different set of regression coefficients.  It is also possible to motivate each of the separate latent variables as the theoretical utility associated with making the associated choice, and thus motivate logistic regression in terms of utility theory. (In terms of utility theory, a rational actor always chooses the choice with the greatest associated utility.) This is the approach taken by economists when formulating discrete choice models, because it both provides a theoretically strong foundation and facilitates intuitions about the model, which in turn makes it easy to consider various sorts of extensions. (See the example below.)\\nThe choice of the type-1 extreme value distribution seems fairly arbitrary, but it makes the mathematics work out, and it may be possible to justify its use through rational choice theory.\\nIt turns out that this model is equivalent to the previous model, although this seems non-obvious, since there are now two sets of regression coefficients and error variables, and the error variables have a different distribution.  In fact, this model reduces directly to the previous one with the following substitutions:\\n\\n\\n\\nAn intuition for this comes from the fact that, since we choose based on the maximum of two values, only their difference matters, not the exact values  and this effectively removes one degree of freedom. Another critical fact is that the difference of two type-1 extreme-value-distributed variables is a logistic distribution, i.e.  We can demonstrate the equivalent as follows:\\n\\n\\nExample\\nAs an example, consider a province-level election where the choice is between a right-of-center party, a left-of-center party, and a secessionist party (e.g. the Parti Qubcois, which wants Quebec to secede from Canada).  We would then use three latent variables, one for each choice.  Then, in accordance with utility theory, we can then interpret the latent variables as expressing the utility that results from making each of the choices.  We can also interpret the regression coefficients as indicating the strength that the associated factor (i.e. explanatory variable) has in contributing to the utility  or more correctly, the amount by which a unit change in an explanatory variable changes the utility of a given choice.  A voter might expect that the right-of-center party would lower taxes, especially on rich people.  This would give low-income people no benefit, i.e. no change in utility (since they usually don\\'t pay taxes); would cause moderate benefit (i.e. somewhat more money, or moderate utility increase) for middle-incoming people; would cause significant benefits for high-income people.  On the other hand, the left-of-center party might be expected to raise taxes and offset it with increased welfare and other assistance for the lower and middle classes.  This would cause significant positive benefit to low-income people, perhaps a weak benefit to middle-income people, and significant negative benefit to high-income people.  Finally, the secessionist party would take no direct actions on the economy, but simply secede. A low-income or middle-income voter might expect basically no clear utility gain or loss from this, but a high-income voter might expect negative utility since he/she is likely to own companies, which will have a harder time doing business in such an environment and probably lose money.\\nThese intuitions can be expressed as follows:\\n\\n\\nEstimated strength of regression coefficient for different outcomes (party choices) and different values of explanatory variables\\n\\n\\n\\nCenter-right\\nCenter-left\\nSecessionist\\n\\n\\nHigh-income\\n\\nstrong +\\nstrong \\nstrong \\n\\n\\nMiddle-income\\n\\nmoderate +\\nweak +\\nnone\\n\\n\\nLow-income\\n\\nnone\\nstrong +\\nnone\\n\\n\\nThis clearly shows that\\n\\nSeparate sets of regression coefficients need to exist for each choice.  When phrased in terms of utility, this can be seen very easily. Different choices have different effects on net utility; furthermore, the effects vary in complex ways that depend on the characteristics of each individual, so there need to be separate sets of coefficients for each characteristic, not simply a single extra per-choice characteristic.\\nEven though income is a continuous variable, its effect on utility is too complex for it to be treated as a single variable.  Either it needs to be directly split up into ranges, or higher powers of income need to be added so that polynomial regression on income is effectively done.\\nAs a \"log-linear\" model\\nYet another formulation combines the two-way latent variable formulation above with the original formulation higher up without latent variables, and in the process provides a link to one of the standard formulations of the multinomial logit.\\nHere, instead of writing the logit of the probabilities pi as a linear predictor, we separate the linear predictor into two, one for each of the two outcomes:\\n\\n\\nNote that two separate sets of regression coefficients have been introduced, just as in the two-way latent variable model, and the two equations appear a form that writes the logarithm of the associated probability as a linear predictor, with an extra term  at the end.  This term, as it turns out, serves as the normalizing factor ensuring that the result is a distribution.  This can be seen by exponentiating both sides:\\n\\n\\nIn this form it is clear that the purpose of Z is to ensure that the resulting distribution over Yi is in fact a probability distribution, i.e. it sums to 1.  This means that Z is simply the sum of all un-normalized probabilities, and by dividing each probability by Z, the probabilities become \"normalized\".  That is:\\n\\n\\nand the resulting equations are\\n\\n\\nOr generally:\\n\\n\\nThis shows clearly how to generalize this formulation to more than two outcomes, as in multinomial logit.\\nNote that this general formulation is exactly the softmax function as in\\n\\n\\nIn order to prove that this is equivalent to the previous model, note that the above model is overspecified, in that  and  cannot be independently specified: rather  so knowing one automatically determines the other.  As a result, the model is nonidentifiable, in that multiple combinations of 0 and 1 will produce the same probabilities for all possible explanatory variables.  In fact, it can be seen that adding any constant vector to both of them will produce the same probabilities:\\n\\n\\nAs a result, we can simplify matters, and restore identifiability, by picking an arbitrary value for one of the two vectors.  We choose to set   Then,\\n\\n\\nand so\\n\\n\\nwhich shows that this formulation is indeed equivalent to the previous formulation. (As in the two-way latent variable formulation, any settings where  will produce equivalent results.)\\nNote that most treatments of the multinomial logit model start out either by extending the \"log-linear\" formulation presented here or the two-way latent variable formulation presented above, since both clearly show the way that the model could be extended to multi-way outcomes.  In general, the presentation with latent variables is more common in econometrics and political science, where discrete choice models and utility theory reign, while the \"log-linear\" formulation here is more common in computer science, e.g. machine learning and natural language processing.\\n\\nAs a single-layer perceptron\\nThe model has an equivalent formulation\\n\\n\\nThis functional form is commonly called a single-layer perceptron or single-layer artificial neural network. A single-layer neural network computes a continuous output instead of a step function. The derivative of pi with respect to  X=(x1, ..., xk) is computed from the general form:\\n\\n\\nwhere f(X) is an analytic function in X. With this choice, the single-layer neural network is identical to the logistic regression model. This function has a continuous derivative, which allows it to be used in backpropagation. This function is also preferred because its derivative is easily calculated:\\n\\n\\nIn terms of binomial data\\nA closely related model assumes that each i is associated not with a single Bernoulli trial but with ni independent identically distributed trials, where the observation Yi is the number of successes observed (the sum of the individual Bernoulli-distributed random variables), and hence follows a binomial distribution:\\n\\n\\nAn example of this distribution is the fraction of seeds (pi) that germinate after ni are planted.\\nIn terms of expected values, this model is expressed as follows:\\n\\n\\nso that\\n\\n\\nOr equivalently:\\n\\n\\nThis model can be fit using the same sorts of methods as the above more basic model.\\n\\nBayesian\\n Comparison of logistic function with a scaled inverse probit function (i.e. the CDF of the normal distribution), comparing  vs. , which makes the slopes the same at the origin.  This shows the heavier tails of the logistic distribution.\\nIn a Bayesian statistics context, prior distributions are normally placed on the regression coefficients, usually in the form of Gaussian distributions.  There is no conjugate prior of the likelihood function in logistic regression.  When Bayesian inference was performed analytically, this made the posterior distribution difficult to calculate except in very low dimensions.  Now, though, automatic software such as OpenBUGS, JAGS, PyMC3 or Stan allows these posteriors to be computed using simulation, so lack of conjugacy is not a concern.  However, when the sample size or the number of parameters is large, full Bayesian simulation can be slow, and people often use approximate methods such as variational Bayesian methods and expectation propagation.\\n\\nHistory\\nA detailed history of the logistic regression is given in Cramer (2002). The logistic function was developed as a model of population growth and named \"logistic\" by Pierre Franois Verhulst in the 1830s and 1840s, under the guidance of Adolphe Quetelet; see Logistic function History for details. In his earliest paper (1838), Verhulst did not specify how he fit the curves to the data. In his more detailed paper (1845), Verhulst determined the three parameters of the model by making the curve pass through three observed points, which yielded poor predictions.\\nThe logistic function was independently developed in chemistry as a model of autocatalysis (Wilhelm Ostwald, 1883). An autocatalytic reaction is one in which one of the products is itself a catalyst for the same reaction, while the supply of one of the reactants is fixed. This naturally gives rise to the logistic equation for the same reason as population growth: the reaction is self-reinforcing but constrained.\\nThe logistic function was independently rediscovered as a model of population growth in 1920 by Raymond Pearl and Lowell Reed, published as Pearl & Reed (1920), which led to its use in modern statistics. They were initially unaware of Verhulst\\'s work and presumably learned about it from L. Gustave du Pasquier, but they gave him little credit and did not adopt his terminology. Verhulst\\'s priority was acknowledged and the term \"logistic\" revived by Udny Yule in 1925 and has been followed since. Pearl and Reed first applied the model to the population of the United States, and also initially fitted the curve by making it pass through three points; as with Verhulst, this again yielded poor results.\\nIn the 1930s, the probit model was developed and systematized by Chester Ittner Bliss, who coined the term \"probit\" in Bliss (1934), and by John Gaddum in Gaddum (1933), and the model fit by maximum likelihood estimation by Ronald A. Fisher in Fisher (1935), as an addendum to Bliss\\'s work. The probit model was principally used in bioassay, and had been preceded by earlier work dating to 1860; see Probit model History. The probit model influenced the subsequent development of the logit model and these models competed with each other.\\nThe logistic model was likely first used as an alternative to the probit model in bioassay by Edwin Bidwell Wilson and his student Jane Worcester in Wilson & Worcester (1943). However, the development of the logistic model as a general alternative to the probit model was principally due to the work of Joseph Berkson over many decades, beginning in Berkson (1944), where he coined \"logit\", by analogy with \"probit\", and continuing through Berkson (1951) and following years. The logit model was initially dismissed as inferior to the probit model, but \"gradually achieved an equal footing with the logit\", particularly between 1960 and 1970. By 1970, the logit model achieved parity with the probit model in use in statistics journals and thereafter surpassed it. This relative popularity was due to the adoption of the logit outside of bioassay, rather than displacing the probit within bioassay, and its informal use in practice; the logit\\'s popularity is credited to the logit model\\'s computational simplicity, mathematical properties, and generality, allowing its use in varied fields.\\nVarious refinements occurred during that time, notably by David Cox, as in Cox (1958).\\nThe multinomial logit model was introduced independently in Cox (1966) and Thiel (1969), which greatly increased the scope of application and the popularity of the logit model. In 1973 Daniel McFadden linked the multinomial logit to the theory of discrete choice, specifically Luce\\'s choice axiom, showing that the multinomial logit followed from the assumption of independence of irrelevant alternatives and interpreting odds of alternatives as relative preferences; this gave a theoretical foundation for the logistic regression.\\n\\nExtensions\\nThere are large numbers of extensions:\\n\\nMultinomial logistic regression (or multinomial logit) handles the case of a multi-way categorical dependent variable (with unordered values, also called \"classification\").  Note that the general case of having dependent variables with more than two values is termed polytomous regression.\\nOrdered logistic regression (or ordered logit) handles ordinal dependent variables (ordered values).\\nMixed logit is an extension of multinomial logit that allows for correlations among the choices of the dependent variable.\\nAn extension of the logistic model to sets of interdependent variables is the conditional random field.\\nConditional logistic regression handles matched or stratified data when the strata are small. It is mostly used in the analysis of observational studies.\\nSoftware\\nMost statistical software can do binary logistic regression.\\n\\nSPSS\\n[1] for basic logistic regression.\\nStata\\nSAS\\nPROC LOGISTIC for basic logistic regression.\\nPROC CATMOD when all the variables are categorical.\\nPROC GLIMMIX for multilevel model logistic regression.\\nR\\nglm in the stats package (using family = binomial)\\nlrm in the rms package\\nGLMNET package for an efficient implementation regularized logistic regression\\nlmer for mixed effects logistic regression\\nRfast package command gm_logistic for fast and heavy calculations involving large scale data.\\narm package for bayesian logistic regression\\nPython\\nLogit in the Statsmodels module.\\nLogisticRegression in the Scikit-learn module.\\nLogisticRegressor in the TensorFlow module.\\nFull example of logistic regression in the Theano tutorial [2]\\nBayesian Logistic Regression with ARD prior code, tutorial\\nVariational Bayes Logistic Regression with ARD prior code , tutorial\\nBayesian Logistic Regression  code, tutorial\\nNCSS\\nLogistic Regression in NCSS\\nMatlab\\nmnrfit in the Statistics and Machine Learning Toolbox (with \"incorrect\" coded as 2 instead of 0)\\nfminunc/fmincon, fitglm, mnrfit, fitclinear, mle can all do logistic regression.\\nJava (JVM)\\nLibLinear\\nApache Flink\\nApache Spark\\nSparkML supports Logistic Regression\\nFPGA\\nLogistic Regresesion IP core in HLS for FPGA.\\nNotably, Microsoft Excel\\'s statistics extension package does not include it.\\n\\n\\n\\nLogistic function\\nDiscrete choice\\nJarrowTurnbull model\\nLimited dependent variable\\nMultinomial logit model\\nOrdered logit\\nHosmerLemeshow test\\nBrier score\\nmlpack - contains a C++ implementation of logistic regression\\nLocal case-control sampling\\nLogistic model tree\\n\\n\\n\\n\\n\\n\\n\\n\\nWikiversity has learning resources about Logistic regression\\n\\n Media related to Logistic regression at Wikimedia Commons\\nEconometrics Lecture (topic: Logit model) on YouTube by Mark Thoma\\nLogistic Regression tutorial\\nmlelr: software in C for teaching purposes'\n",
      "\n",
      "https://en.wikipedia.org/wiki/Normal_distribution#Standard_normal_distribution \t\t\t\t 9975\n",
      "The simplest case of a normal distribution is known as the standard normal distribution. This is a special case when  and , and it is described by this probability density function:\n",
      "\n",
      "\n",
      "The factor  in this expression ensures that the total area under the curve  is equal to one. The factor  in the exponent ensures that the distribution has unit variance (i.e. the variance is equal to one), and therefore also unit standard deviation. This function is symmetric around , where it attains its maximum value  and has inflection points at  and .\n",
      "Authors differ on which normal distribution should be called the \"standard\" one. Gauss defined the standard normal as having variance , that is\n",
      "\n",
      "\n",
      "Stigler goes even further, defining the standard normal with variance :\n",
      "\n",
      "\n",
      "General normal distribution\n",
      "Every normal distribution is a version of the standard normal distribution whose domain has been stretched by a factor  (the standard deviation) and then translated by  (the mean value):\n",
      "\n",
      "\n",
      "The probability density must be scaled by  so that the integral is still 1.\n",
      "If  is a standard normal deviate, then  will have a normal distribution with expected value  and standard deviation . Conversely, if  is a normal deviate with parameters  and , then  will have a standard normal distribution. This variate is called the standardized form of .\n",
      "\n",
      "Notation\n",
      "The probability density of the standard Gaussian distribution (standard normal distribution) (with zero mean and unit variance) is often denoted with the Greek letter  (phi). The alternative form of the Greek letter phi, , is also used quite often.\n",
      "The normal distribution is often referred to as  or . Thus when a random variable  is distributed normally with mean  and variance , one may write\n",
      "\n",
      "\n",
      "Alternative parameterizations\n",
      "Some authors advocate using the precision  as the parameter defining the width of the distribution, instead of the deviation  or the variance . The precision is normally defined as the reciprocal of the variance, . The formula for the distribution then becomes\n",
      "\n",
      "\n",
      "This choice is claimed to have advantages in numerical computations when  is very close to zero and simplify formulas in some contexts, such as in the Bayesian inference of variables with multivariate normal distribution.\n",
      "Also the reciprocal of the standard deviation  might be defined as the precision and the expression of the normal distribution becomes\n",
      "\n",
      "\n",
      "According to Stigler, this formulation is advantageous because of a much simpler and easier-to-remember formula, and simple approximate formulas for the quantiles of the distribution.\n",
      "Normal distributions form an exponential family with natural parameters  and , and natural statistics x and x2. The dual, expectation parameters for normal distribution are η1 = μ and η2 = μ2 + σ2.\n",
      "\n",
      "Cumulative distribution function\n",
      "The cumulative distribution function (CDF) of the standard normal distribution, usually denoted with the capital Greek letter  (phi), is the integral\n",
      "\n",
      "\n",
      "The related error function  gives the probability of a random variable with normal distribution of mean 0 and variance 1/2 falling in the range ; that is\n",
      "\n",
      "\n",
      "These integrals cannot be expressed in terms of elementary functions, and are often said to be special functions. However, many numerical approximations are known; see below.\n",
      "The two functions are closely related, namely\n",
      "\n",
      "\n",
      "For a generic normal distribution with density , mean  and deviation , the cumulative distribution function is\n",
      "\n",
      "\n",
      "The complement of the standard normal CDF, , is often called the Q-function, especially in engineering texts. It gives the probability that the value of a standard normal random variable  will exceed : . Other definitions of the -function, all of which are simple transformations of , are also used occasionally.\n",
      "The graph of the standard normal CDF  has 2-fold rotational symmetry around the point (0,1/2); that is, . Its antiderivative (indefinite integral) is \n",
      "\n",
      "\n",
      "The CDF of the standard normal distribution can be expanded by Integration by parts into a series:\n",
      "\n",
      "\n",
      "where  denotes the double factorial.\n",
      "An asymptotic expansion of the CDF for large x can also be derived using integration by parts; see Error function#Asymptotic expansion.\n",
      "\n",
      "Standard deviation and coverage\n",
      "\n",
      " For the normal distribution, the values less than one standard deviation away from the mean account for 68.27% of the set; while two standard deviations from the mean account for 95.45%; and three standard deviations account for 99.73%.\n",
      "About 68% of values drawn from a normal distribution are within one standard deviation σ away from the mean; about 95% of the values lie within two standard deviations; and about 99.7% are within three standard deviations. This fact is known as the 68-95-99.7 (empirical) rule, or the 3-sigma rule.\n",
      "More precisely, the probability that a normal deviate lies in the range between  and  is given by\n",
      "\n",
      "\n",
      "To 12 significant figures, the values for  are:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "OEIS\n",
      "\n",
      "\n",
      "1\n",
      "0.682689492137\n",
      "0.317310507863\n",
      "\n",
      "\n",
      "\n",
      "3\n",
      ".15148718753\n",
      "\n",
      "\n",
      "OEIS: A178647\n",
      "\n",
      "\n",
      "2\n",
      "0.954499736104\n",
      "0.045500263896\n",
      "\n",
      "\n",
      "\n",
      "21\n",
      ".9778945080\n",
      "\n",
      "\n",
      "OEIS: A110894\n",
      "\n",
      "\n",
      "3\n",
      "0.997300203937\n",
      "0.002699796063\n",
      "\n",
      "\n",
      "\n",
      "370\n",
      ".398347345\n",
      "\n",
      "\n",
      "OEIS: A270712\n",
      "\n",
      "\n",
      "4\n",
      "0.999936657516\n",
      "0.000063342484\n",
      "\n",
      "\n",
      "\n",
      "15787\n",
      ".1927673\n",
      "\n",
      "\n",
      "\n",
      "5\n",
      "0.999999426697\n",
      "0.000000573303\n",
      "\n",
      "\n",
      "\n",
      "1744277\n",
      ".89362\n",
      "\n",
      "\n",
      "\n",
      "6\n",
      "0.999999998027\n",
      "0.000000001973\n",
      "\n",
      "\n",
      "\n",
      "506797345\n",
      ".897\n",
      "\n",
      "\n",
      "Quantile function\n",
      "\n",
      "The quantile function of a distribution is the inverse of the cumulative distribution function.  The quantile function of the standard normal distribution is called the probit function, and can be expressed in terms of the inverse error function:\n",
      "\n",
      "\n",
      "For a normal random variable with mean  and variance , the quantile function is\n",
      "\n",
      "\n",
      "The quantile  of the standard normal distribution is commonly denoted as . These values are used in hypothesis testing, construction of confidence intervals and Q-Q plots. A normal random variable  will exceed  with probability , and will lie outside the interval  with probability . In particular, the quantile  is 1.96; therefore a normal random variable will lie outside the interval  in only 5% of cases.\n",
      "The following table gives the quantile  such that  will lie in the range  with a specified probability . These values are useful to determine tolerance interval for sample averages and other statistical estimators with normal (or asymptotically normal) distributions:. NOTE: the following table shows , not  as defined above.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "0.80\n",
      "1.281551565545\n",
      "0.999\n",
      "3.290526731492\n",
      "\n",
      "\n",
      "0.90\n",
      "1.644853626951\n",
      "0.9999\n",
      "3.890591886413\n",
      "\n",
      "\n",
      "0.95\n",
      "1.959963984540\n",
      "0.99999\n",
      "4.417173413469\n",
      "\n",
      "\n",
      "0.98\n",
      "2.326347874041\n",
      "0.999999\n",
      "4.891638475699\n",
      "\n",
      "\n",
      "0.99\n",
      "2.575829303549\n",
      "0.9999999\n",
      "5.326723886384\n",
      "\n",
      "\n",
      "0.995\n",
      "2.807033768344\n",
      "0.99999999\n",
      "5.730728868236\n",
      "\n",
      "\n",
      "0.998\n",
      "3.090232306168\n",
      "0.999999999\n",
      "6.109410204869\n",
      "\n",
      "For small , the quantile function has the useful asymptotic expansion \n",
      "\n",
      "\n",
      "Properties\n",
      "The normal distribution is the only distribution whose cumulants beyond the first two (i.e., other than the mean and variance) are zero. It is also the continuous distribution with the maximum entropy for a specified mean and variance. Geary has shown, assuming that the mean and variance are finite, that the normal distribution is the only distribution where the mean and variance calculated from a set of independent draws are independent of each other.\n",
      "The normal distribution is a subclass of the elliptical distributions. The normal distribution is symmetric about its mean, and is non-zero over the entire real line. As such it may not be a suitable model for variables that are inherently positive or strongly skewed, such as the weight of a person or the price of a share. Such variables may be better described by other distributions, such as the log-normal distribution or the Pareto distribution.\n",
      "The value of the normal distribution is practically zero when the value  lies more than a few standard deviations away from the mean (e.g., a spread of three standard deviations covers all but 0.27% of the total distribution). Therefore, it may not be an appropriate model when one expects a significant fraction of outliers—values that lie many standard deviations away from the mean—and least squares and other statistical inference methods that are optimal for normally distributed variables often become highly unreliable when applied to such data. In those cases, a more heavy-tailed distribution should be assumed and the appropriate robust statistical inference methods applied.\n",
      "The Gaussian distribution belongs to the family of stable distributions which are the attractors of sums of independent, identically distributed distributions whether or not the mean or variance is finite. Except for the Gaussian which is a limiting case, all stable distributions have heavy tails and infinite variance. It is one of the few distributions that are stable and that have probability density functions that can be expressed analytically, the others being the Cauchy distribution and the Lévy distribution.\n",
      "\n",
      "Symmetries and derivatives\n",
      "The normal distribution with density  (mean  and standard deviation ) has the following properties:\n",
      "\n",
      "It is symmetric around the point  which is at the same time the mode, the median and the mean of the distribution.\n",
      "It is unimodal: its first derivative is positive for  negative for  and zero only at \n",
      "The area under the curve and over the -axis is unity (i.e. equal to one).\n",
      "Its density has two inflection points (where the second derivative of  is zero and changes sign), located one standard deviation away from the mean, namely at  and \n",
      "Its density is log-concave.\n",
      "Its density is infinitely differentiable, indeed supersmooth of order 2.\n",
      "Furthermore, the density  of the standard normal distribution (i.e.  and ) also has the following properties:\n",
      "\n",
      "Its first derivative is \n",
      "Its second derivative is \n",
      "More generally, its th derivative is  where  is the th (probabilist) Hermite polynomial.\n",
      "The probability that a normally distributed variable  with known  and  is in a particular set, can be calculated by using the fact that the fraction  has a standard normal distribution.\n",
      "Moments\n",
      "\n",
      "The plain and absolute moments of a variable  are the expected values of  and , respectively. If the expected value  of  is zero, these parameters are called central moments.  Usually we are interested only in moments with integer order .\n",
      "If  has a normal distribution, these moments exist and are finite for any  whose real part is greater than −1. For any non-negative integer , the plain central moments are:\n",
      "\n",
      "\n",
      "Here  denotes the double factorial, that is, the product of all numbers from  to 1 that have the same parity as \n",
      "The central absolute moments coincide with plain moments for all even orders, but are nonzero for odd orders. For any non-negative integer \n",
      "\n",
      "\n",
      "The last formula is valid also for any non-integer   When the mean  the plain and absolute moments can be expressed in terms of confluent hypergeometric functions  and []\n",
      "\n",
      "\n",
      "These expressions remain valid even if  is not integer. See also generalized Hermite polynomials.\n",
      "\n",
      "\n",
      "\n",
      "Order\n",
      "Non-central moment\n",
      "Central moment\n",
      "\n",
      "\n",
      "1\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "2\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "3\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "4\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "5\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "6\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "7\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "8\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "The expectation of  conditioned on the event that  lies in an interval  is given by\n",
      "\n",
      "\n",
      "where  and  respectively are the density and the cumulative distribution function of . For  this is known as the inverse Mills ratio. Note that above, density  of  is used instead of standard normal density as in inverse Mills ratio, so here we have  instead of .\n",
      "\n",
      "Fourier transform and characteristic function\n",
      "The Fourier transform of a normal density  with mean  and standard deviation  is\n",
      "\n",
      "\n",
      "where  is the imaginary unit. If the mean , the first factor is 1, and the Fourier transform is, apart from a constant factor, a normal density on the frequency domain, with mean 0 and standard deviation . In particular, the standard normal distribution  is an eigenfunction of the Fourier transform.\n",
      "In probability theory, the Fourier transform of the probability distribution of a real-valued random variable  is closely connected to the characteristic function  of that variable, which is defined as the expected value of , as a function of the real variable  (the frequency parameter of the Fourier transform). This definition can be analytically extended to a complex-value variable . The relation between both is:\n",
      "\n",
      "\n",
      "Moment and cumulant generating functions\n",
      "The moment generating function of a real random variable  is the expected value of , as a function of the real parameter . For a normal distribution with density , mean  and deviation , the moment generating function exists and is equal to\n",
      "\n",
      "\n",
      "The cumulant generating function is the logarithm of the moment generating function, namely\n",
      "\n",
      "\n",
      "Since this is a quadratic polynomial in , only the first two cumulants are nonzero, namely the mean  and the variance .\n",
      "\n",
      "Stein operator and class\n",
      "Within Stein's method the Stein operator  and class of a random variable   are  and .\n",
      "\n",
      "Zero-variance limit\n",
      "In the limit when  tends to zero, the probability density  eventually tends to zero at any , but grows without limit if , while its integral remains equal to 1. Therefore, the normal distribution cannot be defined as an ordinary function when .\n",
      "However, one can define the normal distribution with zero variance as a generalized function; specifically, as Dirac's \"delta function\"  translated by the mean , that is \n",
      "Its CDF is then the Heaviside step function translated by the mean , namely\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Maximum entropy\n",
      "Of all probability distributions over the reals with a specified mean  and variance , the normal distribution  is the one with maximum entropy. If  is a continuous random variable with probability density , then the entropy of  is defined as\n",
      "\n",
      "\n",
      "where  is understood to be zero whenever . This functional can be maximized, subject to the constraints that the distribution is properly normalized and has a specified variance, by using variational calculus. A function with two Lagrange multipliers is defined:\n",
      "\n",
      "\n",
      "where  is, for now, regarded as some density function with mean  and standard deviation .\n",
      "At maximum entropy, a small variation  about  will produce a variation  about  which is equal to 0:\n",
      "\n",
      "\n",
      "Since this must hold for any small , the term in brackets must be zero, and solving for  yields:\n",
      "\n",
      "\n",
      "Using the constraint equations to solve for  and  yields the density of the normal distribution:\n",
      "\n",
      "\n",
      "The entropy of normal distribution equals to\n",
      "\n",
      "\n",
      "Operations on normal deviates\n",
      "The family of normal distributions is closed under linear transformations: if   is normally distributed with mean  and standard deviation , then the variable  , for any real numbers  and , is also normally distributed, with\n",
      "mean  and standard deviation .\n",
      "Also if  and  are two independent normal random variables, with means ,  and standard deviations , , then their sum  will also be normally distributed,[proof] with mean  and variance .\n",
      "In particular, if  and  are independent normal deviates with zero mean and variance , then  and  are also independent and normally distributed, with zero mean and variance . This is a special case of the polarization identity.\n",
      "Also, if ,  are two independent normal deviates with mean  and deviation , and ,  are arbitrary real numbers, then the variable\n",
      "\n",
      "\n",
      "is also normally distributed with mean  and deviation . It follows that the normal distribution is stable (with exponent ).\n",
      "More generally, any linear combination of independent normal deviates is a normal deviate.\n",
      "\n",
      "Infinite divisibility and Cramér's theorem\n",
      "For any positive integer , any normal distribution with mean  and variance  is the distribution of the sum of  independent normal deviates, each with mean  and variance .  This property is called infinite divisibility.\n",
      "Conversely, if  and  are independent random variables and their sum  has a normal distribution, then both  and  must be normal deviates.\n",
      "This result is known as Cramér’s decomposition theorem, and is equivalent to saying that the convolution of two distributions is normal if and only if both are normal. Cramér's theorem implies that a linear combination of independent non-Gaussian variables will never have an exactly normal distribution, although it may approach it arbitrarily closely.\n",
      "\n",
      "Bernstein's theorem\n",
      "Bernstein's theorem states that if  and  are independent and  and  are also independent, then both X and Y must necessarily have normal distributions.\n",
      "More generally, if  are independent random variables, then two distinct linear combinations  and will be independent if and only if all  are normal and , where  denotes the variance of .\n",
      "\n",
      "Other properties\n",
      "If the characteristic function  of some random variable  is of the form , where  is a polynomial, then the Marcinkiewicz theorem (named after Józef Marcinkiewicz) asserts that  can be at most a quadratic polynomial, and therefore  is a normal random variable. The consequence of this result is that the normal distribution is the only distribution with a finite number (two) of non-zero cumulants.If  and  are jointly normal and uncorrelated, then they are independent. The requirement that  and  should be jointly normal is essential; without it the property does not hold.[proof] For non-normal random variables uncorrelatedness does not imply independence.The Kullback–Leibler divergence of one normal distribution  from another  is given by:\n",
      "\n",
      "The Hellinger distance between the same distributions is equal to\n",
      "\n",
      "The Fisher information matrix for a normal distribution is diagonal and takes the form\n",
      "The conjugate prior of the mean of a normal distribution is another normal distribution. Specifically, if  are iid  and the prior is , then the posterior distribution for the estimator of  will be\n",
      "The family of normal distributions not only forms an exponential family (EF), but in fact forms a natural exponential family (NEF) with quadratic variance function (NEF-QVF). Many properties of normal distributions generalize to properties of NEF-QVF distributions, NEF distributions, or EF distributions generally. NEF-QVF distributions comprises 6 families, including Poisson, Gamma, binomial, and negative binomial distributions, while many of the common families studied in probability and statistics are NEF or EF.In information geometry, the family of normal distributions forms a statistical manifold with constant curvature . The same family is flat with respect to the (±1)-connections ∇ and ∇.\n",
      "Related distributions\n",
      "Central limit theorem\n",
      " As the number of discrete events increases, the function begins to resemble a normal distribution\n",
      " Comparison of probability density functions,  for the sum of  fair 6-sided dice to show their convergence to a normal distribution with increasing , in accordance to the central limit theorem. In the bottom-right graph, smoothed profiles of the previous graphs are rescaled, superimposed and compared with a normal distribution (black curve).\n",
      "\n",
      "The central limit theorem states that under certain (fairly common) conditions, the sum of many random variables will have an approximately normal distribution. More specifically, where  are independent and identically distributed random variables with the same arbitrary distribution, zero mean, and variance  and  is their\n",
      "mean scaled by \n",
      "\n",
      "\n",
      "Then, as  increases, the probability distribution of  will tend to the normal distribution with zero mean and variance .\n",
      "The theorem can be extended to variables  that are not independent and/or not identically distributed if certain constraints are placed on the degree of dependence and the moments of the distributions.\n",
      "Many test statistics, scores, and estimators encountered in practice contain sums of certain random variables in them, and even more estimators can be represented as sums of random variables through the use of influence functions.  The central limit theorem implies that those statistical parameters will have asymptotically normal distributions.\n",
      "The central limit theorem also implies that certain distributions can be approximated by the normal distribution, for example:\n",
      "\n",
      "The binomial distribution  is approximately normal with mean  and variance  for large  and for  not too close to 0 or 1.\n",
      "The Poisson distribution with parameter  is approximately normal with mean  and variance , for large values of .\n",
      "The chi-squared distribution  is approximately normal with mean  and variance , for large .\n",
      "The Student's t-distribution  is approximately normal with mean 0 and variance 1 when  is large.\n",
      "Whether these approximations are sufficiently accurate depends on the purpose for which they are needed, and the rate of convergence to the normal distribution. It is typically the case that such approximations are less accurate in the tails of the distribution.\n",
      "A general upper bound for the approximation error in the central limit theorem is given by the Berry–Esseen theorem, improvements of the approximation are given by the Edgeworth expansions.\n",
      "\n",
      "Operations on a single random variable\n",
      "If X is distributed normally with mean μ and variance σ2, then\n",
      "\n",
      "The exponential of X is distributed log-normally: eX ~ ln(N (μ, σ2)).\n",
      "The absolute value of X has folded normal distribution: |X| ~ Nf (μ, σ2). If μ = 0 this is known as the half-normal distribution.\n",
      "The absolute value of normalized residuals, |X − μ|/σ, has chi distribution with one degree of freedom: |X − μ|/σ ~ .\n",
      "The square of X/σ has the noncentral chi-squared distribution with one degree of freedom: X2/σ2 ~ (μ2/σ2). If μ = 0, the distribution is called simply chi-squared.\n",
      "The distribution of the variable X restricted to an interval [a, b] is called the truncated normal distribution.\n",
      "(X − μ)−2 has a Lévy distribution with location 0 and scale σ−2.\n",
      "Combination of two independent random variables\n",
      "If  and  are two independent standard normal random variables with mean 0 and variance 1, then\n",
      "\n",
      "Their sum and difference is distributed normally with mean zero and variance two: .\n",
      "Their product  follows the \"product-normal\" distribution with density function  where  is the modified Bessel function of the second kind. This distribution is symmetric around zero, unbounded at , and has the characteristic function .\n",
      "Their ratio follows the standard Cauchy distribution: .\n",
      "Their Euclidean norm  has the Rayleigh distribution.\n",
      "Combination of two or more independent random variables\n",
      "If  are independent standard normal random variables, then the sum of their squares has the chi-squared distribution with  degrees of freedom\n",
      "\n",
      "If  are independent normally distributed random variables with means  and variances , then their sample mean is independent from the sample standard deviation, which can be demonstrated using Basu's theorem or Cochran's theorem. The ratio of these two quantities will have the Student's t-distribution with  degrees of freedom:\n",
      "\n",
      "If ,  are independent standard normal random variables, then the ratio of their normalized sums of squares will have the F-distribution with  degrees of freedom:\n",
      "\n",
      "Operations on the density function\n",
      "The split normal distribution is most directly defined in terms of joining scaled sections of the density functions of different normal distributions and rescaling the density to integrate to one.  The truncated normal distribution results from rescaling a section of a single density function.\n",
      "\n",
      "Extensions\n",
      "The notion of normal distribution, being one of the most important distributions in probability theory, has been extended far beyond the standard framework of the univariate (that is one-dimensional) case (Case 1). All these extensions are also called normal or Gaussian laws, so a certain ambiguity in names exists.\n",
      "\n",
      "The multivariate normal distribution describes the Gaussian law in the k-dimensional Euclidean space. A vector X ∈ Rk is multivariate-normally distributed if any linear combination of its components ∑kj=1aj Xj has a (univariate) normal distribution. The variance of X is a k×k symmetric positive-definite matrix V. The multivariate normal distribution is a special case of the elliptical distributions. As such, its iso-density loci in the k = 2 case are ellipses and in the case of arbitrary k are ellipsoids.\n",
      "Rectified Gaussian distribution a rectified version of normal distribution with all the negative elements reset to 0\n",
      "Complex normal distribution deals with the complex normal vectors. A complex vector X ∈ Ck is said to be normal if both its real and imaginary components jointly possess a 2k-dimensional multivariate normal distribution. The variance-covariance structure of X is described by two matrices: the variance matrix Γ, and the relation matrix C.\n",
      "Matrix normal distribution describes the case of normally distributed matrices.\n",
      "Gaussian processes are the normally distributed stochastic processes. These can be viewed as elements of some infinite-dimensional Hilbert space H, and thus are the analogues of multivariate normal vectors for the case k = ∞. A random element h ∈ H is said to be normal if for any constant a ∈ H the scalar product (a, h) has a (univariate) normal distribution. The variance structure of such Gaussian random element can be described in terms of the linear covariance operator K: H → H. Several Gaussian processes became popular enough to have their own names:\n",
      "Brownian motion,\n",
      "Brownian bridge,\n",
      "Ornstein–Uhlenbeck process.\n",
      "Gaussian q-distribution is an abstract mathematical construction that represents a \"q-analogue\" of the normal distribution.\n",
      "the q-Gaussian is an analogue of the Gaussian distribution, in the sense that it maximises the Tsallis entropy, and is one type of Tsallis distribution. Note that this distribution is different from the Gaussian q-distribution above.\n",
      "A random variable X has a two-piece normal distribution if it has a distribution\n",
      "\n",
      "\n",
      "\n",
      "where μ is the mean and σ1 and σ2 are the standard deviations of the distribution to the left and right of the mean respectively.\n",
      "The mean, variance and third central moment of this distribution have been determined\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "where E(X), V(X) and T(X) are the mean, variance, and third central moment respectively.\n",
      "One of the main practical uses of the Gaussian law is to model the empirical distributions of many different random variables encountered in practice. In such case a possible extension would be a richer family of distributions, having more than two parameters and therefore being able to fit the empirical distribution more accurately. The examples of such extensions are:\n",
      "\n",
      "Pearson distribution — a four-parameter family of probability distributions that extend the normal law to include different skewness and kurtosis values.\n",
      "The generalized normal distribution, also known as the exponential power distribution, allows for distribution tails with thicker or thinner asymptotic behaviors.\n",
      "\n",
      "\n",
      "Statistical Inference\n",
      "Estimation of parameters\n",
      "\n",
      "It is often the case that we don't know the parameters of the normal distribution, but instead want to estimate them. That is, having a sample  from a normal  population we would like to learn the approximate values of parameters  and . The standard approach to this problem is the maximum likelihood method, which requires maximization of the log-likelihood function:\n",
      "\n",
      "\n",
      "Taking derivatives with respect to  and  and solving the resulting system of first order conditions yields the maximum likelihood estimates:\n",
      "\n",
      "\n",
      "Sample mean\n",
      "\n",
      "Estimator  is called the sample mean, since it is the arithmetic mean of all observations. The statistic  is complete and sufficient for , and therefore by the Lehmann–Scheffé theorem,  is the uniformly minimum variance unbiased (UMVU) estimator. In finite samples it is distributed normally:\n",
      "\n",
      "\n",
      "The variance of this estimator is equal to the μμ-element of the inverse Fisher information matrix . This implies that the estimator is finite-sample efficient. Of practical importance is the fact that the standard error of  is proportional to , that is, if one wishes to decrease the standard error by a factor of 10, one must increase the number of points in the sample by a factor of 100. This fact is widely used in determining sample sizes for opinion polls and the number of trials in Monte Carlo simulations.\n",
      "From the standpoint of the asymptotic theory,  is consistent, that is, it converges in probability to  as . The estimator is also asymptotically normal, which is a simple corollary of the fact that it is normal in finite samples:\n",
      "\n",
      "\n",
      "Sample variance\n",
      "\n",
      "The estimator  is called the sample variance, since it is the variance of the sample (). In practice, another estimator is often used instead of the . This other estimator is denoted  , and is also called the sample variance, which represents a certain ambiguity in terminology; its square root  is called the sample standard deviation. The estimator  differs from  by having (n − 1) instead of n in the denominator (the so-called Bessel's correction):\n",
      "\n",
      "\n",
      "The difference between  and  becomes negligibly small for large n's. In finite samples however, the motivation behind the use of  is that it is an unbiased estimator of the underlying parameter , whereas  is biased. Also, by the Lehmann–Scheffé theorem the estimator  is uniformly minimum variance unbiased (UMVU), which makes it the \"best\" estimator among all unbiased ones. However it can be shown that the biased estimator  is \"better\" than the  in terms of the mean squared error (MSE) criterion. In finite samples both  and  have scaled chi-squared distribution with (n − 1) degrees of freedom:\n",
      "\n",
      "\n",
      "The first of these expressions shows that the variance of  is equal to , which is slightly greater than the σσ-element of the inverse Fisher information matrix . Thus,  is not an efficient estimator for , and moreover, since  is UMVU, we can conclude that the finite-sample efficient estimator for  does not exist.\n",
      "Applying the asymptotic theory, both estimators  and  are consistent, that is they converge in probability to  as the sample size . The two estimators are also both asymptotically normal:\n",
      "\n",
      "\n",
      "In particular, both estimators are asymptotically efficient for  .\n",
      "\n",
      "Confidence intervals\n",
      "\n",
      "By Cochran's theorem, for normal distributions the sample mean  and the sample variance s2 are independent, which means there can be no gain in considering their joint distribution. There is also a converse theorem: if in a sample the sample mean and sample variance are independent, then the sample must have come from the normal distribution. The independence between  and s can be employed to construct the so-called t-statistic:\n",
      "\n",
      "\n",
      "This quantity t has the Student's t-distribution with (n − 1) degrees of freedom, and it is an ancillary statistic (independent of the value of the parameters). Inverting the distribution of this t-statistics will allow us to construct the confidence interval for μ; similarly, inverting the χ2 distribution of the statistic s2 will give us the confidence interval for σ2:\n",
      "\n",
      "\n",
      "\n",
      "where tk,p and χ 2k,p  are the pth quantiles of the t- and χ2-distributions respectively. These confidence intervals are of the confidence level 1 − α, meaning that the true values μ and σ2 fall outside of these intervals with probability (or significance level) α. In practice people usually take α = 5%, resulting in the 95% confidence intervals. The approximate formulas in the display above were derived from the asymptotic distributions of  and s2. The approximate formulas become valid for large values of n, and are more convenient for the manual calculation since the standard normal quantiles zα/2 do not depend on n. In particular, the most popular value of α = 5%, results in |z0.025| = 1.96.\n",
      "\n",
      "\n",
      "Normality tests\n",
      "\n",
      "Normality tests assess the likelihood that the given data set {x1, ..., xn} comes from a normal distribution. Typically the null hypothesis H0 is that the observations are distributed normally with unspecified mean μ and variance σ2, versus the alternative Ha that the distribution is arbitrary. Many tests (over 40) have been devised for this problem, the more prominent of them are outlined below:\n",
      "\n",
      "\"Visual\" tests are more intuitively appealing but subjective at the same time, as they rely on informal human judgement to accept or reject the null hypothesis.\n",
      "Q-Q plot— is a plot of the sorted values from the data set against the expected values of the corresponding quantiles from the standard normal distribution. That is, it's a plot of point of the form (Φ−1(pk), x(k)), where plotting points pk are equal to pk = (k − α)/(n + 1 − 2α) and α is an adjustment constant, which can be anything between 0 and 1. If the null hypothesis is true, the plotted points should approximately lie on a straight line.\n",
      "P-P plot— similar to the Q-Q plot, but used much less frequently. This method consists of plotting the points (Φ(z(k)), pk), where . For normally distributed data this plot should lie on a 45° line between (0, 0) and (1, 1).\n",
      "Shapiro-Wilk test employs the fact that the line in the Q-Q plot has the slope of σ. The test compares the least squares estimate of that slope with the value of the sample variance, and rejects the null hypothesis if these two quantities differ significantly.\n",
      "Normal probability plot (rankit plot)\n",
      "Moment tests:\n",
      "D'Agostino's K-squared test\n",
      "Jarque–Bera test\n",
      "Empirical distribution function tests:\n",
      "Lilliefors test (an adaptation of the Kolmogorov–Smirnov test)\n",
      "Anderson–Darling test\n",
      "Bayesian analysis of the normal distribution\n",
      "Bayesian analysis of normally distributed data is complicated by the many different possibilities that may be considered:\n",
      "\n",
      "Either the mean, or the variance, or neither, may be considered a fixed quantity.\n",
      "When the variance is unknown, analysis may be done directly in terms of the variance, or in terms of the precision, the reciprocal of the variance.  The reason for expressing the formulas in terms of precision is that the analysis of most cases is simplified.\n",
      "Both univariate and multivariate cases need to be considered.\n",
      "Either conjugate or improper prior distributions may be placed on the unknown variables.\n",
      "An additional set of cases occurs in Bayesian linear regression, where in the basic model the data is assumed to be normally distributed, and normal priors are placed on the regression coefficients. The resulting analysis is similar to the basic cases of independent identically distributed data, but more complex.\n",
      "The formulas for the non-linear-regression cases are summarized in the conjugate prior article.\n",
      "\n",
      "Sum of two quadratics\n",
      "Scalar form\n",
      "The following auxiliary formula is useful for simplifying the posterior update equations, which otherwise become fairly tedious.\n",
      "\n",
      "\n",
      "This equation rewrites the sum of two quadratics in x by expanding the squares, grouping the terms in x, and completing the square.  Note the following about the complex constant factors attached to some of the terms:\n",
      "\n",
      "The factor  has the form of a weighted average of y and z.\n",
      " This shows that this factor can be thought of as resulting from a situation where the reciprocals of quantities a and b add directly, so to combine a and b themselves, it's necessary to reciprocate, add, and reciprocate the result again to get back into the original units.  This is exactly the sort of operation performed by the harmonic mean, so it is not surprising that  is one-half the harmonic mean of a and b.\n",
      "Vector form\n",
      "A similar formula can be written for the sum of two vector quadratics: If x, y, z are vectors of length k, and A and B are symmetric, invertible matrices of size , then\n",
      "\n",
      "\n",
      "where\n",
      "\n",
      "\n",
      "Note that the form x′ A x is called a quadratic form and is a scalar:\n",
      "\n",
      "\n",
      "In other words, it sums up all possible combinations of products of pairs of elements from x, with a separate coefficient for each.  In addition, since , only the sum  matters for any off-diagonal elements of A, and there is no loss of generality in assuming that A is symmetric.  Furthermore, if A is symmetric, then the form \n",
      "\n",
      "Sum of differences from the mean\n",
      "Another useful formula is as follows:\n",
      "\n",
      "\n",
      "where \n",
      "\n",
      "With known variance\n",
      "For a set of i.i.d. normally distributed data points X of size n where each individual point x follows  with known variance σ2, the conjugate prior distribution is also normally distributed.\n",
      "This can be shown more easily by rewriting the variance as the precision, i.e. using τ = 1/σ2. Then if  and  we proceed as follows.\n",
      "First, the likelihood function is (using the formula above for the sum of differences from the mean):\n",
      "\n",
      "\n",
      "Then, we proceed as follows:\n",
      "\n",
      "\n",
      "In the above derivation, we used the formula above for the sum of two quadratics and eliminated all constant factors not involving μ.  The result is the kernel of a normal distribution, with mean  and precision , i.e.\n",
      "\n",
      "\n",
      "This can be written as a set of Bayesian update equations for the posterior parameters in terms of the prior parameters:\n",
      "\n",
      "\n",
      "That is, to combine n data points with total precision of nτ (or equivalently, total variance of n/σ2) and mean of values , derive a new total precision simply by adding the total precision of the data to the prior total precision, and form a new mean through a precision-weighted average, i.e. a weighted average of the data mean and the prior mean, each weighted by the associated total precision. This makes logical sense if the precision is thought of as indicating the certainty of the observations: In the distribution of the posterior mean, each of the input components is weighted by its certainty, and the certainty of this distribution is the sum of the individual certainties. (For the intuition of this, compare the expression \"the whole is (or is not) greater than the sum of its parts\".  In addition, consider that the knowledge of the posterior comes from a combination of the knowledge of the prior and likelihood, so it makes sense that we are more certain of it than of either of its components.)\n",
      "The above formula reveals why it is more convenient to do Bayesian analysis of conjugate priors for the normal distribution in terms of the precision.  The posterior precision is simply the sum of the prior and likelihood precisions, and the posterior mean is computed through a precision-weighted average, as described above.  The same formulas can be written in terms of variance by reciprocating all the precisions, yielding the more ugly formulas\n",
      "\n",
      "\n",
      "With known mean\n",
      "For a set of i.i.d. normally distributed data points X of size n where each individual point x follows  with known mean μ, the conjugate prior of the variance has an inverse gamma distribution or a scaled inverse chi-squared distribution.  The two are equivalent except for having different parameterizations.  Although the inverse gamma is more commonly used, we use the scaled inverse chi-squared for the sake of convenience.  The prior for σ2 is as follows:\n",
      "\n",
      "\n",
      "The likelihood function from above, written in terms of the variance, is:\n",
      "\n",
      "\n",
      "where\n",
      "\n",
      "\n",
      "Then:\n",
      "\n",
      "\n",
      "The above is also a scaled inverse chi-squared distribution where\n",
      "\n",
      "\n",
      "or equivalently\n",
      "\n",
      "\n",
      "Reparameterizing in terms of an inverse gamma distribution, the result is:\n",
      "\n",
      "\n",
      "With unknown mean and unknown variance\n",
      "For a set of i.i.d. normally distributed data points X of size n where each individual point x follows  with unknown mean μ and unknown variance σ2, a combined (multivariate) conjugate prior is placed over the mean and variance, consisting of a normal-inverse-gamma distribution.\n",
      "Logically, this originates as follows:\n",
      "\n",
      "From the analysis of the case with unknown mean but known variance, we see that the update equations involve sufficient statistics computed from the data consisting of the mean of the data points and the total variance of the data points, computed in turn from the known variance divided by the number of data points.\n",
      "From the analysis of the case with unknown variance but known mean, we see that the update equations involve sufficient statistics over the data consisting of the number of data points and sum of squared deviations.\n",
      "Keep in mind that the posterior update values serve as the prior distribution when further data is handled.  Thus, we should logically think of our priors in terms of the sufficient statistics just described, with the same semantics kept in mind as much as possible.\n",
      "To handle the case where both mean and variance are unknown, we could place independent priors over the mean and variance, with fixed estimates of the average mean, total variance, number of data points used to compute the variance prior, and sum of squared deviations.  Note however that in reality, the total variance of the mean depends on the unknown variance, and the sum of squared deviations that goes into the variance prior (appears to) depend on the unknown mean.  In practice, the latter dependence is relatively unimportant: Shifting the actual mean shifts the generated points by an equal amount, and on average the squared deviations will remain the same.  This is not the case, however, with the total variance of the mean: As the unknown variance increases, the total variance of the mean will increase proportionately, and we would like to capture this dependence.\n",
      "This suggests that we create a conditional prior of the mean on the unknown variance, with a hyperparameter specifying the mean of the pseudo-observations associated with the prior, and another parameter specifying the number of pseudo-observations.  This number serves as a scaling parameter on the variance, making it possible to control the overall variance of the mean relative to the actual variance parameter.  The prior for the variance also has two hyperparameters, one specifying the sum of squared deviations of the pseudo-observations associated with the prior, and another specifying once again the number of pseudo-observations.  Note that each of the priors has a hyperparameter specifying the number of pseudo-observations, and in each case this controls the relative variance of that prior.  These are given as two separate hyperparameters so that the variance (aka the confidence) of the two priors can be controlled separately.\n",
      "This leads immediately to the normal-inverse-gamma distribution, which is the product of the two distributions just defined, with conjugate priors used (an inverse gamma distribution over the variance, and a normal distribution over the mean, conditional on the variance) and with the same four parameters just defined.\n",
      "The priors are normally defined as follows:\n",
      "\n",
      "\n",
      "The update equations can be derived, and look as follows:\n",
      "\n",
      "\n",
      "The respective numbers of pseudo-observations add the number of actual observations to them.  The new mean hyperparameter is once again a weighted average, this time weighted by the relative numbers of observations.  Finally, the update for  is similar to the case with known mean, but in this case the sum of squared deviations is taken with respect to the observed data mean rather than the true mean, and as a result a new \"interaction term\" needs to be added to take care of the additional error source stemming from the deviation between prior and data mean.\n",
      "\n",
      "[Proof]\n",
      "The prior distributions are\n",
      "\n",
      "\n",
      "Therefore, the joint prior is\n",
      "\n",
      "\n",
      "The likelihood function from the section above with known variance is:\n",
      "\n",
      "\n",
      "Writing it in terms of variance rather than precision, we get:\n",
      "\n",
      "\n",
      "where \n",
      "Therefore, the posterior is (dropping the hyperparameters as conditioning factors):\n",
      "\n",
      "\n",
      "In other words, the posterior distribution has the form of a product of a normal distribution over p(μ | σ2) times an inverse gamma distribution over p(σ2), with parameters that are the same as the update equations above.\n",
      "\n",
      "\n",
      "Occurrence and applications\n",
      "The occurrence of normal distribution in practical problems can be loosely classified into four categories:\n",
      "\n",
      "Exactly normal distributions;\n",
      "Approximately normal laws, for example when such approximation is justified by the central limit theorem; and\n",
      "Distributions modeled as normal – the normal distribution being the distribution with maximum entropy for a given mean and variance.\n",
      "Regression problems – the normal distribution being found after systematic effects have been modeled sufficiently well.\n",
      "Exact normality\n",
      " The ground state of a quantum harmonic oscillator has the Gaussian distribution.\n",
      "Certain quantities in physics are distributed normally, as was first demonstrated by James Clerk Maxwell. Examples of such quantities are:\n",
      "\n",
      "Probability density function of a ground state in a quantum harmonic oscillator.\n",
      "The position of a particle that experiences diffusion. If initially the particle is located at a specific point (that is its probability distribution is the dirac delta function), then after time t its location is described by a normal distribution with variance t, which satisfies the diffusion equation . If the initial location is given by a certain density function , then the density at time t is the convolution of g and the normal PDF.\n",
      "Approximate normality\n",
      "Approximately normal distributions occur in many situations, as explained by the central limit theorem. When the outcome is produced by many small effects acting additively and independently, its distribution will be close to normal. The normal approximation will not be valid if the effects act multiplicatively (instead of additively), or if there is a single external influence that has a considerably larger magnitude than the rest of the effects.\n",
      "\n",
      "In counting problems, where the central limit theorem includes a discrete-to-continuum approximation and where infinitely divisible and decomposable distributions are involved, such as\n",
      "Binomial random variables, associated with binary response variables;\n",
      "Poisson random variables, associated with rare events;\n",
      "Thermal radiation has a Bose–Einstein distribution on very short time scales, and a normal distribution on longer timescales due to the central limit theorem.\n",
      "Assumed normality\n",
      " Histogram of sepal widths for Iris versicolor from Fisher's Iris flower data set, with superimposed best-fitting normal distribution.\n",
      "I can only recognize the occurrence of the normal curve – the Laplacian curve of errors – as a very abnormal phenomenon. It is roughly approximated to in certain distributions; for this reason, and on account for its beautiful simplicity, we may, perhaps, use it as a first approximation, particularly in theoretical investigations.— Pearson (1901)\n",
      "\n",
      "There are statistical methods to empirically test that assumption, see the above Normality tests section.\n",
      "\n",
      "In biology, the logarithm of various variables tend to have a normal distribution, that is, they tend to have a log-normal distribution (after separation on male/female subpopulations), with examples including:\n",
      "Measures of size of living tissue (length, height, skin area, weight);\n",
      "The length of inert appendages (hair, claws, nails, teeth) of biological specimens, in the direction of growth; presumably the thickness of tree bark also falls under this category;\n",
      "Certain physiological measurements, such as blood pressure of adult humans.\n",
      "In finance, in particular the Black–Scholes model, changes in the logarithm of exchange rates, price indices, and stock market indices are assumed normal (these variables behave like compound interest, not like simple interest, and so are multiplicative). Some mathematicians such as Benoit Mandelbrot have argued that log-Levy distributions, which possesses heavy tails would be a more appropriate model, in particular for the analysis for stock market crashes. The use of the assumption of normal distribution occurring in financial models has also been criticized by Nassim Nicholas Taleb in his works.\n",
      "Measurement errors in physical experiments are often modeled by a normal distribution. This use of a normal distribution does not imply that one is assuming the measurement errors are normally distributed, rather using the normal distribution produces the most conservative predictions possible given only knowledge about the mean and variance of the errors.\n",
      "In standardized testing, results can be made to have a normal distribution by either selecting the number and difficulty of questions (as in the IQ test) or transforming the raw test scores into \"output\" scores by fitting them to the normal distribution. For example, the SAT's traditional range of 200–800 is based on a normal distribution with a mean of 500 and a standard deviation of 100.\n",
      " Fitted cumulative normal distribution to October rainfalls, see distribution fitting\n",
      "Many scores are derived from the normal distribution, including percentile ranks (\"percentiles\" or \"quantiles\"), normal curve equivalents, stanines, z-scores, and T-scores. Additionally, some behavioral statistical procedures assume that scores are normally distributed; for example, t-tests and ANOVAs. Bell curve grading assigns relative grades based on a normal distribution of scores.\n",
      "In hydrology the distribution of long duration river discharge or rainfall, e.g. monthly and yearly totals, is often thought to be practically normal according to the central limit theorem. The blue picture, made with CumFreq, illustrates an example of fitting the normal distribution to ranked October rainfalls showing the 90% confidence belt based on the binomial distribution. The rainfall data are represented by plotting positions as part of the cumulative frequency analysis.\n",
      "Produced normality\n",
      "In regression analysis, lack of normality in residuals simply indicates that the model postulated is inadequate in accounting for the tendency in the data and needs to be augmented; in other words, normality in residuals can always be achieved given a properly constructed model.\n",
      "\n",
      "Computational methods\n",
      "Generating values from normal distribution\n",
      " The bean machine, a device invented by Francis Galton, can be called the first generator of normal random variables. This machine consists of a vertical board with interleaved rows of pins. Small balls are dropped from the top and then bounce randomly left or right as they hit the pins. The balls are collected into bins at the bottom and settle down into a pattern resembling the Gaussian curve.\n",
      "In computer simulations, especially in applications of the Monte-Carlo method, it is often desirable to generate values that are normally distributed. The algorithms listed below all generate the standard normal deviates, since a N(μ, σ2) can be generated as X = μ + σZ, where Z is standard normal. All these algorithms rely on the availability of a random number generator U capable of producing uniform random variates.\n",
      "\n",
      "The most straightforward method is based on the probability integral transform property: if U is distributed uniformly on (0,1), then Φ−1(U) will have the standard normal distribution. The drawback of this method is that it relies on calculation of the probit function Φ−1, which cannot be done analytically. Some approximate methods are described in Hart (1968) and in the erf article. Wichura gives a fast algorithm for computing this function to 16 decimal places, which is used by R to compute random variates of the normal distribution.\n",
      "An easy to program approximate approach, that relies on the central limit theorem, is as follows: generate 12 uniform U(0,1) deviates, add them all up, and subtract 6 – the resulting random variable will have approximately standard normal distribution. In truth, the distribution will be Irwin–Hall, which is a 12-section eleventh-order polynomial approximation to the normal distribution. This random deviate will have a limited range of (−6, 6).\n",
      "The Box–Muller method uses two independent random numbers U and V distributed uniformly on (0,1). Then the two random variables X and Y\n",
      "\n",
      "will both have the standard normal distribution, and will be independent. This formulation arises because for a bivariate normal random vector (X, Y) the squared norm X2 + Y2 will have the chi-squared distribution with two degrees of freedom, which is an easily generated exponential random variable corresponding to the quantity −2ln(U) in these equations; and the angle is distributed uniformly around the circle, chosen by the random variable V.\n",
      "The Marsaglia polar method is a modification of the Box–Muller method which does not require computation of the sine and cosine functions. In this method, U and V are drawn from the uniform (−1,1) distribution, and then S = U2 + V2 is computed. If S is greater or equal to 1, then the method starts over, otherwise the two quantities\n",
      "\n",
      "are returned. Again, X and Y are independent, standard normal random variables.\n",
      "The Ratio method is a rejection method. The algorithm proceeds as follows:\n",
      "Generate two independent uniform deviates U and V;\n",
      "Compute X = √8/e (V − 0.5)/U;\n",
      "Optional: if X2 ≤ 5 − 4e1/4U then accept X and terminate algorithm;\n",
      "Optional: if X2 ≥ 4e−1.35/U + 1.4 then reject X and start over from step 1;\n",
      "If X2 ≤ −4 lnU then accept X, otherwise start over the algorithm.\n",
      "The two optional steps allow the evaluation of the logarithm in the last step to be avoided in most cases.  These steps can be greatly improved so that the logarithm is rarely evaluated.\n",
      "The ziggurat algorithm is faster than the Box–Muller transform and still exact. In about 97% of all cases it uses only two random numbers, one random integer and one random uniform, one multiplication and an if-test. Only in 3% of the cases, where the combination of those two falls outside the \"core of the ziggurat\" (a kind of rejection sampling using logarithms), do exponentials and more uniform random numbers have to be employed.\n",
      "Integer arithmetic can be used to sample from the standard normal distribution.  This method is exact in the sense that it satisfies the conditions of ideal approximation; i.e., it is equivalent to sampling a real number from the standard normal distribution and rounding this to the nearest representable floating point number.\n",
      "There is also some investigation into the connection between the fast Hadamard transform and the normal distribution, since the transform employs just addition and subtraction and by the central limit theorem random numbers from almost any distribution will be transformed into the normal distribution. In this regard a series of Hadamard transforms can be combined with random permutations to turn arbitrary data sets into a normally distributed data.\n",
      "Numerical approximations for the normal CDF\n",
      "The standard normal CDF is widely used in scientific and statistical computing.\n",
      "The values Φ(x) may be approximated very accurately by a variety of methods, such as numerical integration, Taylor series, asymptotic series and continued fractions. Different approximations are used depending on the desired level of accuracy.\n",
      "\n",
      "Zelen & Severo (1964) give the approximation for Φ(x) for x > 0 with the absolute error |ε(x)| < 7.5·10−8 (algorithm 26.2.17):\n",
      "\n",
      "where ϕ(x) is the standard normal PDF, and b0 = 0.2316419, b1 = 0.319381530, b2 = −0.356563782, b3 =  1.781477937, b4 = −1.821255978, b5 = 1.330274429.Hart (1968) lists some dozens of approximations – by means of rational functions, with or without exponentials – for the erfc() function. His algorithms vary in the degree of complexity and the resulting precision, with maximum absolute precision of 24 digits. An algorithm by West (2009) combines Hart's algorithm 5666 with a continued fraction approximation in the tail to provide a fast computation algorithm with a 16-digit precision.Cody (1969) after recalling Hart68 solution is not suited for erf, gives a solution for both erf and erfc, with maximal relative error bound, via Rational Chebyshev Approximation.Marsaglia (2004) suggested a simple algorithm based on the Taylor series expansion\n",
      "\n",
      "\n",
      "\n",
      "for calculating Φ(x) with arbitrary precision. The drawback of this algorithm is comparatively slow calculation time (for example it takes over 300 iterations to calculate the function with 16 digits of precision when x = 10).The GNU Scientific Library calculates values of the standard normal CDF using Hart's algorithms and approximations with Chebyshev polynomials.\n",
      "Shore (1982) introduced simple approximations that may be incorporated in stochastic optimization models of engineering and operations research, like reliability engineering and inventory analysis. Denoting p=Φ(z), the simplest approximation for the quantile function is:\n",
      "\n",
      "\n",
      "This approximation delivers for z a maximum absolute error of 0.026 (for 0.5 ≤ p ≤ 0.9999, corresponding to 0 ≤ z ≤ 3.719). For p < 1/2 replace p by 1 − p and change sign. Another approximation, somewhat less accurate, is the single-parameter approximation:\n",
      "\n",
      "\n",
      "The latter had served to derive a simple approximation for the loss integral of the normal distribution, defined by\n",
      "\n",
      "\n",
      "This approximation is particularly accurate for the right far-tail (maximum error of 10−3 for z≥1.4). Highly accurate approximations for the CDF, based on Response Modeling Methodology (RMM, Shore, 2011, 2012), are shown in Shore (2005).\n",
      "Some more approximations can be found at: Error function#Approximation with elementary functions. In particular, small relative error on the whole domain for the CDF  and the quantile function  as well, is achieved via an explicitly invertible formula by Sergei Winitzki in 2008.\n",
      "\n",
      "History\n",
      "Development\n",
      "Some authors attribute the credit for the discovery of the normal distribution to de Moivre, who in 1738 published in the second edition of his \"The Doctrine of Chances\" the study of the coefficients in the binomial expansion of (a + b)n. De Moivre proved that the middle term in this expansion has the approximate magnitude of , and that \"If m or ½n be a Quantity infinitely great, then the Logarithm of the Ratio, which a Term distant from the middle by the Interval ℓ, has to the middle Term, is .\" Although this theorem can be interpreted as the first obscure expression for the normal probability law, Stigler points out that de Moivre himself did not interpret his results as anything more than the approximate rule for the binomial coefficients, and in particular de Moivre lacked the concept of the probability density function.\n",
      "\n",
      " Carl Friedrich Gauss discovered the normal distribution in 1809 as a way to rationalize the method of least squares.\n",
      "In 1809 Gauss published his monograph \"Theoria motus corporum coelestium in sectionibus conicis solem ambientium\" where among other things he introduces several important statistical concepts, such as the method of least squares, the method of maximum likelihood, and the normal distribution. Gauss used M, M′, M′′, ... to denote the measurements of some unknown quantity V, and sought the \"most probable\" estimator of that quantity: the one that maximizes the probability φ(M − V) · φ(M′ − V) · φ(M′′ − V) · ... of obtaining the observed experimental results. In his notation φΔ is the probability law of the measurement errors of magnitude Δ. Not knowing what the function φ is, Gauss requires that his method should reduce to the well-known answer: the arithmetic mean of the measured values. Starting from these principles, Gauss demonstrates that the only law that rationalizes the choice of arithmetic mean as an estimator of the location parameter, is the normal law of errors:\n",
      "\n",
      "\n",
      "where h is \"the measure of the precision of the observations\". Using this normal law as a generic model for errors in the experiments, Gauss formulates what is now known as the non-linear weighted least squares (NWLS) method.\n",
      "\n",
      " Marquis de Laplace proved the central limit theorem in 1810, consolidating the importance of the normal distribution in statistics.\n",
      "Although Gauss was the first to suggest the normal distribution law, Laplace made significant contributions. It was Laplace who first posed the problem of aggregating several observations in 1774, although his own solution led to the Laplacian distribution. It was Laplace who first calculated the value of the integral ∫ e−t2 dt = √ in 1782, providing the normalization constant for the normal distribution. Finally, it was Laplace who in 1810 proved and presented to the Academy the fundamental central limit theorem, which emphasized the theoretical importance of the normal distribution.\n",
      "It is of interest to note that in 1809 an American mathematician Adrain published two derivations of the normal probability law, simultaneously and independently from Gauss. His works remained largely unnoticed by the scientific community, until in 1871 they were \"rediscovered\" by Abbe.\n",
      "In the middle of the 19th century Maxwell demonstrated that the normal distribution is not just a convenient mathematical tool, but may also occur in natural phenomena: \"The number of particles whose velocity, resolved in a certain direction, lies between x and x + dx is\n",
      "\n",
      "\n",
      "Naming\n",
      "Since its introduction, the normal distribution has been known by many different names: the law of error, the law of facility of errors, Laplace's second law, Gaussian law, etc. Gauss himself apparently coined the term with reference to the \"normal equations\" involved in its applications, with normal having its technical meaning of orthogonal rather than \"usual\". However, by the end of the 19th century some authors had started using the name normal distribution, where the word \"normal\" was used as an adjective – the term now being seen as a reflection of the fact that this distribution was seen as typical, common – and thus \"normal\". Peirce (one of those authors) once defined \"normal\" thus: \"...the 'normal' is not the average (or any other kind of mean) of what actually occurs, but of what would, in the long run, occur under certain circumstances.\" Around the turn of the 20th century Pearson popularized the term normal as a designation for this distribution.\n",
      "\n",
      "Many years ago I called the Laplace–Gaussian curve the normal curve, which name, while it avoids an international question of priority, has the disadvantage of leading people to believe that all other distributions of frequency are in one sense or another 'abnormal'. — Pearson (1920)\n",
      "\n",
      "Also, it was Pearson who first wrote the distribution in terms of the standard deviation σ as in modern notation. Soon after this, in year 1915, Fisher added the location parameter to the formula for normal distribution, expressing it in the way it is written nowadays:\n",
      "\n",
      "\n",
      "The term \"standard normal\", which denotes the normal distribution with zero mean and unit variance came into general use around the 1950s, appearing in the popular textbooks by P.G. Hoel (1947) \"Introduction to mathematical statistics\" and A.M. Mood (1950) \"Introduction to the theory of statistics\".\n",
      "When the name is used, the \"Gaussian distribution\" was named after Carl Friedrich Gauss, who introduced the distribution in 1809 as a way of rationalizing the method of least squares as outlined above. Among English speakers, both \"normal distribution\" and \"Gaussian distribution\" are in common use, with different terms preferred by different communities.\n",
      "\n",
      "\n",
      "\n",
      "Wrapped normal distribution — the Normal distribution applied to a circular domain\n",
      "Bates distribution — similar to the Irwin–Hall distribution, but rescaled back into the 0 to 1 range\n",
      "Behrens–Fisher problem — the long-standing problem of testing whether two normal samples with different variances have same means;\n",
      "Bhattacharyya distance – method used to separate mixtures of normal distributions\n",
      "Erdős–Kac theorem—on the occurrence of the normal distribution in number theory\n",
      "Gaussian blur—convolution, which uses the normal distribution as a kernel\n",
      "Normally distributed and uncorrelated does not imply independent\n",
      "Reciprocal normal distribution\n",
      "Ratio normal distribution\n",
      "Standard normal table\n",
      "Sub-Gaussian distribution\n",
      "Sum of normally distributed random variables\n",
      "Tweedie distribution — The normal distribution is a member of the family of Tweedie exponential dispersion models\n",
      "Z-test— using the normal distribution\n",
      "Stein's lemma\n",
      "Notes\n",
      "\n",
      "\n",
      "Citations\n",
      "\n",
      "Sources\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Wikimedia Commons has media related to Normal distribution.\n",
      "\n",
      "\n",
      "Normal distribution calculator, More powerful calculator\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, concept in enumerate(tokens):\n",
    "    if len(tokens[concept]) > 5000:\n",
    "        print (concept,'\\t\\t\\t\\t', len(tokens[concept]))\n",
    "        print (data[concept])\n",
    "        print ()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/1034 [00:00<?, ?it/s]WARNING: Logging before flag parsing goes to stderr.\n",
      "W0228 18:59:07.423355 17812 deprecation_wrapper.py:119] From C:\\Users\\spark\\PycharmProjects\\capstone\\embedder\\embedder.py:22: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "W0228 18:59:08.904329 17812 deprecation_wrapper.py:119] From C:\\Users\\spark\\PycharmProjects\\capstone\\embedder\\embedder.py:23: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.\n",
      "\n",
      "W0228 18:59:09.524894 17812 deprecation_wrapper.py:119] From C:\\Users\\spark\\PycharmProjects\\capstone\\embedder\\embedder.py:24: The name tf.tables_initializer is deprecated. Please use tf.compat.v1.tables_initializer instead.\n",
      "\n",
      "  6%|████▉                                                                         | 65/1034 [08:01<3:41:45, 13.73s/it]"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "2 root error(s) found.\n  (0) Resource exhausted: OOM when allocating tensor with shape[9036,256,1,46] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[node module_apply_default_65/bilm/CNN/Conv2D_4 (defined at c:\\users\\spark\\tfenv\\lib\\site-packages\\tensorflow_hub\\native_module.py:610) ]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\t [[module_apply_default_65/truediv/_157]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n  (1) Resource exhausted: OOM when allocating tensor with shape[9036,256,1,46] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[node module_apply_default_65/bilm/CNN/Conv2D_4 (defined at c:\\users\\spark\\tfenv\\lib\\site-packages\\tensorflow_hub\\native_module.py:610) ]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n0 successful operations.\n0 derived errors ignored.\n\nOriginal stack trace for 'module_apply_default_65/bilm/CNN/Conv2D_4':\n  File \"C:\\Program Files\\Python36\\lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"C:\\Program Files\\Python36\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"c:\\users\\spark\\tfenv\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"c:\\users\\spark\\tfenv\\lib\\site-packages\\traitlets\\config\\application.py\", line 658, in launch_instance\n    app.start()\n  File \"c:\\users\\spark\\tfenv\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 505, in start\n    self.io_loop.start()\n  File \"c:\\users\\spark\\tfenv\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 132, in start\n    self.asyncio_loop.run_forever()\n  File \"C:\\Program Files\\Python36\\lib\\asyncio\\base_events.py\", line 430, in run_forever\n    self._run_once()\n  File \"C:\\Program Files\\Python36\\lib\\asyncio\\base_events.py\", line 1443, in _run_once\n    handle._run()\n  File \"C:\\Program Files\\Python36\\lib\\asyncio\\events.py\", line 145, in _run\n    self._callback(*self._args)\n  File \"c:\\users\\spark\\tfenv\\lib\\site-packages\\tornado\\ioloop.py\", line 758, in _run_callback\n    ret = callback()\n  File \"c:\\users\\spark\\tfenv\\lib\\site-packages\\tornado\\stack_context.py\", line 300, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"c:\\users\\spark\\tfenv\\lib\\site-packages\\tornado\\gen.py\", line 1233, in inner\n    self.run()\n  File \"c:\\users\\spark\\tfenv\\lib\\site-packages\\tornado\\gen.py\", line 1147, in run\n    yielded = self.gen.send(value)\n  File \"c:\\users\\spark\\tfenv\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 357, in process_one\n    yield gen.maybe_future(dispatch(*args))\n  File \"c:\\users\\spark\\tfenv\\lib\\site-packages\\tornado\\gen.py\", line 326, in wrapper\n    yielded = next(result)\n  File \"c:\\users\\spark\\tfenv\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 267, in dispatch_shell\n    yield gen.maybe_future(handler(stream, idents, msg))\n  File \"c:\\users\\spark\\tfenv\\lib\\site-packages\\tornado\\gen.py\", line 326, in wrapper\n    yielded = next(result)\n  File \"c:\\users\\spark\\tfenv\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 534, in execute_request\n    user_expressions, allow_stdin,\n  File \"c:\\users\\spark\\tfenv\\lib\\site-packages\\tornado\\gen.py\", line 326, in wrapper\n    yielded = next(result)\n  File \"c:\\users\\spark\\tfenv\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 294, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"c:\\users\\spark\\tfenv\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 536, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"c:\\users\\spark\\tfenv\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2819, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"c:\\users\\spark\\tfenv\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2845, in _run_cell\n    return runner(coro)\n  File \"c:\\users\\spark\\tfenv\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 67, in _pseudo_sync_runner\n    coro.send(None)\n  File \"c:\\users\\spark\\tfenv\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3020, in run_cell_async\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"c:\\users\\spark\\tfenv\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3185, in run_ast_nodes\n    if (yield from self.run_code(code, result)):\n  File \"c:\\users\\spark\\tfenv\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3267, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-6-e376d400eccb>\", line 2, in <module>\n    embeddings['concept'] = embedder.embed(embedder.tokenize(data[concept]))\n  File \"C:\\Users\\spark\\PycharmProjects\\capstone\\embedder\\embedder.py\", line 21, in embed\n    as_dict=True)['default']\n  File \"c:\\users\\spark\\tfenv\\lib\\site-packages\\tensorflow_hub\\module.py\", line 261, in __call__\n    name=name)\n  File \"c:\\users\\spark\\tfenv\\lib\\site-packages\\tensorflow_hub\\native_module.py\", line 610, in create_apply_graph\n    import_scope=relative_scope_name)\n  File \"c:\\users\\spark\\tfenv\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 1449, in import_meta_graph\n    **kwargs)[0]\n  File \"c:\\users\\spark\\tfenv\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 1473, in _import_meta_graph_with_return_elements\n    **kwargs))\n  File \"c:\\users\\spark\\tfenv\\lib\\site-packages\\tensorflow\\python\\framework\\meta_graph.py\", line 857, in import_scoped_meta_graph_with_return_elements\n    return_elements=return_elements)\n  File \"c:\\users\\spark\\tfenv\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py\", line 507, in new_func\n    return func(*args, **kwargs)\n  File \"c:\\users\\spark\\tfenv\\lib\\site-packages\\tensorflow\\python\\framework\\importer.py\", line 443, in import_graph_def\n    _ProcessNewOps(graph)\n  File \"c:\\users\\spark\\tfenv\\lib\\site-packages\\tensorflow\\python\\framework\\importer.py\", line 236, in _ProcessNewOps\n    for new_op in graph._add_new_tf_operations(compute_devices=False):  # pylint: disable=protected-access\n  File \"c:\\users\\spark\\tfenv\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3751, in _add_new_tf_operations\n    for c_op in c_api_util.new_tf_operations(self)\n  File \"c:\\users\\spark\\tfenv\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3751, in <listcomp>\n    for c_op in c_api_util.new_tf_operations(self)\n  File \"c:\\users\\spark\\tfenv\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3641, in _create_op_from_tf_operation\n    ret = Operation(c_op, self)\n  File \"c:\\users\\spark\\tfenv\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 2005, in __init__\n    self._traceback = tf_stack.extract_stack()\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[1;32mc:\\users\\spark\\tfenv\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1355\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1356\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1357\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\spark\\tfenv\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1340\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m-> 1341\u001b[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1342\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\spark\\tfenv\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1428\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1429\u001b[1;33m         run_metadata)\n\u001b[0m\u001b[0;32m   1430\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mResourceExhaustedError\u001b[0m: 2 root error(s) found.\n  (0) Resource exhausted: OOM when allocating tensor with shape[9036,256,1,46] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[{{node module_apply_default_65/bilm/CNN/Conv2D_4}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\t [[module_apply_default_65/truediv/_157]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n  (1) Resource exhausted: OOM when allocating tensor with shape[9036,256,1,46] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[{{node module_apply_default_65/bilm/CNN/Conv2D_4}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n0 successful operations.\n0 derived errors ignored.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-e376d400eccb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mconcept\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtqdm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0membeddings\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'concept'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0membedder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0membedder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mconcept\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\PycharmProjects\\capstone\\embedder\\embedder.py\u001b[0m in \u001b[0;36membed\u001b[1;34m(self, tokens)\u001b[0m\n\u001b[0;32m     23\u001b[0m             \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mglobal_variables_initializer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m             \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtables_initializer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m             \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0membedding\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\spark\\tfenv\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    948\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    949\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 950\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    951\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    952\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\spark\\tfenv\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1171\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1172\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1173\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1174\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1175\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\spark\\tfenv\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1348\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1349\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1350\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1351\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1352\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\spark\\tfenv\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1368\u001b[0m           \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1369\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0merror_interpolation\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minterpolate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1370\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1371\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1372\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mResourceExhaustedError\u001b[0m: 2 root error(s) found.\n  (0) Resource exhausted: OOM when allocating tensor with shape[9036,256,1,46] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[node module_apply_default_65/bilm/CNN/Conv2D_4 (defined at c:\\users\\spark\\tfenv\\lib\\site-packages\\tensorflow_hub\\native_module.py:610) ]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\t [[module_apply_default_65/truediv/_157]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n  (1) Resource exhausted: OOM when allocating tensor with shape[9036,256,1,46] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[node module_apply_default_65/bilm/CNN/Conv2D_4 (defined at c:\\users\\spark\\tfenv\\lib\\site-packages\\tensorflow_hub\\native_module.py:610) ]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n0 successful operations.\n0 derived errors ignored.\n\nOriginal stack trace for 'module_apply_default_65/bilm/CNN/Conv2D_4':\n  File \"C:\\Program Files\\Python36\\lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"C:\\Program Files\\Python36\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"c:\\users\\spark\\tfenv\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"c:\\users\\spark\\tfenv\\lib\\site-packages\\traitlets\\config\\application.py\", line 658, in launch_instance\n    app.start()\n  File \"c:\\users\\spark\\tfenv\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 505, in start\n    self.io_loop.start()\n  File \"c:\\users\\spark\\tfenv\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 132, in start\n    self.asyncio_loop.run_forever()\n  File \"C:\\Program Files\\Python36\\lib\\asyncio\\base_events.py\", line 430, in run_forever\n    self._run_once()\n  File \"C:\\Program Files\\Python36\\lib\\asyncio\\base_events.py\", line 1443, in _run_once\n    handle._run()\n  File \"C:\\Program Files\\Python36\\lib\\asyncio\\events.py\", line 145, in _run\n    self._callback(*self._args)\n  File \"c:\\users\\spark\\tfenv\\lib\\site-packages\\tornado\\ioloop.py\", line 758, in _run_callback\n    ret = callback()\n  File \"c:\\users\\spark\\tfenv\\lib\\site-packages\\tornado\\stack_context.py\", line 300, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"c:\\users\\spark\\tfenv\\lib\\site-packages\\tornado\\gen.py\", line 1233, in inner\n    self.run()\n  File \"c:\\users\\spark\\tfenv\\lib\\site-packages\\tornado\\gen.py\", line 1147, in run\n    yielded = self.gen.send(value)\n  File \"c:\\users\\spark\\tfenv\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 357, in process_one\n    yield gen.maybe_future(dispatch(*args))\n  File \"c:\\users\\spark\\tfenv\\lib\\site-packages\\tornado\\gen.py\", line 326, in wrapper\n    yielded = next(result)\n  File \"c:\\users\\spark\\tfenv\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 267, in dispatch_shell\n    yield gen.maybe_future(handler(stream, idents, msg))\n  File \"c:\\users\\spark\\tfenv\\lib\\site-packages\\tornado\\gen.py\", line 326, in wrapper\n    yielded = next(result)\n  File \"c:\\users\\spark\\tfenv\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 534, in execute_request\n    user_expressions, allow_stdin,\n  File \"c:\\users\\spark\\tfenv\\lib\\site-packages\\tornado\\gen.py\", line 326, in wrapper\n    yielded = next(result)\n  File \"c:\\users\\spark\\tfenv\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 294, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"c:\\users\\spark\\tfenv\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 536, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"c:\\users\\spark\\tfenv\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2819, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"c:\\users\\spark\\tfenv\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2845, in _run_cell\n    return runner(coro)\n  File \"c:\\users\\spark\\tfenv\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 67, in _pseudo_sync_runner\n    coro.send(None)\n  File \"c:\\users\\spark\\tfenv\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3020, in run_cell_async\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"c:\\users\\spark\\tfenv\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3185, in run_ast_nodes\n    if (yield from self.run_code(code, result)):\n  File \"c:\\users\\spark\\tfenv\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3267, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-6-e376d400eccb>\", line 2, in <module>\n    embeddings['concept'] = embedder.embed(embedder.tokenize(data[concept]))\n  File \"C:\\Users\\spark\\PycharmProjects\\capstone\\embedder\\embedder.py\", line 21, in embed\n    as_dict=True)['default']\n  File \"c:\\users\\spark\\tfenv\\lib\\site-packages\\tensorflow_hub\\module.py\", line 261, in __call__\n    name=name)\n  File \"c:\\users\\spark\\tfenv\\lib\\site-packages\\tensorflow_hub\\native_module.py\", line 610, in create_apply_graph\n    import_scope=relative_scope_name)\n  File \"c:\\users\\spark\\tfenv\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 1449, in import_meta_graph\n    **kwargs)[0]\n  File \"c:\\users\\spark\\tfenv\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 1473, in _import_meta_graph_with_return_elements\n    **kwargs))\n  File \"c:\\users\\spark\\tfenv\\lib\\site-packages\\tensorflow\\python\\framework\\meta_graph.py\", line 857, in import_scoped_meta_graph_with_return_elements\n    return_elements=return_elements)\n  File \"c:\\users\\spark\\tfenv\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py\", line 507, in new_func\n    return func(*args, **kwargs)\n  File \"c:\\users\\spark\\tfenv\\lib\\site-packages\\tensorflow\\python\\framework\\importer.py\", line 443, in import_graph_def\n    _ProcessNewOps(graph)\n  File \"c:\\users\\spark\\tfenv\\lib\\site-packages\\tensorflow\\python\\framework\\importer.py\", line 236, in _ProcessNewOps\n    for new_op in graph._add_new_tf_operations(compute_devices=False):  # pylint: disable=protected-access\n  File \"c:\\users\\spark\\tfenv\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3751, in _add_new_tf_operations\n    for c_op in c_api_util.new_tf_operations(self)\n  File \"c:\\users\\spark\\tfenv\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3751, in <listcomp>\n    for c_op in c_api_util.new_tf_operations(self)\n  File \"c:\\users\\spark\\tfenv\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3641, in _create_op_from_tf_operation\n    ret = Operation(c_op, self)\n  File \"c:\\users\\spark\\tfenv\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 2005, in __init__\n    self._traceback = tf_stack.extract_stack()\n"
     ]
    }
   ],
   "source": [
    "for concept in tqdm.tqdm(data):\n",
    "    embeddings['concept'] = embedder.embed(embedder.tokenize(data[concept]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'embedder' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-e4bd4f98de3e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0membedder\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'embedder' is not defined"
     ]
    }
   ],
   "source": [
    "embedder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
